-- Compact batch 6/29: rows 251-300

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Create a script that connects to a PostgreSQL database via psql and computes per-customer lifecycle metrics over the last 3 years—first_order_at, last_order_at, total_orders, lifetime_gross_revenue, refunds_amount, net_revenue, and avg_days_between_orders—using CTEs, window functions, and UTC-normalized timestamps while excluding test/fraudulent activity. Export the result sorted by customer_id to /app/customer_ltv.csv using COPY with a header and exactly those columns.', NULL, ARRAY['postgresql', 'database']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Implement a terminal-run script that reads credentials from environment variables, connects to PostgreSQL and MongoDB, reconciles active users by email with at least one paid order in the last 90 days, and exports a flattened dataset to /app/output/users_aggregate.csv and /app/output/users_aggregate.parquet with deterministic ordering. Provide a PostgreSQL .sql file to create a materialized view and indexes used by the export, and ensure the script uses server-side COPY for CSV and batched streaming for Parquet to support million-row exports.', NULL, ARRAY['postgresql', 'mongodb', 'sql']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Spin up local Postgres and MongoDB with seed data, then implement a CLI script that joins Postgres orders to MongoDB user profiles by user_id, computes 30/60/90-day LTV and signup-month cohorts per acquisition_channel, and exports results to a partitioned Parquet dataset plus a summary CSV. The script must support incremental runs via a persisted watermark (last_order_ts) and perform idempotent upserts of partitions.', NULL, ARRAY['mongodb']),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Create a Bash script that reads SSH credentials and paths from /app/remote.toml, uploads /app/input/* to a remote server, triggers a Nextflow pipeline there in a detached session, and polls its status until success/failure with a timeout. On completion, download the produced artifacts to /app/output and write /app/run_report.json containing run_id, status, start/end timestamps, and SHA256 checksums for each artifact.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Create a CLI that SSHes into a remote SLURM head node, renders a batch script from a local YAML, submits it with sbatch, and robustly monitors the job by polling squeue/sacct and tailing logs (including retrying if preempted). On completion, scp the results back, verify checksums from a manifest, and write a run_report.json with job ID, final state, runtime, and artifact hashes.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Create a CLI that submits a SLURM job array to process CSV shards listed in /app/manifest.json, capturing the sbatch JobID and polling squeue/sacct until every task completes successfully. When done, fetch each task’s output via scp into /app/results, verify SHA256 checksums from the manifest, and merge the outputs into /app/final.parquet in deterministic shard order; exit non-zero if any task fails or times out.', 'hard', NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Implement a CLI that snapshots a local pipeline directory, syncs it to a remote host over SSH/SCP, launches it in a detached session with a unique run ID, and continuously monitors status/logs with automatic reconnect after SSH drops. On completion or timeout, fetch artifacts and emit /app/run_summary.json containing run ID, start/stop times, exit code, SHA256 of outputs, and a flag indicating if execution was skipped due to idempotent content-hash deduplication.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Implement a CLI tool that reads an SSH inventory and a jobs.json describing data partitions, then dispatches a processing script to multiple hosts via rsync+ssh, launches jobs under nohup with a unique run_id, and periodically polls remote status/logs until completion with bounded retries. On success it collects artifacts into /app/results/{run_id} and writes a summary manifest (per-host status, duration, and stderr tail), supporting resume and cancel by run_id.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Build a CLI that converts a folder of Markdown notes (with YAML front matter, [[wiki-links]], reference-style links, and images) into a single self-contained HTML and a PDF: rewrite intra-note links to anchors, deduplicate references, number sections/figures/tables, apply a supplied BibTeX+CSL, and inline all assets. Also emit search_index.json mapping each heading to its text snippet and anchor.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Build a command-line tool that migrates a static HTML site in /app/site to a markdown-first repo: convert each article HTML to Markdown with YAML front matter (title/date/tags), mirror images locally and rewrite internal links to .md. Generate index.md as a site map and report.txt listing broken links and missing assets.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Create a script that compiles a directory of Markdown notes (with YAML front matter and [[WikiLinks]]) into a self-contained HTML site and a JSONL plain-text search index. Resolve {{include:...}} directives, convert wikilinks to relative paths, inline local images as data URIs, and output a broken-link report.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Create a script that converts all Markdown files under /app/pages into self-contained HTML, rendering fenced mermaid and graphviz code blocks to inline SVG (using mermaid-cli and dot) and converting the remainder via pandoc. It must rewrite cross-file links to .html, preserve front-matter metadata as meta tags, and generate a tag index page from YAML front matter.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Create a script that scans /app/posts/*.md (with YAML front matter: title, date, tags), converts each to a standalone HTML article in /app/site via pandoc, preserving code fences and rewriting relative links using a BASE_URL. Also generate an Atom feed at /app/site/atom.xml listing the 20 most recent posts with RFC 3339 UTC timestamps, canonical URLs, and a summary taken from the first paragraph.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Create a command-line script that recursively scans /app/logs (including .gz files), uses regular expressions to parse heterogeneous log lines into structured fields (timestamp in multiple formats, level, message) and extract entities (IPv4/IPv6, emails, GUIDs, request/user IDs). Redact PII by masking emails and Luhn-valid credit card numbers with stable pseudonyms, write redacted_logs.ndjson, and output templates.csv by normalizing messages to regex-derived templates (replace numbers/hex/IDs/IPs with placeholders) with aggregated counts.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Create a command-line script that scans all .txt files in /app/contracts for calendar dates written in diverse styles (YYYY-MM-DD, MM/DD/YYYY, DD Mon YYYY, Mon DD, YYYY, DDth of Month ’YY, and ranges like Jan 3–5, 2024) using regexes that handle ordinal suffixes, commas, stray punctuation, and Unicode dashes. Normalize each concrete date to YYYY-MM-DD, expand ranges into individual dates, and write a CSV to /app/contract_dates.csv with columns file,line_number,original_snippet,normalized_date, sorted by file then normalized_date.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Create a script that scans /app/paper for .tex and .bib files, uses regex to extract citation keys from LaTeX citation commands (handling optional arguments and comma-separated lists), and extracts BibTeX entry keys. Write /app/citation_audit.txt listing ordered unique cited keys, missing citations (cited but absent in .bib), unused .bib entries, and per-file citation counts.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Scan /app/docs for Markdown files and use regex to extract external URLs, bare domain mentions, and email addresses from prose while excluding fenced code blocks and inline code. Also detect reference-style link labels to report undefined and unused labels, writing links.csv (type,value,first_file,count) and ref_audit.txt with the findings.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Write a script that scans all Markdown files under /app/docs and, using only regex-based parsing, extracts every fenced code block (```lang ... ```) with its language tag, 1-based start/end line numbers, and any inline TODO: ... comments inside or on immediately adjacent lines, producing /app/blocks.csv with columns file_path, block_index, language, start_line, end_line, todos (semicolon-separated). Also perform an in-place substitution that wraps any shell code block containing a line matching rm\s+-rf\s+[^;]+ in <!-- DANGEROUS --> ... <!-- /DANGEROUS --> markers without altering code content.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Create a CLI that profiles all CSV/TSV files in /app/data, infers column types, computes per-column quality metrics (null %, distinct count, numeric stats, text length ranges, top values), and flags mixed-type anomalies and candidate ID/date columns. Output a readable /app/profile.md and a structured /app/profile.json summarizing per-file results and a cross-file schema reconciliation section.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Create a command-line script that scans /app/adr for Markdown Architecture Decision Records with YAML front matter (id, title, date, status, supersedes/superseded_by), validates cross-references, and generates /app/adr_index.md. The report must include a chronological table, sections grouped by status, and ASCII supersession chains for each lineage with deterministic ordering.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Generate release notes by parsing a Git repo at /app/repo using Conventional Commits, producing /app/RELEASE_NOTES.md and a machine-readable /app/release.json for the latest tag range (or last 50 commits if no tags). Categorize entries by type and scope, detect BREAKING CHANGE footers, count contributors, and convert issue/PR references (e.g., #123) into links.', NULL, ARRAY['git']),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Implement a script that scans Nginx-style logs in /app/logs/*.log and a release timeline in /app/releases.json to detect outage windows where per-minute 5xx response rate exceeds a given threshold and attribute each window to the nearest release. Generate a Markdown incident report (/app/incident_report.md) with a timeline and a CSV (/app/incident_windows.csv) with start,end,duration,peak_rate,top_errors,release_sha, sorted chronologically.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Write a command-line script that scans a Git repository at /app/repo and generates /app/RELEASE_NOTES.md for the tag range specified in /app/range.txt by parsing Conventional Commits and cross-referencing issue/PR titles from /app/issues.json. The Markdown must include grouped sections by type and scope, a BREAKING CHANGES section from footers or ! commits, an alphabetically ordered contributor summary with commit counts, and deterministic ordering within each section.', NULL, ARRAY['git']),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Create a CLI that renders Jinja2 templates in /app/templates using layered variables from /app/config/base.yaml, /app/config/env/*.yaml, and /app/config/secrets.json, and for each locale in /app/locales.csv merges the matching /app/i18n/<locale>.yaml. It must support simple macros for pluralization and date formatting, fail on unresolved placeholders, emit outputs to /app/build/<locale>/ preserving paths, and write /app/build_manifest.json listing each file with its locale and SHA256.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Create a CLI that renders a directory of Markdown pages with YAML front matter through Jinja2 layouts/includes/macros, applying variable precedence of page > section defaults > global defaults > environment and custom filters (datefmt, slugify, markdown). Write HTML to /app/dist mirroring the source tree and emit a manifest.json mapping each source file to its chosen layout and output path.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Create a script that renders all Jinja2 templates under /app/templates to /app/build using layered variables from /app/defaults.yaml, environment variables, and optional per-template .vars.yaml files (per-template > env > defaults), preserving directory structure and failing on undefined variables. Implement custom filters slug (kebab-case) and sha256_file(path under /app/assets), support {% include %}, and write a manifest.json in /app/build listing each output and its SHA-256.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Implement a preprocessor that renders a set of .tmpl files into environment-specific configs by expanding user-defined macros, {{ENV_VAR}} placeholders, and conditional blocks from a vars.yml inventory, with nested include support and cycle detection. Output dev, staging, and prod variants to /app/build and exit non-zero if any placeholder remains unresolved.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Implement a two-stage renderer that processes all .tmpl files under /app/templates: first expand ${VAR} using envsubst limited to a whitelist in /app/allowed_env.txt, then render Jinja2 with values from /app/values.yml and custom filters (slugify, to_kebab, join_path). Write outputs to /app/rendered preserving structure and produce /app/report.json with per-file SHA256 and any unresolved placeholders, exiting non-zero if any remain.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Diagnose and fix a Node.js CLI that crashes with unhandled promise rejections when the upstream API stalls or returns invalid JSON by adding robust async error handling, per-request timeouts, and a retry-with-exponential-backoff fallback. Ensure it surfaces clear messages, writes a partial cached result when available, and exits with a nonzero code on unrecoverable errors; verify using the provided flaky server and integration tests.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Fix a Python CLI downloader that crashes on transient HTTP errors and leaves corrupted partial files by adding robust try/except handling with timeouts, retry-with-exponential-backoff, mirror fallback, and safe atomic writes that can resume from .part files. Verify by simulating failures and confirming the final artifact matches a provided checksum.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Fix a Python ETL loader that crashes on malformed CSV rows, encoding errors, and SQLite constraint violations by adding robust try/except handling, per-row validation, and a dead-letter output. The job should complete with partial success, write rejected rows to /app/bad_rows.csv, and exit zero once all recoverable errors are handled.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Harden a Python CLI that crashes with BrokenPipeError when its output is piped to another process (e.g., head) by adding robust EPIPE/SIGPIPE handling and a clean shutdown path. Verify it emits no stack trace, exits successfully, and does not leave partial lines in the downstream output.', 'hard', ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Harden a Python CLI that ingests a directory of JSON Lines files into SQLite; it currently crashes on malformed JSON, oversized records, missing keys, and SQLITE_BUSY/locked errors. Add robust try/except and validation to quarantine bad lines, apply bounded retries with exponential backoff for transient DB errors, fall back to defaults for missing fields, and produce a deterministic /app/ingest_summary.json so the run completes without uncaught exceptions.', 'hard', ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Debug an intermittent crash in a multithreaded C program by using gdb to reproduce the failure, inspect thread states, and set conditional breakpoints and memory watchpoints to trace a use-after-free. Patch the lifetime bug and verify stability by running the provided stress test until it passes consistently.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Diagnose and fix a sporadic SIGSEGV in a multithreaded C ring-buffer logger by reproducing under load, attaching with gdb, and using thread-aware stepping and watchpoints on head/tail indices to find the out-of-bounds write. Patch the bug using proper atomics/locking and verify stability with a stress test.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Reproduce and debug an intermittent segmentation fault in a minimal C HTTP chunked-encoding parser by running it under gdb with a crafted malformed request, stepping through the state machine and inspecting buffer pointers to locate an out-of-bounds write. Implement the fix with proper bounds checks/index corrections, recompile, and verify the parser handles the input without crashing and produces correct output.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Track down an intermittent segmentation fault in a multi-threaded C log processor by using gdb to reproduce, inspect per-thread backtraces, and set watchpoints on a shared ring buffer to catch the corrupting write. Fix the use-after-free in the consumer, rebuild, and verify stability under a provided parallel stress test.', NULL, ARRAY['parallel']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Use gdb to diagnose and fix a sporadic crash in a multithreaded C ring buffer that only fails under -O2 by setting thread-aware breakpoints and watchpoints to catch an out-of-bounds write caused by missing synchronization. Implement correct memory ordering or locking and verify stability by running the provided stress harness until it completes without errors.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Diagnose and fix a Rust implementation of topological sort that yields incorrect or nondeterministic orders by mutating a set during iteration and miscounting in-degrees. Implement a correct queue-based Kahn’s algorithm and add a regression test covering cycles, duplicate edges, and deterministic ordering.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Fix a Rust CLI’s Dijkstra implementation that returns incorrect shortest-path distances on graphs with zero-weight and parallel edges due to premature visited-marking and missing decrease-key handling. Correct the relaxation and priority-queue logic, add regression tests, and verify outputs on provided fixtures.', NULL, ARRAY['rust', 'parallel']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Identify and fix a Python topological sort implementation that returns incorrect orders and occasionally fails to detect cycles due to flawed in-degree updates and non-deterministic node selection. Implement a deterministic Kahn’s algorithm with explicit cycle detection and verify stable ordering across repeated runs on provided DAGs.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Repair a shortest-path tool that returns wrong Dijkstra distances on graphs with zero-weight edges due to stale priority-queue entries and off-by-one node indexing. Update the algorithm to correctly handle decrease-key via ignoring stale heap nodes and proper indexing so outputs match provided golden results across multiple graphs, including disconnected cases.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'A Rust CLI builds successfully but crashes with SIGSEGV only in --release. Use RUST_BACKTRACE with AddressSanitizer or Miri to locate undefined behavior inside an unsafe block, fix the memory error, and verify cargo test and cargo run complete without crashes.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Diagnose a C++17 CLI tool that compiles but intermittently segfaults on large inputs. Use AddressSanitizer/UBSan and gdb to trace iterator invalidation from std::vector reallocation in a loop, refactor the code to avoid undefined behavior, and verify by running the provided corpus without crashes.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Diagnose and repair a C++ CLI tool that crashes under -O2 with a SIGSEGV due to vector iterator invalidation in a custom deduplication routine. Use AddressSanitizer or gdb to pinpoint the fault, refactor the loop to avoid invalidated iterators, then rebuild with -O2 and verify the program completes on the provided dataset without crashing.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Repair a Rust CLI that fails to compile due to OpenSSL linkage errors and then panics at runtime when a config file is absent by switching TLS dependencies to rustls, fixing Cargo feature flags, and making config loading non-panicking. Verify by building on stable, running cargo test, and executing the binary with and without a config file.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Repair a flaky multithreaded C11 program that randomly segfaults under -O2 by reproducing the crash, using AddressSanitizer/ThreadSanitizer to locate a data race/use-after-free, and identifying the offending code paths. Implement proper synchronization and lifetime management so the binary runs reliably and passes a provided stress test.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Diagnose a nightly cron job that runs a Python ETL (CSV to Parquet) but fails under cron with ModuleNotFoundError and missing files due to minimal PATH, unset locale, and relative paths. Fix the job by using a virtualenv and absolute paths via a wrapper script, exporting PATH/LANG/TZ, ensuring execute permissions and logging, and verify success by running it under a simulated cron environment.', NULL, ARRAY['python', 'logging']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Diagnose a nightly cron-scheduled CSV-to-JSON ETL that works manually but fails under cron due to CRLF line endings, a bad shebang, and reliance on relative paths/implicit virtualenv activation. Convert line endings, fix executable/shebang and use absolute paths to the venv’s Python, then verify the job runs from cron and writes the expected outputs and logs.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Diagnose why a nightly cron job defined in /etc/cron.d/etl that should transform /app/input/*.csv into /app/out/data.parquet never produces output. Identify and fix cron-specific issues (PATH/virtualenv, working directory/relative paths, stale flock lockfile) and dependency/import errors so the job runs under cron; verify by forcing a run and ensuring the Parquet is generated.', NULL, NULL);
