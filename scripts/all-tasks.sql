-- Compact batch 1/29: rows 1-50

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Build & Dependency Management', 'Build Optimization & Performance', 'Build Profiling & Benchmarking', 'Benchmark a CMake/Ninja C++ project across linkers (ld.bfd vs lld) and with/without LTO, capturing compile vs link phase durations and Clang -ftime-trace to identify the five slowest translation units. Produce a JSON report comparing full and incremental rebuild times (after touching a common header) and recommend the fastest configuration.', NULL, NULL),
('Build & Dependency Management', 'Build Optimization & Performance', 'Build Profiling & Benchmarking', 'Instrument a CMake-based C++ project to build with Ninja and Clang using -ftime-trace, aggregating the emitted JSON traces to identify the five slowest translation units and the link step. Add CMake options for precompiled headers and unity builds, run baseline and optimized clean builds, and emit /app/metrics.json with total wall times, per-step timings, and percent improvement.', NULL, NULL),
('Build & Dependency Management', 'Build Optimization & Performance', 'Build Profiling & Benchmarking', 'Instrument a provided CMake/Ninja C++ project with clang’s -ftime-trace and Ninja -d stats to profile cold and incremental builds, reporting the slowest translation units and link steps. Enable ccache and a precompiled header via CMake, then re-benchmark and output a before/after summary of total build time, parallelism, and cache hit rates.', NULL, NULL),
('Build & Dependency Management', 'Build Optimization & Performance', 'Build Profiling & Benchmarking', 'Profile a CMake+Ninja C++ project by enabling Clang -ftime-trace and Ninja -d stats to collect per-file compile/link timings and parallelism, then benchmark clean vs. incremental builds across -j values with and without ccache/PCH and using GNU ld vs. ld.lld. Emit a JSON summary ranking configurations by wall time, flag the two slowest targets, and apply minimal flag tweaks to achieve at least a 30% speedup over the baseline.', NULL, NULL),
('Build & Dependency Management', 'Build Optimization & Performance', 'Build Profiling & Benchmarking', 'Profile a medium C++ CMake/Ninja project by collecting compile and link timings (e.g., clang -ftime-trace and ninja -d stats), then identify the worst-offending translation units and headers. Introduce a precompiled header and an optional unity-build mode, rebuild, and emit a metrics.json comparing baseline vs optimized total build time, longest TU time, and percent improvement.', 'medium', NULL),
('Build & Dependency Management', 'Build Optimization & Performance', 'Incremental Builds & Caching', 'Configure a Bazel workspace to use a local HTTP remote cache (bazel-remote) persisted under /cache, build a mixed C++/Java project to prime the cache, then simulate a fresh checkout and rebuild. Verify the second build achieves >95% remote cache hits with no compile actions executed, based on Bazel’s build event summary.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Incremental Builds & Caching', 'Configure sccache as a unified compiler cache for a mixed Rust (Cargo) + C (CMake) workspace, routing rustc, CC, and CXX through sccache with stable flags. Verify incremental behavior by performing back-to-back builds to record cache hits, then modifying a single Rust crate so only that crate recompiles while C targets are served from cache, as shown by sccache stats and build logs.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Incremental Builds & Caching', 'Configure sccache as the compiler wrapper for a mixed Rust + C++ workspace (Cargo + CMake) so both toolchains share a persistent cache and clean rebuilds are served from it. Validate by building twice and then after a targeted source edit, reporting sccache stats showing cache hits for unchanged targets and minimal recompilation for the modified component.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Incremental Builds & Caching', 'Configure sccache for a Rust workspace and wire cargo to use it with rustc, enabling incremental compilation with a shared on-disk cache. Build twice across two clean worktrees, then modify a single crate to demonstrate minimal rebuilds and record timings plus sccache hit/miss stats to /app/metrics.json.', NULL, ARRAY['rust', 'compilation']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Incremental Builds & Caching', 'Provision a local Bazel remote cache (bazel-remote) and configure a polyglot Bazel workspace (C++, Java, Python) to use it, then perform two clean builds to prove incremental and remote cache hits. Parse the Bazel Build Event Protocol to write a summary of action counts, cache-hit ratios, and elapsed time improvements.', NULL, ARRAY['java', 'python']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Parallelization & Resource Utilization', 'Configure distributed, parallel compilation for a medium-size CMake+Ninja C/C++ project using distcc by launching multiple local distccd workers and wiring CC/CXX and DISTCC_HOSTS, then build with -j$(nproc). Validate by parsing distcc logs to confirm a majority of object files were compiled remotely and that total parallel jobs respect the configured CPU quota without oversubscription.', 'medium', ARRAY['distributed', 'parallel', 'compilation']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Parallelization & Resource Utilization', 'Convert the provided CMake-based C++ project to use Ninja with CMake Unity builds and ccache, and enable ThinLTO to parallelize the link stage. Auto-detect available cores, run a cold and warm build with -j$(nproc), and output a report showing wall-time improvement and cache hits on the second build.', NULL, NULL),
('Build & Dependency Management', 'Build Optimization & Performance', 'Parallelization & Resource Utilization', 'Reconfigure a CMake-based C++ monorepo to use the Ninja generator with CMAKE_JOB_POOLS/CMAKE_JOB_POOL_LINK to fully parallelize compilation while capping memory-heavy link jobs, and enable ccache for reuse across repeated builds. Run a baseline (Unix Makefiles, -j1) and the optimized build, then emit /app/parallel_metrics.json with cores utilized, wall-time speedup, and cache hit rate.', NULL, ARRAY['compilation']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Parallelization & Resource Utilization', 'Refactor a multi-module Gradle project to build and test in parallel by enabling org.gradle.parallel, tuning workers and test forks based on available CPU/memory, and enabling the local build cache. Capture baseline single-threaded vs optimized build times and emit a metrics JSON with speedup while ensuring produced JARs and test outcomes remain identical.', NULL, ARRAY['parallel']),
('Build & Dependency Management', 'Build Optimization & Performance', 'Parallelization & Resource Utilization', 'Refactor a recursive Makefile-based C/C++ project to be jobserver-aware and compile in parallel via distcc with ccache, spawning multiple local distccd agents to simulate a distributed build. Ensure sub-makes share the jobserver tokens, utilize all CPU cores, and emit a build log proving compilation fanned out across the agents.', NULL, ARRAY['parallel', 'distributed', 'compilation']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Build Output Validation', 'Diagnose and correct a Go (1.22) project''s build settings so it outputs a fully static, reproducible binary. Validate by ensuring ldd reports ''not a dynamic executable'', readelf shows no INTERP or NEEDED entries, and two builds with identical SOURCE_DATE_EPOCH have identical SHA-256 hashes.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Build Output Validation', 'Fix a CMake-based project that embeds absolute RPATHs causing the installed executable to fail when moved. Reconfigure and rebuild to emit $ORIGIN-relative RUNPATH for bundled shared libraries, then validate by relocating the install tree, executing the binary, and confirming via readelf/ldd that no absolute paths remain.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Build Output Validation', 'Repair a CMake-based C shared library that leaks internal symbols and lacks proper SONAME/versioning by enforcing hidden visibility and a GNU ld version script. Validate that the built lib has SONAME libfoo.so.1 and exports only the intended API symbols under version FOO_1.0 (checked via nm/readelf) and that a minimal client links and runs successfully.', NULL, ARRAY['api']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Build Output Validation', 'Repair a CMake-based build so the produced shared library is libvector.so.3 with an exact exported symbol set matching /app/exports.txt, no undefined symbols, no RUNPATH/RPATH, and the correct SONAME. Validate by diffing nm -D --defined-only against the reference, checking readelf -d for SONAME/RPATH, and building a tiny client that links dynamically and prints a known checksum to confirm ABI correctness.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Build Output Validation', 'Repair a failing CMake build of a versioned shared library (libimagelib) by fixing PIC/visibility flags and defining a proper SONAME and symlink chain. Validate by ensuring libimagelib.so.2 resolves via ldd, nm -D exposes only intended symbols, a tiny consumer links and runs, and a sample transform output matches an expected SHA256.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Compiler & Linker Errors', 'Fix a CMake-based C++ project where building a shared plugin fails to link due to a vendored static library compiled without -fPIC (relocation against .text/.rodata). Reconfigure the build to rebuild that static library with POSITION_INDEPENDENT_CODE and correct link interface so the plugin links and a provided dlopen smoke test runs.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Compiler & Linker Errors', 'Repair a CMake-based C++ project that links a shared library against a bundled static archive and currently fails with R_X86_64_32 relocation errors and undefined references to dlopen/pthread/clock_gettime. Rebuild the static archive with -fPIC and correct the CMake link configuration (Threads::Threads, dl, rt) so the library links cleanly and the demo executable runs.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Compiler & Linker Errors', 'Repair a CMake-based C++17 project that fails to link a shared library due to non-PIC static dependencies and missing OpenMP/filesystem linkage. Rebuild static libs with -fPIC (or use shared), enable OpenMP via CMake and conditionally link stdc++fs for older GCC, set $ORIGIN rpath, and validate by running a threaded test that dlopens the library.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Compiler & Linker Errors', 'Repair a failing JNI native library build by fixing missing jni.h includes, enabling position-independent code, and linking against the correct libjvm with proper RPATH in the build configuration. Verify by producing a loadable libnative.so that the provided Java class can load and call to print an expected message.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Configuration & Environment Issues', 'Diagnose and fix a Meson-based C project that fails to detect SDL2 because it is installed under a non-standard prefix (/opt/sdl2). Configure environment variables (e.g., PKG_CONFIG_PATH and LD_LIBRARY_PATH) and, if needed, a Meson cross file so the project configures, builds, and the resulting binary links against /opt/sdl2 as verified by ldd.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Configuration & Environment Issues', 'Diagnose and repair a Rust crate build that fails to compile openssl-sys and bindgen by installing the proper system tools and correctly setting PKG_CONFIG_PATH/OPENSSL_DIR and LIBCLANG_PATH/LD_LIBRARY_PATH so Cargo can find OpenSSL and libclang. Verify by building in release mode and running a provided HTTPS client example to confirm TLS works end-to-end.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Configuration & Environment Issues', 'Repair a Java Maven project that fails because it requires JDK 11 via the maven-toolchains-plugin but the environment defaults to JDK 17, by installing/selecting the correct JDK and configuring ~/.m2/toolchains.xml plus JAVA_HOME/PATH so mvn -B -DskipTests package succeeds without changing pom.xml. Verify by printing the javac -version used and running the built JAR to confirm it targets Java 11.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Configuration & Environment Issues', 'Repair a failing Maven Java build caused by an incorrect JDK and unset JAVA_HOME by installing the required JDK 21, setting JAVA_HOME/PATH, and configuring a Maven toolchains.xml so the compiler uses the correct toolchain. Verify by running mvn -DskipTests package to produce the expected JAR at a specified path without changing dependency declarations.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Dependency & Compatibility Issues', 'Diagnose a CMake C++ project that fails to build and link because the protoc used for code generation does not match the installed libprotobuf runtime version. Install or build a matching protobuf toolchain, update PATH and CMAKE_PREFIX_PATH so generation and linking use the same version, then rebuild to produce a binary that runs and prints a provided test message.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Dependency & Compatibility Issues', 'Diagnose a Rust Cargo build failing on Linux due to an OpenSSL headers/library version mismatch (openssl-sys expecting OpenSSL 3 while the OS provides 1.1). Resolve by only modifying Cargo.toml (pin compatible crate versions or enable the vendored OpenSSL feature) and installing minimal build tools so cargo build --release succeeds and the produced binary runs and prints "TLS: OK".', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Dependency & Compatibility Issues', 'Diagnose and fix a Rust CLI project that fails to compile because the openssl-sys crate expects OpenSSL 1.1 while the system only provides OpenSSL 3. Adjust Cargo features and environment (e.g., enable the vendored feature or point pkg-config to the correct libs), rebuild successfully, and verify with a TLS request plus ldd showing the intended libssl linkage.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Dependency & Compatibility Issues', 'Diagnose and repair a Node.js project failing to build because the sharp native addon is incompatible with the installed Node.js version and the required libvips system libraries are missing. Align the Node/ABI and dependency versions (or rebuild sharp from source) and install the OS-level libraries so npm install completes and the test suite passes.', NULL, NULL),
('Build & Dependency Management', 'Build Troubleshooting & Repair', 'Dependency & Compatibility Issues', 'Repair a Maven-based Java project that fails to build on JDK 21 due to incompatible plugin/dependency versions (maven-compiler-plugin, animal-sniffer) and javax→jakarta namespace conflicts. Update version pins and dependency exclusions so mvn -DskipTests=false verify succeeds and the assembled Spring Boot jar launches and responds on /actuator/health.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Artifact Publishing & Deployment', 'Build a multi-arch OCI image for the provided service using Docker Buildx, push it to a local registry, then sign it with a locally generated cosign key and attach an SPDX SBOM as an OCI artifact. Verify by pulling the manifest list to confirm both architectures, validating the signature against the public key, reading the SBOM from the registry, and deploying via a digest-pinned Docker Compose file.', NULL, ARRAY['docker']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Artifact Publishing & Deployment', 'Create a CI-style release pipeline that cross-compiles a Go CLI for linux/amd64 and linux/arm64, packages deterministic tarballs with embedded build metadata, generates SBOM and checksums, signs the artifacts, and publishes them to a local “release” store; verify by installing from that store and validating the signature.', NULL, NULL),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Artifact Publishing & Deployment', 'Package a small CLI tool as a Debian .deb and publish it to a locally hosted APT repository served over HTTP. Generate and sign repository metadata, add the repo to sources.list, install the package via apt by name, and verify the installed binary runs.', NULL, NULL),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Script Development', 'Create a Bash build script that compiles a small CMake-based library and a Python extension wheel using a fixed SOURCE_DATE_EPOCH and deterministic toolchain/linker flags, then packages outputs into a tarball with a manifest of checksums and build metadata. The script must run tests, reuse a local cache across invocations, and verify reproducibility by proving two consecutive runs produce identical tarball checksums.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Script Development', 'Create a deterministic Bash build script and accompanying CI YAML that build a Rust-backed Python extension (PyO3) into manylinux wheels for CPython 3.11 and 3.12 using maturin, cache Cargo/pip artifacts, run pytest, and place wheels in /app/dist. The pipeline must honor SOURCE_DATE_EPOCH, run in a matrix for linux/amd64 and linux/arm64 via QEMU, and fail on formatting or type-check violations (cargo fmt --check, ruff, mypy).', NULL, ARRAY['rust', 'python']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Script Development', 'Create a portable build.sh and CI workflow (YAML) that build, audit, and test manylinux wheels for a PyO3-based Rust extension across CPython 3.9–3.12 using maturin/auditwheel with Cargo and pip caches, then install each wheel in fresh venvs to run smoke/unit tests. The script must run locally and in CI, emit a dist/ manifest with checksums, and fail if any wheel is non-manylinux-compliant or import tests fail.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Script Development', 'Write a portable Bash build script for a CMake-based C/C++ repo that runs a compiler×sanitizer matrix (gcc/clang × none/ASan/UBSan), executes ctest for each, and emits JUnit XML plus Cobertura coverage (gcovr) for gcc builds. The script must read a coverage threshold from a config file, fail the build if unmet, cache builds with ccache, and place all artifacts under /app/artifacts.', NULL, NULL),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Verification & Reproducibility', 'Build a provided Go CLI twice in separate working directories and with different PATH orders while enforcing hermetic, reproducible outputs (e.g., -trimpath, -buildid=, vendor-only modules, fixed SOURCE_DATE_EPOCH), then assert the two binaries have identical SHA256 hashes. Only adjust build scripts/configuration without modifying source code, and the task fails on any network access or hash mismatch.', NULL, NULL),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Verification & Reproducibility', 'Configure a Gradle multi-module Java project to produce bit-for-bit reproducible artifacts (JARs and a shaded fat JAR) by normalizing file order, timestamps, and manifest fields, and pinning plugin/wrapper versions. Add a CI shell script that builds twice in isolated workspaces and fails if any artifact checksums differ.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Verification & Reproducibility', 'Convert a multi-module Maven Java project to produce bit-for-bit reproducible JARs by configuring project.build.outputTimestamp from SOURCE_DATE_EPOCH, normalizing MANIFEST entries, and locking dependency versions, then perform two clean builds to verify identical SHA256s for every artifact. The task should fail if any JAR differs or includes non-deterministic metadata such as timestamps, build paths, or environment-specific fields.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Verification & Reproducibility', 'Create a hermetic CI-style script that builds a provided Go CLI twice in separate directories with different GOPATHs, using -trimpath, -buildvcs=false, and SOURCE_DATE_EPOCH to eliminate path and timestamp leakage, then asserts identical SHA-256 hashes of the binaries. On success, emit a brief reproducibility report and a minimal provenance JSON; on failure, output a targeted diff (e.g., cmp/strings) to highlight remaining non-determinism.', NULL, NULL),
('Build & Dependency Management', 'Continuous Integration & Automation', 'Build Verification & Reproducibility', 'Transform the provided Python package into a bit-for-bit reproducible wheel by normalizing zip entry timestamps, ordering, and metadata, eliminating VCS-derived version noise, and honoring SOURCE_DATE_EPOCH. Perform two clean builds in different build paths and verify identical SHA256 of the .whl, then install it and run a supplied test script to confirm functionality.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'CI Configuration & Maintenance', 'Configure a GitHub Actions workflow that builds and tests a Rust project in a matrix (Linux and macOS) with sccache and dependency caching, then on tags performs two clean release builds and asserts the binaries are bit-for-bit identical before uploading them as artifacts. The pipeline must also generate and upload an SBOM for each artifact and fail if reproducibility or SBOM generation checks fail.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'CI Configuration & Maintenance', 'Create a GitHub Actions configuration for a monorepo containing a Python package and a Rust crate using a reusable workflow and two caller workflows with path filters so only relevant jobs run, with pip/cargo caching and cross-job artifact sharing. Validate locally (e.g., with act) that pytest and cargo test execute when their respective directories change and are skipped otherwise.', NULL, ARRAY['python', 'rust']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'CI Configuration & Maintenance', 'Create a GitHub Actions workflow for a Python/Node monorepo that uses path filters to trigger language-specific jobs, caches pip and pnpm, provisions a Postgres service for integration tests, and builds manylinux wheels with cibuildwheel and auditwheel; on tags, publish release artifacts with checksums. Validate locally with act by ensuring all jobs pass and expected wheels plus a merged coverage report are uploaded as artifacts.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Continuous Integration & Automation', 'CI Configuration & Maintenance', 'Fix and harden a GitHub Actions workflow for a CMake-based C project by pinning actions to SHAs, enabling ccache with cache restore/save across a gcc/clang matrix, and splitting build/test/release with minimal permissions and concurrency cancellation. Validate locally with act that pushes run build+test across the matrix and that tagging v* produces an uploaded release artifact.', 'hard', NULL),
('Build & Dependency Management', 'Continuous Integration & Automation', 'CI Configuration & Maintenance', 'Repair a failing GitHub Actions workflow by migrating deprecated features (set-output, add-path, old v1 actions) to current best practices: pin actions by commit SHA, upgrade to setup-node v4 with Node 20, switch to upload-artifact v4, add proper caching and a concurrency group. Validate locally with act that the workflow installs with npm ci, runs tests, and successfully uploads a build artifact.', NULL, NULL);
-- Compact batch 2/29: rows 51-100

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Build & Dependency Management', 'Dependency Management', 'Dependency Installation & Version Control', 'Convert a Rust Cargo workspace into a fully pinned, offline build by replacing loose version requirements with exact pins, generating a deterministic Cargo.lock, and vendoring all crates with cargo vendor while wiring .cargo/config.toml to the vendor directory. Verify success by clearing the Cargo registry/cache and building and testing the workspace with cargo --offline to demonstrate no network access is needed.', NULL, ARRAY['rust', 'testing']),
('Build & Dependency Management', 'Dependency Management', 'Dependency Installation & Version Control', 'Diagnose and fix a Go module dependency graph that fails to resolve due to mixed v1/v2+ module paths and a transitive break. Update only go.mod by pinning compatible versions and adding replace directives (including a local path for a provided fork) so that go mod tidy and go build ./... succeed.', NULL, NULL),
('Build & Dependency Management', 'Dependency Management', 'Dependency Installation & Version Control', 'In a Rust workspace, resolve a security advisory by pinning a transitive crate to a specific commit via [patch.crates-io], regenerate Cargo.lock, and vendor all dependencies so the project builds strictly offline. Verify by compiling with cargo build -Z offline using only the vendored directory and running the binary to print its version.', NULL, ARRAY['rust', 'security']),
('Build & Dependency Management', 'Dependency Management', 'Dependency Installation & Version Control', 'Migrate a JavaScript monorepo to pnpm workspaces and resolve conflicting peerDependencies (e.g., React 18 and TypeScript 5) by editing package.json ranges and adding pnpm.overrides to pin a single compatible set. Generate a deterministic pnpm-lock.yaml, install with a frozen lockfile, and validate by building all packages and running a provided cross-package integration script.', NULL, ARRAY['javascript', 'typescript']),
('Build & Dependency Management', 'Dependency Management', 'Dependency Installation & Version Control', 'Resolve peerDependency conflicts in a Yarn v3 Plug''n''Play monorepo by using resolutions and packageExtensions to align versions of React, Webpack loaders, and ESLint plugins. Produce a reproducible install (yarn.lock) and verify the workspace builds and lints successfully under Node 20.', NULL, NULL),
('Build & Dependency Management', 'Dependency Management', 'Dependency Resolution & Conflict Fixing', 'Diagnose and fix a Rust project’s build failure caused by a dependency tree pulling in both native-tls (openssl-sys) and rustls while the host OpenSSL version is incompatible. Align Cargo features and versions (e.g., select rustls-tls, enable vendored OpenSSL, or install the correct libssl-dev) so cargo build and the test suite complete successfully.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Dependency Management', 'Dependency Resolution & Conflict Fixing', 'Fix a Rust Cargo workspace that fails to build due to conflicting versions and feature flags among tokio, hyper, and reqwest by aligning version pins and enabling compatible features in Cargo.toml without adding or removing dependencies. Ensure cargo build --locked succeeds across all workspace members with the provided toolchain.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Dependency Management', 'Dependency Resolution & Conflict Fixing', 'In a Rust Cargo workspace, diagnose and resolve a build failure caused by conflicting serde/serde_json versions and incompatible feature requirements across multiple member crates and a pinned transitive dependency. Modify only Cargo.toml constraints (workspace dependency versions, [patch.crates-io], and feature flags) so that cargo build completes without changing source code or removing dependencies.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Dependency Management', 'Dependency Resolution & Conflict Fixing', 'Resolve a Rust Cargo workspace that fails to compile due to transitive version/feature conflicts between reqwest, openssl-sys, and tokio plus a system OpenSSL 1.1 vs 3.0 mismatch. Pin compatible crate versions, adjust features and Cargo patches, configure pkg-config to link the intended OpenSSL, vendor dependencies for offline build, and validate by building and running a CLI that performs an HTTPS request.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Dependency Management', 'Dependency Resolution & Conflict Fixing', 'Resolve a Rust Cargo workspace where crates pull in incompatible TLS stacks (native-tls via openssl-sys pinned for OpenSSL 1.1 vs rustls) causing builds to fail against system OpenSSL 3.x. Unify feature flags and version pins (or enable openssl "vendored") so cargo build --workspace succeeds and the provided HTTPS CLI test runs successfully.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Dependency Management', 'Lockfile & Manifest Maintenance', 'In a Rust workspace with crates using branch-based git and wildcard semver dependencies, pin each to exact versions and commit SHAs in Cargo.toml and regenerate a deterministic Cargo.lock. Verify reproducibility by building with cargo build --frozen in a clean state, ensuring no yanked or platform-specific mismatches remain in the lockfile.', NULL, ARRAY['rust', 'git']),
('Build & Dependency Management', 'Dependency Management', 'Lockfile & Manifest Maintenance', 'Regenerate deterministic lockfiles for a polyglot monorepo—Python (pip-compile with --generate-hashes), Node (npm@10 package-lock v3), and Rust (Cargo.lock)—replacing version ranges with exact pins and eliminating platform-specific drift. Verify by performing two clean, offline installs that yield byte-for-byte identical dependency trees and checksums.', NULL, ARRAY['python', 'rust']),
('Build & Dependency Management', 'Dependency Management', 'Lockfile & Manifest Maintenance', 'Regenerate fully hashed requirements.txt and requirements-dev.txt from a pyproject.toml that includes extras and environment markers, using pip-tools pinned to Python 3.11 and honoring an existing constraints.txt without pulling in optional extras. Verify that pip install --require-hashes succeeds entirely offline from the provided wheelhouse and fails if any dependency would need the network.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Dependency Management', 'Lockfile & Manifest Maintenance', 'Regenerate reproducible Python lockfiles (with hashes) from a pyproject.toml that includes extras, environment markers, a VCS-pinned dependency, and a local path package using pip-tools, updating the manifest as needed for Python 3.12 on Debian bookworm. Validate by installing offline from a provided wheelhouse with pip --require-hashes and verifying that re-locking with the same SOURCE_DATE_EPOCH produces identical files.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Dependency Management', 'Lockfile & Manifest Maintenance', 'Resolve peerDependency conflicts in a Yarn v3 workspaces monorepo by aligning manifest ranges and regenerating a deterministic yarn.lock with Corepack enabled so yarn install --immutable succeeds on Linux. Prove reproducibility by performing two clean installs that yield identical resolved versions and an unchanged install-state file.', NULL, NULL),
('Build & Dependency Management', 'Dependency Management', 'System vs. Project Dependency Isolation', 'Build OpenSSL 1.1.x from source into a project-local prefix and compile a provided C utility to link exclusively against it (not the system OpenSSL), embedding an rpath to the local lib directory. Prove isolation by showing ldd resolves libssl/libcrypto from the project prefix and the program prints the 1.1.x version while the system ''openssl version'' remains unchanged.', NULL, NULL),
('Build & Dependency Management', 'Dependency Management', 'System vs. Project Dependency Isolation', 'Create a Python project-local virtual environment at /app/.venv and use pip-tools to lock dependencies so the project installs NumPy 1.24.x and a compatible SciPy while the system Python retains NumPy 2.x. Prove isolation by printing NumPy versions from both interpreters and successfully running a bundled script that relies on APIs removed in NumPy 2.0.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Dependency Management', 'System vs. Project Dependency Isolation', 'Initialize an R project with renv to create a project-local library and lockfile, installing specific versions of data.table and ggplot2 that differ from the system-wide packages. Run the provided R script using the isolated library and verify at runtime that .libPaths()[1] resolves to the renv project path, not the system library.', NULL, NULL),
('Build & Dependency Management', 'Dependency Management', 'System vs. Project Dependency Isolation', 'Pin a per-project Rust toolchain via rust-toolchain.toml and vendor all Cargo dependencies into ./vendor, configuring .cargo/config.toml so the crate builds fully offline without touching global cargo/rustup caches. Verify isolation by clearing ~/.cargo and ~/.rustup, setting CARGO_HOME to a temp dir, disabling network, and successfully building and running the tests.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'C/C++ & Systems Builds', 'Build a versioned shared library (e.g., libmathshim.so.1.0) with a proper SONAME and install a matching pkg-config file, then compile a consumer executable that discovers it via pkg-config and embeds a $ORIGIN-based RUNPATH for relocatable execution. Verify by relocating the bin and lib into a new directory tree and demonstrating the program runs and ldd shows RUNPATH without needing LD_LIBRARY_PATH.', NULL, NULL),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'C/C++ & Systems Builds', 'Configure a CMake-based C project to produce bit-for-bit reproducible builds by enabling deterministic archives and stripping build paths (e.g., SOURCE_DATE_EPOCH, ar D, -ffile-prefix-map/-fdebug-prefix-map) with split DWARF. Build twice in different directories and verify identical SHA-256 digests while ensuring no absolute build paths remain in the binary.', NULL, NULL),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'C/C++ & Systems Builds', 'Create a CMake superbuild that compiles vendored zlib and libpng via ExternalProject_Add and then builds a C++ CLI (pngdim) that links them. Ensure correct SONAME and RUNPATH so the produced binary runs in the sandbox and transforms a provided PNG to /app/out.png with an expected SHA256.', NULL, NULL),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'C/C++ & Systems Builds', 'Create and install a C shared library with GNU symbol versioning and proper SONAME/symlink layout (e.g., libcalc.so.2) plus a pkg-config file, exporting both v1 and v2 of an API via a linker version script. Build two test programs (one expecting v1, one v2) and verify via readelf/nm and runtime execution that both resolve correctly against the single installed library.', NULL, ARRAY['api']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'C/C++ & Systems Builds', 'Refactor a small C library to use modern CMake, installing both shared and static variants along with a FooConfig.cmake export and a pkg-config foo.pc that correctly express include and link interfaces. Build a separate consumer in a clean environment using find_package(Foo CONFIG) and pkg-config, run it to generate /app/out.txt, and verify correct SONAME and RPATH on the linked binary.', NULL, NULL),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Java & JVM-Based Builds', 'Build a Java 17 multi-module Maven project (app, core, provider) as JPMS modules, configuring the compiler and JAR plugins to produce modular artifacts and using jlink to assemble a custom runtime image at /app/dist. Validate that the image’s launcher runs the CLI and that ServiceLoader discovers the provider implementation at runtime.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Java & JVM-Based Builds', 'Configure a multi-module Gradle (Kotlin DSL) Java/Kotlin project to build offline against a pre-seeded local Maven repository, enable dependency locking and verification, and produce a deterministic shaded fat JAR targeting JDK 17. Verify by building twice with the same SOURCE_DATE_EPOCH to obtain identical SHA256 sums and by running the JAR to print its git-derived version.', NULL, ARRAY['java', 'git']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Java & JVM-Based Builds', 'Configure and build a mixed-language (Java + Kotlin) multi-module Maven project using toolchains to compile one legacy module with JDK 8 and the rest with JDK 17, wiring Lombok and MapStruct as annotation processors on the correct processor path. Produce a deterministic, shaded CLI artifact with relocated dependencies and attached sources/javadoc, publish to the local Maven repository, and verify by building and running a tiny consumer project that imports the library and executes the CLI.', NULL, ARRAY['java', 'rest']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Java & JVM-Based Builds', 'Create a Gradle multi-module Java build that replaces a transitive dependency with a local fork via dependency substitution, then produces a reproducible shaded (relocated) fat JAR with deterministic timestamps and entry order. Validate that unit tests pass, the runnable JAR executes without classpath conflicts, and two builds with the same SOURCE_DATE_EPOCH are bit-for-bit identical.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Java & JVM-Based Builds', 'Refactor a Maven multi-module Java project to use a Bill of Materials (BOM) and the Maven Enforcer plugin to resolve transitive version conflicts, then configure the maven-shade-plugin to build a reproducible, executable app JAR with relocated Guava classes and a proper Main-Class manifest. Validate by running the JAR to produce the expected output and by proving two consecutive builds are byte-identical (same SHA256) when SOURCE_DATE_EPOCH is set.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'JavaScript & Frontend Builds', 'Convert a TypeScript library to a dual-module build that emits ESM and CommonJS bundles with Rollup or tsup, generating declaration files and source maps, and configure package.json exports/typings so both import and require work. Verify by running two provided consumers (Node ESM and CJS) and by ensuring the ESM bundle is tree-shakeable and under a specified size budget.', NULL, ARRAY['typescript']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'JavaScript & Frontend Builds', 'Create a Yarn 3 Plug''n''Play workspace that builds a TypeScript library and a Node.js CLI via Rollup, emitting dual ESM/CJS outputs plus .d.ts, with zero node_modules on disk. Validate by running the CLI through the PnP loader to consume the workspace library and by proving two builds are byte-for-byte identical when SOURCE_DATE_EPOCH is fixed.', NULL, ARRAY['typescript']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'JavaScript & Frontend Builds', 'Migrate a legacy CommonJS React app to an ESM-based toolchain using Vite and TypeScript, resolving peer dependency/version conflicts and tsconfig path aliasing so the project compiles and runs. Produce an optimized, code-split production build with hashed filenames and verify that tree-shaking removed a specified debug utility from the final bundles.', NULL, ARRAY['typescript']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Python Builds & Packaging', 'Package a Python project containing a C extension into a manylinux2014 wheel using PEP 517 (pyproject.toml with setuptools), statically linking its vendored C library and repairing the artifact with auditwheel to be self-contained. Build sdist and wheel to dist/, then install from those artifacts in a fresh virtualenv and verify both module import and a console_script entry point’s expected output.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Python Builds & Packaging', 'Refactor a Cython-based Python project to a PEP 517 build with setuptools, ensuring generated C sources are included in the sdist so pip install from the sdist works without Cython present. Build reproducible sdist and wheel (respecting SOURCE_DATE_EPOCH), then verify in a clean virtualenv by installing only from the sdist and running the provided import/CLI smoke tests.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Rust/Go/Other Modern Toolchains', 'Convert a Rust workspace to build fully offline by vendoring all dependencies and configuring Cargo source replacement, pinning a git-sourced crate via [patch.crates-io] to a specific commit to resolve a version conflict. Build for both the host target and wasm32-wasi, run native tests, and produce reproducible artifacts with a locked Cargo.lock.', NULL, ARRAY['rust', 'git']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Rust/Go/Other Modern Toolchains', 'Create a reproducible offline build for a Rust workspace by using cargo vendor and [patch.crates-io] to pin a dependency to a specific git revision, then compile the project for both x86_64-unknown-linux-gnu and wasm32-unknown-unknown using wasm-bindgen with TypeScript bindings emitted to fixed output paths. Validate by running native cargo tests and a Node.js script that loads the generated WebAssembly to confirm an exported function returns the expected value.', NULL, ARRAY['rust', 'git', 'typescript']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Rust/Go/Other Modern Toolchains', 'Refactor a provided multi-crate Rust project into a Cargo workspace, vendor all dependencies with cargo vendor, and configure .cargo/config.toml so the project builds fully offline. Cross-compile a static x86_64-unknown-linux-musl release binary with feature "cli" (default-features=false) and verify by running cargo test --workspace and executing the binary to emit a deterministic SHA256.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Rust/Go/Other Modern Toolchains', 'Vendor all Cargo dependencies for the provided Rust CLI, switch TLS features from OpenSSL to rustls, and cross-compile a fully static x86_64-unknown-linux-musl binary in strict offline mode. Validate by proving no network access during build, ldd reports it is not a dynamic executable, and repeated builds with the same SOURCE_DATE_EPOCH produce identical SHA256 sums.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Language & Ecosystem-Specific Build Management', 'Rust/Go/Other Modern Toolchains', 'Vendor all crates for a multi-crate Rust workspace and configure Cargo for a fully offline build using a provided local registry mirror; resolve a feature-induced dependency graph conflict without adding or removing crates, then cross-compile the workspace for x86_64-unknown-linux-gnu and wasm32-wasi. Run tests for the native target, execute the WASI binary with wasmtime to verify expected stdout, and write artifact sizes and resolved crate versions to /app/build-metadata.json.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Branch-Based Build Rules', 'Create a Git-aware build pipeline for a small CMake project where branch names control behavior: main builds produce a reproducible, stripped, LTO-enabled tarball with SBOM and version from the latest tag; release/* builds append -rcN; feature/* builds are debug, include an experimental module, and are blocked from publish. Validate by switching branches/tags to confirm artifact names, embedded --version/--info metadata, and that publish is refused on feature/*.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Branch-Based Build Rules', 'Create a Git-backed project whose Makefile/build script alters outputs by branch/tag: main and tags v* produce stripped, versioned release tarballs; staging builds include debug symbols and a visible STAGING banner; feature/* builds emit -SNAPSHOT artifacts and block the release target. Enforce these rules via branch-aware build logic and lightweight Git hooks/CI entrypoints, with tests that checkout branches/tags and verify artifact names, flags, and refusal behavior.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Branch-Based Build Rules', 'Create a branch-aware build script for a small Go CLI that enforces different behaviors per branch: main builds are stripped and versioned from the latest tag, feature/* builds enable -race and debug info with a -canary+<shortSHA> suffix, and release/* builds require a matching CHANGELOG entry and emit both a SHA256SUMS file and a CycloneDX SBOM. Validate by creating and checking out branches and running the script to produce and verify the expected artifacts and metadata.', NULL, NULL),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Branch-Based Build Rules', 'Implement a Git-aware Makefile for a Rust CLI that enforces branch-specific builds: main produces a stripped LTO release with version from the latest tag and a sha256 checksums.txt; staging enables a telemetry-staging feature and adds an -rc suffix; feature/* appends +branch.sha and writes to /app/dist/feature, while release/* fails if Cargo.toml version doesn’t match the tag. The harness will check out different branches and verify artifact names, --version output, and the presence of checksums only on main.', NULL, ARRAY['git', 'rust']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Branch-Based Build Rules', 'Implement a Git-aware build script that enforces branch-specific rules: feature/* runs tests only and emits a -SNAPSHOT artifact, staging builds produce debug binaries with embedded staging config, and main requires an annotated tag to create a stripped release tarball with semver from the tag and a SHA256 checksum in /app/dist. Validate by creating branches and tags and showing the correct artifacts and version strings are generated for each branch.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Signed & Verified Builds', 'Create an offline release pipeline that signs a Git tag for a provided source tree, builds a normalized tar.gz and binary, and generates SHA256SUMS along with both GPG (.asc) and minisign signatures for the artifacts. Provide a verify.sh that validates the tag signature against a supplied public key, checks checksums and signatures, and confirms the binary’s embedded commit hash matches the signed tag.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Signed & Verified Builds', 'Implement a release pipeline that signs the vX.Y.Z Git tag and a SHA256SUMS file with GPG, builds a binary embedding git describe/commit hash, and provides verify.sh to validate the tag signature, the embedded version against the tag, the checksum signature, and the artifact hashes. The task must detect and fail on any tampering with either artifacts or checksums.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Signed & Verified Builds', 'Implement a release workflow for a small Rust CLI where an annotated, signed Git tag triggers building a deterministic static binary, packaging it into a tar.gz, generating SHA256SUMS, and creating detached GPG signatures for both the tag and checksum file in a temporary GNUPGHOME. Provide a verify-release.sh that, from a clean clone, validates the tag signature, checksums, and that the binary prints the tagged version; the release must fail if HEAD is not a signed tag or any verification step fails.', NULL, ARRAY['rust', 'git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Signed & Verified Builds', 'Implement a release workflow that only builds from an annotated, GPG-signed git tag: produce a deterministic git-archive tar.gz, compute SHA256SUMS, and create detached ASCII-armored signatures for both the tarball and the checksum file using a provided maintainer key. Import only the supplied public key to verify the tag signature and both artifact signatures; emit to /app/dist only if all verifications succeed and the archive reproduces bit-for-bit under a fixed SOURCE_DATE_EPOCH.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Signed & Verified Builds', 'Implement an offline release workflow that builds a Go project at a signed git tag, produces a deterministic tar.gz of the binary, emits SHA256SUMS, and creates detached GPG signatures for the tag, the tarball, and the checksum file. Provide a verification script that imports the public key, verifies the tag and artifact signatures, validates the checksums, and checks an in-toto provenance attestation proving the tarball was produced from the tagged commit with the specified build command.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'Create a POSIX shell release script that parses Conventional Commits since the last tag to compute the next semantic version, updates a version file, generates/updates CHANGELOG.md, creates an annotated git tag, and pushes tags to a provided local bare remote. Validate by running the script twice on a seeded repo to produce v1.1.0 then v1.1.1 and confirm the tags, changelog sections, and versioned build artifact output match the computed versions.', NULL, ARRAY['git']);
-- Compact batch 3/29: rows 101-150

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'Create a portable release script that parses Conventional Commits since the last git tag to compute the next semver (major/minor/patch), updates a VERSION file and CHANGELOG.md, then creates an annotated tag (vX.Y.Z). On success, build the project to dist/, archive it as project-vX.Y.Z.tar.gz with SHA256SUMS, and refuse to run on a dirty tree while verifying the changelog covers exactly the tagged commit range.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'Implement a release pipeline for a two-package monorepo (Python lib at libs/pycalc and Go CLI at apps/gocalc) that reads Conventional Commits since each package’s last namespaced tag to compute the next semantic version per package. Update pyproject.toml/go.mod, generate a combined CHANGELOG.md with per-package sections, create annotated tags pycalc-vX.Y.Z and gocalc-vA.B.C only for changed packages, and place built artifacts plus SHA256 checksums in dist/.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'Implement a repo-local release automation that, using Conventional Commits, calculates the next semantic version since the last git tag, updates version fields in both package.json and pyproject.toml, regenerates CHANGELOG.md, creates an annotated tag, and produces tarball artifacts with a SHA256SUMS file. Validate by seeding example commits, invoking the release script twice, and asserting that tags, changelog sections, and checksums match expected outputs.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'In a Git monorepo with a Rust workspace of multiple crates, implement a release script that parses Conventional Commits since the last per‑crate tag, bumps versions in Cargo.toml/Cargo.lock with dependency propagation, writes per‑crate and aggregate changelogs, and creates annotated tags (v{crate}-{version}) plus a workspace tag. The run must support dry‑run vs apply, handle prerelease channels (e.g., -beta) and BREAKING CHANGE semantics, be idempotent on a second run, and leave the workspace building successfully at the new versions.', NULL, ARRAY['git', 'rust']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Build the provided C++ library into both a versioned shared object (with correct SONAME symlinks) and a static archive, and generate pkg-config and CMake package config files while staging headers and licenses under a prefix. Package the staged tree into a relocatable tar.gz release and verify by compiling a tiny consumer that locates the library via pkg-config and find_package.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Configure a CMake project and use CPack to produce both .deb and .rpm packages for a small C/C++ CLI, including /usr/bin binary, a man(1) page, bash completion, and a separate -dbg package with split debug symbols. Validate package metadata (version from git tag), ownership/permissions, dependencies, and contents via dpkg-deb and rpm queries, and by extracting to a temporary root to confirm correct install paths.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Convert the provided Java library into an OSGi-compliant bundle and produce reproducible release artifacts: the main .jar with correct Bundle-* MANIFEST headers plus -sources.jar and -javadoc.jar. Publish them to a local Maven repository and verify a sample consumer project resolves the bundle and that bnd/osgi-info validates the metadata.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Develop a small C library using CMake with correct SONAME/versioned symlinks and a pkg-config .pc file, then produce release artifacts: a tar.gz containing headers and the shared library and distribution packages (.deb and .rpm) splitting runtime and -dev contents. Validate by building and running a tiny consumer program that uses pkg-config to link against the installed package and confirms the correct SONAME is loaded at runtime.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Package a small C library built with CMake into two Debian packages: a versioned runtime shared library with correct SONAME (e.g., libfoo2) and a -dev package containing headers and a pkg-config file, using debhelper and dpkg-buildpackage. Install the .debs and verify by compiling and running a tiny consumer that links via pkg-config against the installed library.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Configure a CMake superbuild that fetches and builds pinned zlib and libpng from source via FetchContent/ExternalProject, then builds a small C/C++ image utility that links to them. Install targets with an exported CMake package (MyImgToolConfig.cmake), verify find_package works from a separate minimal project, and run the utility on a provided PNG sample.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Configure a CMake-based build for /app/engine using Ninja, CMakePresets.json, and a cross-compiling toolchain file to produce both native (x86_64) and ARMv7 binaries with an ENABLE_SIMD option that toggles sources and compile definitions. The workflow must emit compile_commands.json, run ctest, and generate relocatable cpack TGZ packages for each configuration, verifying the ARM artifact is an ARM ELF and the native build links against system zlib via find_package(ZLIB).', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Configure a CMake/Ninja project to run a two-stage Profile-Guided Optimization workflow: first build instrumented binaries and execute a provided training script to generate profiles, then rebuild using those profiles to emit an optimized executable at a fixed path. Verify that the profile was consumed (via build logs/artifacts) and that the optimized binary demonstrates a measurable runtime improvement over a baseline build.', NULL, ARRAY['optimization']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Create a CMake superbuild using ExternalProject_Add to compile vendored zlib and libpng from local tarballs with Ninja, then build a top-level C++ library and CLI that link them and install to /opt/app with correct RPATH. Emit pkg-config and CMake export targets, and verify by running the CLI to convert a provided PNG into a raw byte dump without network access.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Replace the project''s Autotools build with Meson + Ninja, adding a subproject wrap for a missing dependency, installing headers, and generating a pkg-config file, and then add a Meson cross file to support armv7hf cross-compilation with hard-float. Verify by building native and cross variants, running the native tests, and using qemu-arm to execute a sample program linked against the cross-built shared library while pkg-config resolves the correct paths.', 'hard', ARRAY['compilation']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile a CGO-based Go CLI that embeds SQLite into static binaries for linux/amd64, linux/arm64, linux/riscv64, and windows/amd64 using zig as the cross C compiler/linker. Verify by running the riscv64 binary under qemu-user to execute a SQL query and confirming the Windows PE binary via wine or PE header inspection outputs the exact expected result.', NULL, ARRAY['sql']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile a provided C/C++ utility into static aarch64 and riscv64 Linux binaries using a musl-based toolchain (e.g., Zig as cc). Validate by running both under qemu-user to process the same input and write matching SHA256 output hashes to a verification file.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile a provided Rust CLI into x86_64-unknown-linux-musl and aarch64-unknown-linux-musl static binaries plus an x86_64-pc-windows-gnu .exe using cargo with the appropriate cross-linkers. Verify outputs by running the native binary directly, the aarch64 binary under qemu-aarch64, and the Windows .exe under wine, asserting exact stdout including the embedded target triplet in --version.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile the provided CMake-based CLI to produce three artifacts: x86_64-linux-musl (fully static), aarch64-linux-gnu, and x86_64-w64-mingw32. Verify correctness by inspecting ELF/PE headers and running the ARM64 build under qemu-aarch64 and the Windows build under wine, ensuring the musl binary has no glibc dependency and all builds honor a fixed SOURCE_DATE_EPOCH.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile the ripgrep Rust project into statically-linked binaries for x86_64-unknown-linux-musl and aarch64-unknown-linux-musl from an x86_64 host, packaging each artifact with a LICENSE file and SHA256 checksum. Validate the ARM64 build by executing it under qemu-aarch64 on a provided test corpus and verifying the expected grep results.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Implement and manually compile a Java+JNI demo: generate headers with javac -h, compile the C code into libnativeops.so using gcc -fPIC -shared with the correct JDK include paths, and compile the Java class. Verify by running the Java harness with -Djava.library.path to confirm correct native results and by inspecting exported JNI symbols with nm/objdump.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Manually compile a C shared library and a Java program that calls it via JNI using only javac/java and gcc/clang, correctly setting include paths, -fPIC/-shared, and runtime library paths (java.library.path or LD_LIBRARY_PATH). Execute the Java program to verify the native method loads and returns an exact computed result.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Manually compile a modular Java application using javac, then assemble a minimized custom runtime image with jlink that runs the app without relying on a system JRE. Validate the image by launching the program and confirming only required modules are included with debug symbols and locales stripped.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Manually compile a small Java project into a multi‑release executable JAR: build base classes with javac --release 8, compile alternate implementations for Java 11 into META-INF/versions/11, and assemble with a manifest declaring Multi-Release: true and Main-Class using only javac and jar. Verify by running the JAR to see Java‑11‑specific behavior and by listing the archive to confirm versioned entries.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Perform a two-phase profile-guided optimization build of the C program in /app/pgotask: first compile with -fprofile-generate and run it over the input corpus in /app/corpus to emit profiles, then recompile with -fprofile-use to produce /app/bin/app_pgo. Verify by timing both pre- and post-PGO binaries on the same workload and writing the speedup and file sizes to /app/pgo_report.txt.', NULL, ARRAY['optimization']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Build a pipeline that scans /app/logs for mixed-format Apache/Nginx logs (plain, .gz, .bz2, .zst), parses into a normalized schema, enriches with country and ASN via provided lookup CSVs, applies prefix-preserving IP anonymization, and outputs a SQLite database plus per-day aggregates (CSV and Parquet).', NULL, ARRAY['database']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Create a script that ingests a mixed drop folder of .csv.gz, newline-delimited .json, and .xlsx transaction files, normalizes columns and timestamps to UTC, converts currencies using /app/rates.toml, and de-identifies emails by hashing with a SALT env var. Load the cleaned data into a SQLite database with indexes and emit per-day partitioned CSVs under /app/out plus a manifest.json of row counts and SHA256 checksums.', NULL, ARRAY['database']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Create a script that ingests mixed Apache/Nginx logs under /app/logs, normalizes timestamps to UTC, de-duplicates requests, and stitches events into user sessions using IP+User-Agent with a 30-minute inactivity threshold. Load results into /app/sessions.db (SQLite) with sessions and events tables, and write /app/landing_pages.csv listing the top 10 landing pages with session counts and bounce rate.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Create a script that ingests mixed-format web server access logs from /app/logs (plain .log, .gz, .bz2 in Common/Combined Log Format), parses and normalizes timestamps to ISO-8601 UTC, enriches with country via /app/GeoLite2-Country.mmdb, and de-duplicates by (ip, timestamp, request). Write partitioned, snappy-compressed Parquet files with a fixed schema to /app/out/year=YYYY/month=MM/day=DD.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Implement an idempotent incremental ETL that extracts customer transactions from CSV files in /app/inbox/, newline-delimited JSON in /app/stream/, and a local mock REST endpoint, normalizes timestamps/timezones and converts currencies to USD using a provided FX rates file, and de-duplicates near-duplicates. Load results into a SQLite star schema, persist a checkpoint so only new or changed data is processed on re-run, and write a data-quality report of nulls, duplicate keys, and outliers to /app/reports/.', NULL, ARRAY['rest']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Build a Makefile-driven ETL that ingests JSONL.gz logs from /app/raw, validates/normalizes them, partitions to date-based Parquet, aggregates per-endpoint metrics, and emits a CSV plus HTML report. The DAG must be incrementally rebuildable using checksum stamps so only affected partitions and downstream artifacts are recomputed, with targets for full run, incremental update, and clean.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Create a Makefile-driven ETL that reads a manifest of resources (URL, filename, sha256), downloads and verifies them, normalizes mixed CSV/TSV/JSONL into a common JSONL schema, merges/deduplicates by timestamp, and outputs final.jsonl plus a machine-readable provenance report. The workflow must use stamp files and checksum-based invalidation so reruns are no-ops when inputs are unchanged, and touching any single source triggers only the necessary downstream steps.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Create a Makefile-driven ETL that verifies and decompresses /app/logs/*.log.gz, normalizes records to JSONL, partitions by day, and generates daily summaries plus a SHA256 manifest of inputs/outputs. The DAG must be idempotent and incremental (reruns only when dependencies change) and provide targets all, day-YYYYMMDD, and clean.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Create a tiny DAG runner that reads /app/dag.yaml and executes a 5-stage local ETL (extract, normalize, dedupe, aggregate, report) with dependency-aware incremental rebuilds using content hashes, per-task timeouts/retries, max-parallelism, and a dry-run mode. The harness mutates a single source file and expects only downstream nodes to re-run, emitting updated outputs plus a provenance log and a DOT graph of the executed sub-DAG.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Implement a lightweight DAG runner that reads /app/pipeline.yml (tasks with command, inputs, outputs, and depends) and executes them in topological order with a max concurrency of 2, skipping tasks whose outputs are up-to-date and retrying failures once. Write an execution log to /app/run.log and concatenate the outputs of all leaf tasks into /app/build/summary.txt.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Python/Perl/Ruby Scripting', 'Implement a Python CLI that reads /app/pipeline.yaml to run a small DAG-based ETL over mixed CSV/JSON/TSV in /app/data: normalize schemas and timestamps to UTC, join with /app/exchange_rates.json to convert currencies to USD, and write a partitioned Parquet dataset to /app/warehouse plus a Markdown summary report. The runner must compute content hashes to skip unchanged steps, execute dependencies in parallel, and emit a reproducibility manifest of inputs/outputs at /app/manifest.json.', NULL, ARRAY['python', 'parallel']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Python/Perl/Ruby Scripting', 'Implement a Python tool that scans /app/config for JSON, YAML, TOML, and INI files, expands ${ENV_VAR:-default} and ${config.some.path} references with type-aware coercions (e.g., durations like 5m, sizes like 1.5GB), merges them by a specified priority, and validates against /app/schema.json (JSON Schema). Output a normalized config.json and a resolution_report.json detailing per-key source precedence, coercions, defaults applied, and any unresolved references.', NULL, ARRAY['python']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Python/Perl/Ruby Scripting', 'Write a Python script that reads a workflow YAML at /app/workflow.yaml defining tasks (id, command, deps, env, max_retries, timeout), executes the DAG in dependency order with retries/timeouts, and captures per-task stdout/stderr to /app/logs/<id>.log. Output /app/report.json summarizing each task’s status (success/failed/skipped), attempt count, start/end timestamps, exit code, and a topologically sorted execution list.', NULL, ARRAY['python']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Shell Scripting & CLI Automation', 'Create two Bash scripts: backup.sh performs idempotent, incremental backups of /app/data into date-stamped snapshots using rsync --link-dest with per-snapshot SHA256 manifests and a flock-based lock; prune.sh enforces a configurable GFS retention policy, verifies snapshot integrity before deletion, and writes a JSON report of kept/removed snapshots and verification results.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Shell Scripting & CLI Automation', 'Write a Bash script that organizes media files from /app/photos into a date-based library at /app/library/YYYY/MM/DD by reading capture timestamps from EXIF/QuickTime metadata (fall back to file mtime), renames files to timestamp_shortsha.ext, and de-duplicates exact content by SHA-256. Produce /app/manifest.csv with header original_path,new_path,taken_at,sha256,mime and make the workflow idempotent so re-running makes no changes.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Task Scheduling & Cron Jobs', 'Create two cron jobs: one runs every 10 minutes to promote ''stable'' CSVs from /app/incoming to /app/staging (unchanged for ≥90s), the other runs at 00:05 UTC to consolidate all staged CSVs into a deduplicated, timestamp-sorted daily report at /app/reports/YYYY-MM-DD.csv.gz. Use flock to prevent overlaps, write last-run metadata as JSON to /app/status.json, and provide a one-shot script to trigger both jobs immediately for verification.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Task Scheduling & Cron Jobs', 'Set up a cron-orchestrated ETL with three scripts: every 2 minutes ingest and idempotently upsert JSON files from /app/inbox into SQLite, hourly export a rolling 24h metrics CSV to /app/out/report.csv and prune old rows, and daily rotate/compress processed inputs to /app/archive. Enforce flock-based locking, CRON_TZ=UTC, and a catch-up mechanism that detects downtime and replays missed windows.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Task Scheduling & Cron Jobs', 'Write a backup script that snapshots /app/db.sqlite to /app/backups as timestamped compressed files, retaining the last 7 daily and last 4 Sunday weekly backups. Install a cron.d entry to run it at 02:17 UTC daily with flock-based locking and append-only logging to /app/backup.log.', NULL, ARRAY['logging']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Build a script that ingests mixed-format web access logs (CSV and JSONL), normalizes them to a single schema, sessionizes requests per client with a 30-minute inactivity timeout, and computes per-session aggregates (request count, duration, total bytes). Output a compact session table and a brief text summary with top landing paths and percentile session lengths.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a command-line script that scans /app/logs for web access logs (plain or .gz), normalizes URLs by collapsing numeric IDs/UUIDs to placeholders, and aggregates per-normalized-endpoint metrics (request count, total bytes, median/p95/p99 latency, and 5xx error rate) for a specified UTC day. Output a 2-space-indented JSON array sorted by descending request count to /app/endpoint_metrics.json.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a script that ingests a directory of mixed-format telemetry files (CSV and JSONL) with heterogeneous timestamp formats, normalizes them to UTC minute buckets, and deduplicates events by (device_id, metric, ts). Aggregate per device and metric to produce hourly stats (count, mean, median, p95, min, max) and write aggregates.parquet plus a concise anomalies.csv listing hours with data coverage <80%.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a script that reads /app/points.parquet of GPS pings, converts timestamps to UTC and rounds to 15-minute buckets, and bins lat/lon into 7-character geohashes. Remove per-cell daily outliers using median absolute deviation and write /app/aggregates.csv with rows (bucket_start, geohash7, count, distinct_src, mean_value, min_value, max_value) sorted by bucket then geohash.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a terminal pipeline that scans /app/logs for web-event files in CSV, JSONL, and .jsonl.gz, normalizes fields (utc_ts, user_id, url, status, bytes), and computes 5-minute rollups with totals, distinct users, bytes p50/p95/p99, and the top-3 URLs by count. Write /app/rollup.parquet and /app/top_urls.csv, and support an incremental mode that updates only new windows using /app/watermark.txt.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that merges a product catalog CSV (product_id, sku, upc), a price events NDJSON (sku, price, timestamp), and a UPC alias TSV by normalizing UPCs and resolving aliases to canonical IDs, then joining on sku/upc. Output a consolidated CSV per product_id with latest price and count of price events, plus an unmatched.csv listing any price events that could not be linked.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that merges an Apache access_log (CLF/combined) and an application event stream (JSON Lines) into a single CSV by first joining on a shared request_id when available; if absent, fall back to the nearest timestamp within ±2 seconds for the same client IP and URL path, and mark which join strategy was used. Output must contain one row per web request with selected fields from both sources and unmatched records discarded.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that merges three datasets—orders.csv, refunds.csv, and chargebacks.jsonl—into a single per-order report by joining on order_id (exact) and de-duping conflicting events by the latest event_timestamp. Compute each order’s final_status (Pending/Fulfilled/Refunded/Chargeback) and net_amount, and write a sorted CSV with a fixed header.', NULL, NULL);
-- Compact batch 4/29: rows 151-200

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that performs a temporal as-of join across three datasets—ad impressions (CSV), clicks (JSONL), and conversions (Parquet)—to attribute each conversion to the most recent prior click (≤7 days) and impression (≤24 hours before the click) for the same user_id with deterministic tie-breaking. Write a unified Parquet with attribution fields and a JSON report listing conversions with ambiguous or missing joins.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Join network event logs with IP ownership datasets by performing longest-prefix-match containment over IPv4/IPv6 CIDR ranges, resolving overlaps via most-specific prefix and source priority. Produce an enriched events output annotated with org/asn and a summary report of per-org counts and unmatched coverage.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Create a command-line script that streams a gzip-compressed NDJSON file at /app/events.ndjson.gz, selecting only records within a given UTC time window whose URL host appears in /app/allowlist.txt while enforcing a per-user cap K (discarding excess events per user in-window). Output the retained events as a stable-order, 2-space-indented JSON array to /app/filtered.json, preserving original key order.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Create a streaming CLI that scans /app/events for newline-delimited JSON files (.jsonl, .jsonl.gz, .jsonl.zst), filters records by a UTC time window, a regex on event_type, and an email-domain whitelist from /app/allowed_domains.txt, and outputs filtered.jsonl with only selected fields in input order. Also write rejected.jsonl logging each discarded record’s id and machine-readable reason, ensuring constant-memory processing without loading entire files.', NULL, ARRAY['logging']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Implement a command-line tool that reads one or more COCO-format annotation JSONs in /app/dataset and filters annotations to categories listed in /app/categories.txt while enforcing bbox area and score thresholds from /app/filter.yaml, dropping any now-orphaned images. Output a pruned COCO JSON with deterministically ordered, remapped contiguous ids and a newline manifest of kept image file paths.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Write a script that scans /app/logs for Apache/Nginx access logs (plain text and .gz), filters entries whose client IP falls within any CIDR in /app/eu_cidrs.txt, whose request path matches any pattern in /app/paths.txt, and whose status is 2xx. Output /app/filtered.csv with ISO8601 timestamp, ip, method, and path, deduplicated per ip+path within rolling 10‑minute windows.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Build a CLI that scans /app/datasets for CSV/TSV and JSONL files with mixed encodings, delimiters, header spellings, and timestamp/number formats; auto-detects encoding and delimiter, canonicalizes Unicode (NFKC), maps header synonyms to a target schema (user_id, event, ts_utc, amount), and standardizes timestamps to ISO 8601 UTC and numbers to dot-decimal, writing RFC4180 UTF-8 CSVs to /app/normalized. Emit /app/audit.json summarizing per-file detection results, header mappings, and counts of fixed/dropped rows with a small before/after sample of transformed values.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that ingests a mixed-format coordinate dataset (DMS strings, decimal degrees with hemisphere letters, and UTM zones) and normalizes all positions to WGS84 latitude/longitude in signed decimal degrees with fixed precision. Validate ranges, correct common notations (e.g., stray hemisphere suffixes), and emit a clean, standardized output with a report of any unresolvable rows.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that ingests mixed CSV/JSON sensor datasets from /app/data where numeric values include heterogeneous units and localized formats (e.g., 72F, 21.1 °C, 55 mph, 24,6 m/s, 1 234.5). Normalize all measurements to SI units with canonical float types and ISO 8601 UTC timestamps, then emit a unified standardized Parquet and a normalization report summarizing per-field conversions, unit assumptions, and any imputations or drops.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that scans /app/data for CSV/TSV/JSON product records and outputs /app/normalized.csv with sku, weight_g, length_mm, width_mm, height_mm, price_usd, and updated_at (UTC ISO-8601). It must normalize unit‑labeled values (g/kg/lb/oz; mm/cm/in/ft), localized numbers (commas, spaces), and currencies (using /app/rates.json), converting all timestamps to UTC and writing irrecoverable rows with reasons to /app/errors.csv.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that scans /app/logs for mixed-format logs (Apache, syslog, JSON), normalizes all timestamps (including timezone and epoch forms) to ISO 8601 UTC (Z) and standardizes severity to a fixed set [DEBUG, INFO, WARN, ERROR, FATAL] while lowercasing hostnames. Emit a single JSON Lines file at /app/normalized_logs.jsonl with a consistent schema and entries sorted strictly by timestamp.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a command-line script that ingests time-series CSV files in /app/sensors, normalizes ISO-8601 timestamps, and detects per-sensor outliers using median absolute deviation plus sanity checks (negative values where forbidden, duplicated timestamps, and sudden step changes). Output cleaned CSVs to /app/cleaned, append all flagged rows with reason codes to /app/anomalies.csv, and write a per-file summary to /app/report.txt.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a script that ingests /app/readings.csv of IoT telemetry and writes /app/clean.csv plus /app/anomalies.csv, flagging schema/plausibility errors (invalid UTF-8, column count, status not in {OK,WARN,FAIL}, out-of-range temp/humidity/battery, duplicate device_id+timestamp, and non-monotonic timestamps per device). Additionally, mark statistical outliers in temperature per device using a 1-hour rolling median with MAD-based thresholds and annotate anomalies with semicolon-separated reason codes.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a script that ingests GPX/CSV GPS tracks, normalizes timestamps and coordinate formats, and flags/removes points that imply impossible speeds, large teleports, or non-monotonic time. Output cleaned tracks and an anomalies report summarizing each file’s discarded points and reasons.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a script that ingests heterogeneous service logs from /app/logs (Apache/Nginx, JSONL app logs, and syslog), normalizes timestamps to UTC, correlates entries by request_id when present, and flags anomalies including unparsable lines, clock-skew/causal-order violations, negative durations, and latency outliers via median absolute deviation. Output a CSV of anomalies with standardized fields and a cleaned, time-ordered normalized JSONL stream for downstream use.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Write a script that scans /app/trips/*.csv (columns: trip_id, device_id, timestamp, lat, lon, odometer_km) to detect geospatial and temporal anomalies such as impossible speeds via haversine distance/time, out-of-bounds coordinates, non-monotonic timestamps per device, duplicate trip_ids, and odometer regressions. Output anomalies.jsonl with one record per issue (including a machine-readable reason and fields) and cleaned_trips.parquet with all offending rows removed.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Build a CLI tool that scans /app/data for CSV, JSONL, or Parquet files and writes profile.json containing per-column summaries (type, non-null count, distinct≈HLL, min/max, mean, median, std, p5/p25/p50/p75/p95) and fixed-bin histograms for numeric fields; if a timestamp column exists, also output hourly counts. Emit a stratified 1% sample that preserves rare categories to /app/sample.ndjson.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Create a CLI that streams over all .jsonl and .jsonl.gz files in /app/events, filters by a UTC date range, and computes overall and per-endpoint latency summaries: count, distinct_user_ids, mean, median, std, p90, p99, and error_rate. Output a 2-space-indented JSON report to /app/latency_summary.json, sorted by descending count and excluding non-numeric or missing latencies from calculations.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Create a streaming profiler that scans all CSV and JSONL (including .gz) files under /app/data and produces per-column summaries: non-null count, distinct count (case-insensitive), min, max, mean, median, and p95 for numeric fields, and top 5 categories with frequencies for non-numeric fields. Write a consolidated report to /app/profile.csv and a machine-readable /app/profile.json, handling mixed schemas across files and locale-formatted numbers (commas and decimals).', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Implement a streaming profiler for /app/data.csv (optionally gzipped) that infers column types from a sample and then computes per-column summaries without loading the entire file: numeric -> count, nulls, mean, std, min, max, median, p95 (approx via P^2), categorical -> count, nulls, distinct count, top-3 modes with counts. Write a single JSON report to /app/profile.json containing total rows, file size, and a per-column section with the computed statistics.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Implement a streaming profiler that scans /app/data for CSV/TSV/JSONL files (optionally .gz) and writes profile.json with per-column stats: total rows, non-null count, min/max, mean, std, quartiles/median, top-3 frequent values, and distinct counts (exact below a threshold, HyperLogLog otherwise). For timestamp columns also emit rolls.csv with per-hour counts and, if a user_id column exists, approximate hourly unique users.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Build a CLI that reads arbitrarily large, multi-file clickstream data (CSV and JSONL) and emits a fixed-size, user-level stratified reservoir sample that preserves the hourly traffic curve and the joint distribution of country×device within ±5% of the original. The tool must stream in one pass, be seed-reproducible, and write the selected records and a manifest to /app/sample/.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a CLI script that, given customers.csv, orders.csv, and order_items.csv, produces a reproducible 1% per-month stratified sample of orders via deterministic hashing, then emits subsetted CSVs for all three tables that include only rows linked to sampled orders while preserving referential integrity and original column order. Additionally, write a summary manifest reporting per-month sample rates and total rows kept per table.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a reproducible, referentially consistent subset of a two-table dataset by selecting a 1% hash-based sample of users (seeded) from /app/data/users.parquet and including all their related events from /app/data/events/*.parquet within an optional --since time window. Output sampled users and events Parquet files plus a summary verifying that the users’ country distribution deviates by no more than ±1% from the full dataset, and a manifest listing sampled user_ids.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a script that builds a deterministic, stratified sample of a normalized CSV dataset in /app/data (e.g., users, orders, order_items, events) by selecting users proportionally across country and signup cohort using a seed from /app/seed.txt and including only their related rows. Output referentially complete subset CSVs to /app/sample and a manifest with per-stratum counts and basic distribution checks.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a script that reads a large time-series CSV at /app/timeseries.csv (columns: series_id,timestamp,value) and produces a representative subset at /app/timeseries_sample.csv by applying Largest-Triangle-Three-Buckets (LTTB) downsampling to keep at most 2,000 points per series while preserving chronological order and the original header. Also emit /app/summary.json with the algorithm parameters, per-series retained counts, and a SHA256 of the sample for reproducibility.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Create a CLI that profiles missing-data patterns in a large CSV/JSONL via streaming with reservoir sampling, then renders an ASCII heatmap (rows=sampled records, columns=fields) using dense/light characters for present vs missing alongside per-field summaries. Write a human-readable report to /app/report.txt including missingness rates, longest present/missing streaks, and the top co-missing field pairs.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Create a command-line script that reads /app/events.ndjson of user event logs, computes a five-step conversion funnel (visit → view → add_to_cart → checkout → purchase) with counts and stage-to-stage rates per day and overall. Output a human-readable ASCII report to /app/funnel_report.txt including a monospaced funnel chart, a histogram of daily purchases, and annotations flagging anomalous days using a 3-sigma rule.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Implement a script that reads /app/events.csv with timestamps and categories, performs stratified reservoir sampling to bound memory, and generates an ASCII calendar heatmap of daily activity for the last month plus a per-category horizontal bar chart of counts. Write a single human-readable report to /app/report.txt.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Write a script that profiles a mixed dataset directory (CSV, JSONL, Parquet), performs deterministic sampling, infers column types, and generates a single CLI report with ASCII histograms for numeric columns, a character-based missingness heatmap by column, and top-k value tables for categoricals. Output the human-readable report to /app/profile.txt with sections per file and columns sorted by variability.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Build a tool that compares two release artifacts (e.g., .tar.gz vs .zip) for content-equivalence by extracting, normalizing metadata (timestamps, owners, entry order), and normalizing text EOL for .txt/.md before computing per-file hashes and a directory Merkle root. Emit a machine-readable JSON report of added/removed/changed/renamed files and permission-only changes, optionally validating against a provided manifest.json and exiting nonzero on drift.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Create a CLI that compares two mixed-format dataset snapshots at /app/v1 and /app/v2 using rules from /app/rules.yaml: JSON compared semantically (ignoring key order/whitespace), CSV diffed by declared primary keys, text normalized for EOL/encoding, and binaries by SHA256. Emit a single /app/diff_report.json summarizing added/removed/modified records/files and exit non‑zero if any change violates the allowlist rules.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Create a script that compares dataset snapshots at /app/stage_a and /app/stage_b by canonicalizing contents (CSV normalized by a configured primary key from /app/diff_config.toml; JSON/JSONL with sorted keys) and computing content checksums per file. Write /app/diff_report.json listing added/removed files, checksum mismatches, and for changed CSVs counts of added/removed/modified rows keyed by the primary key.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Create a script that compares two tar.gz filesystem snapshots at /app/snapshots/v1.tar.gz and v2.tar.gz entirely in-stream (no full extraction), computing SHA256s to report added/removed/changed files and detecting renames by matching content hashes while distinguishing content vs metadata-only changes. It must also validate v2 against /app/manifest.csv of expected paths, sizes, and hashes and emit a concise diff report.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Implement a CLI that compares two dataset snapshots in /app/snap_a and /app/snap_b, verifies SHA-256 integrity, and classifies per-table differences as identical, row-reordered-only, or content-changed across CSV and Parquet files. Detect renamed files by matching content hashes and write /app/changes.json (added/removed/modified/moved with reasons) and /app/violations.txt (checksum failures).', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Create a script that merges CSV enrollment files in /app/enrollments and validates against /app/students.csv, deduplicating repeated registrations (same student_id, course_id, term) and flagging registrations with missing or unknown student_ids. Output a cleaned /app/enrollments_clean.csv plus /app/duplicates.csv (grouped duplicate sets with source filenames) and /app/missing.csv (rows with null or unmatched keys).', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Create a script that scans all NDJSON user records under /app/incoming and /app/archive, flags rows with missing required keys (id, email, last_name) or malformed emails/phones, and deduplicates users across files by case-insensitive email or normalized phone or Levenshtein-1 full-name match with identical birth_date. Write /app/clean.ndjson containing one canonical record per user and /app/anomalies.csv listing file:line, record_id (if present), and the reason for each missing/duplicate/inconsistent record.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Implement a script that scans mixed-format contact datasets in /app/data (CSV and JSONL), normalizes them to a common schema, detects duplicates via canonicalized emails plus approximate name matching, and flags rows with missing required fields. Output a deduplicated clean_contacts.csv, a duplicates_report.json with cluster details and chosen canonical records, and a missing_summary.csv of per-column missing counts.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Write a script that ingests all headered CSVs in /app/logs/daily with fields sensor_id,timestamp,value, deduplicates rows by the (sensor_id,timestamp) key across files, and emits a consolidated /app/clean.csv sorted by sensor_id then timestamp. For each sensor, detect missing 1-minute intervals between its min and max timestamp, insert placeholder rows with empty value for gaps, and produce /app/quality_report.json summarizing per-sensor duplicates removed and gaps inserted.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a CLI regression harness that compares a baseline and candidate directory of mixed CSV/JSON/Parquet outputs by canonicalizing record/order, normalizing floats (1e-6), and keying rows by a provided primary key to detect schema drift, added/removed records, and numeric deltas beyond a tolerance. Write a structured diff to /app/report.json and PASS/FAIL to /app/result.txt using zero-drift for schema and 0.1% relative tolerance for numeric fields.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a CLI regression harness that runs data transformation scripts declared in /app/tasks.yaml, writes outputs (CSV, JSONL, Parquet) to /app/out, and compares them to goldens in /app/golden using canonicalization (deterministic row ordering, normalized JSON keys) and configurable numeric tolerances. Support per-file ignore/remap rules (e.g., timestamps) via config and emit both a JUnit XML report and a concise text diff summary of mismatches.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a regression harness that compares two pipeline outputs in /app/run_prev/ and /app/run_new/ (CSV or Parquet): verify identical table set, column order and types against /app/schema.json, per-partition row counts and key-level checksums match, and numeric aggregates stay within tolerances from /app/thresholds.toml. Emit a JUnit-style XML report to /app/results.xml and exit non-zero on any failure.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a script that compares /app/baseline and /app/candidate outputs across matching CSV or NDJSON files by joining records on id (order-insensitive), enforcing schema compatibility (no removed columns) and comparing numeric fields within a configurable tolerance while ignoring volatile fields like updated_at. Write a summary to /app/regression_report.txt and exit non-zero on missing/extra rows, type changes, or values outside tolerance.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Implement a CLI that compares new aggregation outputs in /app/current/*.parquet to golden snapshots in /app/baseline/*.parquet using /app/spec.yaml to define composite key columns and per-column rules (exact for categoricals, relative/absolute tolerances for numerics, allowed new columns). Align by keys, flag missing/extra keys, schema narrowing, null-rate increases, and tolerance violations; write /app/report.txt and /app/report.json and exit non-zero on any failure.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Build a CLI script that validates all NDJSON event files in /app/events against /app/schema.json (JSON Schema draft-2020-12), enforcing required fields, types, enum values, and custom formats (uuid, email, RFC 3339 timestamps). Coerce only lossless type fixes (e.g., numeric strings to numbers), write all valid records to /app/validated/merged.ndjson, emit a line-level error report to /app/invalid_report.json for rejects, and exit non-zero if any record fails.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Build a validator that reads a JSON Schema from /app/schema.json and validates user records aggregated from three sources (CSV, JSONL, and a SQLite table), performing strict type/format checks (email, uuid, URI, timezone-aware date-time) and conditional rules (e.g., if country == ''US'' then state is a 2-letter code; if status == ''ACTIVE'' then deactivated_at is null). Produce /app/validation_report.json summarizing per-rule violations and global uniqueness errors (id and case-insensitive email), and write a normalized /app/valid_merged.jsonl containing only valid, de-duplicated records with extra fields stripped.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Create a CLI validator that reads a versioned schema.yaml and validates mixed-format ''orders'' datasets (CSV, NDJSON, Parquet) for strict type adherence, including decimal precision/scale, UUID/email formats, UTC datetimes, nested objects/arrays, required fields, enums, and ranges. Produce a machine-readable JSON violations report per file and emit a Parquet containing only the records that pass validation.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Create a script that validates all NDJSON event records in /app/events against /app/schema.json, enforcing cross-record constraints (globally unique event_id, per-session non-decreasing ISO‑8601 timestamps) and conditional typing based on event_type (e.g., purchase requires a positive numeric amount, view must not include amount). Write valid, normalized records (timestamps coerced to RFC3339 Z, safe numeric coercions) to /app/valid.ndjson and emit a machine-readable /app/errors.csv with file, line, and codes for every rejected record.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Create a script that validates all records in /app/events/*.ndjson against /app/schema.json plus extra rules (user_id must be UUIDv4, timestamp ISO 8601 and not in the future, amount nonnegative with currency-specific decimal precision). Output /app/validation_report.csv with line_number,error_code,field,message for all failures and /app/valid.ndjson containing only canonicalized, schema-conforming records.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create a CLI that recursively scans /app/input for text files, auto-detects encoding (e.g., UTF-8/UTF-16/Windows-1252/Shift-JIS), converts them to UTF-8 with LF newlines in /app/normalized, then builds a reproducible tar.zst archive with sorted entries and fixed metadata. Output a manifest.csv listing original path, detected encoding, original/normalized sizes and SHA256 hashes, and support a verify mode that extracts the archive and validates checksums against the manifest.', NULL, NULL);
-- Compact batch 5/29: rows 201-250

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create a script that reassembles a split ZIP in /app/parts (dataset.zip.001, .002...), verifies integrity, and extracts its contents. Auto-detect and convert all text files to UTF-8 with LF line endings (leaving binaries unchanged), then repack the normalized tree into a deterministic /app/normalized.tar.xz with sorted entries, fixed timestamps/permissions, and write a SHA-256 manifest to /app/manifest.txt.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create a script that scans /app/inbox for .eml or mbox files, decodes MIME parts (base64 and quoted-printable) while honoring per-part charsets (UTF-8, ISO-8859-1, Windows-1252, UTF-16LE), and extracts only CSV or CSV.GZ attachments. Convert all text to UTF-8 with LF line endings, decompress .gz attachments, deduplicate identical rows, and write a single merged /app/combined.csv.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create two scripts: one that normalizes a directory by auto-detecting and converting text file encodings to UTF-8 with LF line endings (leaving binaries intact) and then produces a deterministic tar.gz (lexicographic order, fixed uid/gid/uname/gname, mtime from SOURCE_DATE_EPOCH) along with a MANIFEST.sha256 of pre/post hashes. The second script verifies determinism by re-packing and comparing the tarball and manifest, reporting any mismatches.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Implement a script that recursively normalizes a mixed-encoding dataset in /app/inbox: detect each text file’s encoding (BOM-aware; fallback heuristic), convert to UTF-8 without BOM, normalize Unicode to NFC, unify line endings to LF, and transparently gunzip any *.gz logs. Package the result as a deterministic tar.gz at /app/normalized.tar.gz (sorted entries, uid/gid=0, mtime=0) and write /app/manifest.csv summarizing original_path, output_path, original_encoding, original_eol, was_gzipped, bytes_before, bytes_after, and sha256_after.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a CLI script that reads all multi-document YAML files in /app/k8s (resolving anchors/aliases) and converts each document into a normalized JSON object with dot-notated keys, outputting a consolidated JSON Lines file at /app/resources.jsonl. Then convert that JSONL to Parquet at /app/resources.parquet with inferred column types, lexicographically ordered columns, and an added source_filename field.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a CLI that converts a directory of heterogeneous CSV files (mixed encodings: UTF-8/UTF-16/ISO-8859-1; varying delimiters and decimal separators) into a single UTF-8 Parquet dataset at /app/output using a provided schema.yml for field names and types. The tool must auto-detect each file’s encoding and dialect, standardize dates and numerics, and write a detection_summary.json reporting per-file encoding, delimiter, quotechar, decimal separator, and any type coercions.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a script that converts a directory of YAML documents that use anchors, aliases, and merge keys into a single NDJSON file, fully resolving all references and merges. Each line must be a canonicalized JSON object with deterministically sorted keys, ISO-8601-normalized timestamps, and an added source field containing the original filename.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a script that converts all XML files in /app/input_xml (with multiple namespaces) into YAML files in /app/yaml, representing attributes with an @ prefix, text with _text, preserving child order, and turning repeated tags into sequences. If any element named Data contains base64 text, decode it to /app/blobs/<stem>_<n>.bin and replace the YAML value with the relative blob path.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Create a CLI script that infers a JSON Schema (Draft 2020-12) from all NDJSON files in /app/samples, unioning fields and types, distinguishing integer vs number, detecting nullability, min/max, enum for low-cardinality strings, and formats (date-time, uuid, email). Validate /app/new_records.jsonl against the inferred schema, writing the schema to /app/schema.json and a newline-delimited list of invalid record indices with short reasons to /app/invalid.txt.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Create a command-line tool that scans /app/datasets for mixed CSV and JSONL files, infers a unified typed schema (nullability, unions, enums from low-cardinality strings, and date/time/decimal detection) and emits a JSON Schema Draft 2020-12 file. Validate every record against the schema with a detailed per-file report of failures (row number and reason), and write normalized, UTF-8, type-coerced Parquet outputs conforming to the inferred schema.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Create a script that infers a relational schema from CSVs in /app/training (column types, primary keys, and foreign keys via uniqueness and value-domain overlap). Validate CSVs in /app/incoming against the inferred schema, writing /app/schema.json and /app/report.txt with type mismatches, duplicate PKs, and FK violations, handling delimiter and encoding detection.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Implement a CLI that infers a JSON Schema (Draft 2020-12) from /app/samples/*.jsonl by detecting nested field types (including unions), required/optional via coverage threshold, numeric min/max, and common string formats (date-time, email, uuid), then writes /app/schema.json. Validate /app/target.jsonl against the inferred schema and produce /app/validation_report.json with per-record JSON Pointer paths of violations and a summary of counts by rule.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Infer a column schema from a corpus of heterogeneous delimited files in /app/samples (auto-detect delimiter, header presence, and encoding), determining data types, null tokens, and required fields (present in ≥98% of rows), and emit an Arrow-compatible schema to /app/schema.json. Validate /app/batch.csv against that schema, writing /app/invalid_rows.csv with columns row,field,error for all violations and also produce a normalized UTF-8, comma-delimited, headered file of the valid rows at /app/valid.csv.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Create a script that parses multiple timestamped lsof snapshots in /app/snapshots, correlates PIDs across files to compute per-process file-descriptor deltas and type breakdowns (REG/SOCK/FIFO), and flags processes with monotonically increasing counts as potential leaks. Write a ranked summary to /app/leaks.txt listing PID, command, total growth, and per-type growth for the top offenders.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Create a script that scans /app/logs for application logs (including rotated .log and .log.gz files) with RFC3339 timestamps and multi-line Java stack traces, grouping each exception event as the ERROR line plus its following stack frames and normalizing all timestamps to UTC. Output /app/errors.csv listing exception_type, sha1_message_signature (message with volatile values like hex addresses stripped), first_seen, last_seen, count, top_3_frames (semicolon-separated), and up to five associated reqId values (comma-separated).', NULL, ARRAY['java']),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Create a script that scans /app/logs for web access logs in mixed formats (Apache combined, Nginx default, and JSON-lines), including rotated .gz files, normalizes them to a common schema, and writes a timestamp-sorted /app/normalized.ndjson. Then compute per-hour aggregates (requests, unique sessions using a 30-minute inactivity window, 4xx/5xx rates, and P50/P95 latency) and save a 2-space-indented JSON report to /app/traffic_report.json.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Implement a script that scans /app/logs for mixed-format server logs (nginx access, JSON app logs, and syslog; some gzipped), correlates events by a shared request_id, and normalizes timestamps to UTC to reconstruct per-request timelines. Output timeline.csv with per-request start/end/status and phase durations, plus slow_requests.txt listing the 20 slowest requests with the inferred bottleneck phase.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Ingest mixed-format service logs from /app/services/* (Apache/Nginx text, JSON app logs, and RFC5424 syslog), normalize core fields, and correlate events by request_id to reconstruct per-request timelines and end-to-end latencies. Output traces.jsonl (one record per request with ordered spans and missing-service gaps) and summary.csv with per-endpoint counts, mean, and P95 latency.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Create a Bash script that performs rotating, hard-link deduplicated snapshots of /app/source into /app/backups using rsync --link-dest, honoring patterns from /app/.backupignore and default excludes (.git, node_modules, *.tmp). Each run must atomically create a timestamped snapshot, enforce a 7-daily/4-weekly/6-monthly retention policy with pruning, emit a manifest with SHA-256 for new/changed files, compute total and dedup-saved bytes, and append a CSV summary to /app/backup_report.csv.', 'hard', ARRAY['git']),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Create a POSIX shell backup tool that performs rsync-based hard-link snapshots of /data into /repo/YYYY-MM-DD using --link-dest for incrementals, generates a JSON manifest with per-file SHA256 and total logical size, then zstd-compresses and age-encrypts the manifest with a provided public key. Implement verify and restore modes that reconstruct the latest snapshot into /restore (preserving sparse files, symlinks, permissions, and mtimes) and enforce GFS retention (7 daily, 4 weekly, 6 monthly) while producing a human-readable report.txt.', 'hard', NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Implement a Bash snapshot-backup tool that reads /app/backup.toml for include/exclude globs and a retention policy, creates timestamped rsync snapshots with --link-dest under /app/backups, then tars each snapshot, compresses with zstd, and encrypts with age using a provided public key. Provide verify and restore modes that generate/validate a SHA256 manifest per snapshot and prune snapshots per policy.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Write a command-line script that creates time-stamped snapshot backups of /app/source, using rsync with --link-dest for unchanged files and packing small files (<256 KiB) into a single zstd-compressed tar encrypted with GPG, honoring exclude patterns from /app/.backupignore and preserving permissions, symlinks, xattrs, and ACLs. Implement a restore mode that reconstructs to /app/restore from a chosen snapshot by merging the snapshot tree with the decrypted tarball and verifies integrity against a SHA-256 manifest, pruning old snapshots according to /app/retention.yaml.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Write a script that creates timestamped, rsync-based snapshot backups of /app/source into /app/snapshots using hard links for unchanged files, enforces a retention policy defined in /app/retention.json (daily/weekly/monthly), and emits a per-snapshot manifest with SHA256 checksums and sizes. Include restore and verify modes: restore a named snapshot to /app/restore and scan all snapshots to produce /app/verify_report.txt listing missing files and any checksum mismatches.', 'hard', NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create a command-line script that scans /app/photos for JPEG/HEIC images, extracts EXIF capture timestamp and camera model, renames each to YYYYMMDD_HHMMSS_{make-model}.{ext} (collisions resolved with _01, _02...), de-duplicates by SHA256 hash, and moves files into /app/library/YYYY/MM/ directories. Produce a CSV at /app/ingest_log.csv listing original_path,new_path,hash,was_duplicate,source_timestamp_used (exif|mtime), and write a JSON summary (/app/summary.json) with counts by camera and month.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create a script that recursively scans /app/photos, reads EXIF capture time and camera model from image files, renames them to YYYYMMDD_HHMMSS_[Model]_[seq].ext, and moves them into /app/library/YYYY/MM/DD/. Exact duplicates (by SHA-256) should be moved to /app/duplicates and /app/manifest.csv must map each original path to its final path and status (moved, duplicate, exif_fallback).', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create a script that scans /app/photos, renames images to YYYY-MM-DD_HHMMSS_{camera}.ext using EXIF (falling back to mtime), and moves them into /app/library/YYYY/MM/DD while leaving non-images untouched. Detect exact and near-duplicate photos via perceptual hashing, keep a single canonical copy in the dated folder, move duplicates to /app/library/_duplicates, and write a manifest CSV mapping original paths to final locations with hashes and duplicate-of entries.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create two CLI scripts: one scans /app/input for images/videos, extracts capture date from EXIF or filename fallback, deduplicates by perceptual hash, moves a canonical copy to /app/library/YYYY/MM/ with a normalized slug while replacing duplicates with relative symlinks, and writes a manifest.json. The second script uses manifest.json to fully restore the original directory layout and filenames.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Write a script that scans /app/incoming for images/videos, extracts capture timestamps from embedded metadata (falling back to file mtime), and batch renames/moves them to /app/library/YYYY/MM/YYYYMMDD_HHMMSS_{counter}.{ext}, keeping sidecar files (.xmp/.srt) in sync. On timestamp collisions, deterministically order and increment the counter without gaps and output /app/manifest.csv mapping old_path,new_path for every moved file.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Create a command-line tool that scans /app/project to identify orphaned media assets (e.g., png/jpg/svg/gif/pdf/mp4) not referenced by any Markdown/HTML/YAML/CSV files via relative links, front matter, or image tags. The tool must respect .gitignore rules, safely handle symlinks, decode URL-encoded paths, and output orphaned files sorted by size descending to /app/orphans.txt.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Create a script that recursively scans /app/docs for Markdown files, extracts relative image links (e.g., ![...](./img/foo.png)), and verifies that each referenced file exists (ignoring absolute URLs and anchors). Write a JSON array to /app/missing_images_report.json listing missing references with fields: md_file, line, ref, and resolved_path, sorted by md_file then line.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Create a script that recursively scans /app/workspace for filenames that normalize to identical casefolded NFC forms (to detect Unicode confusables/collisions) and flags names containing invisible or bidirectional control characters. Write /app/filename_audit.csv listing original absolute path, normalized form, issue type(s), and any colliding peers, sorted deterministically.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Scan /app/docs for Markdown files, extract relative image and link targets, canonicalize paths, and compare against the real files under /app/docs (excluding hidden and build artifacts). Write /app/report.json with two sorted arrays: missing_references (referenced but absent) and orphan_files (present but never referenced), using paths relative to /app/docs.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Write a script that recursively scans /app/site, finds all asset files under /app/site/assets (extensions: png,jpg,jpeg,svg,gif,mp3,mp4,pdf,webp) and reports those not referenced by any Markdown/HTML/CSS/JS file in the project. Ignore directories .git, node_modules, dist, and build; output a sorted newline-separated list of orphan paths to /app/orphans.txt.', NULL, ARRAY['git']),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Build a CLI that recursively scans a directory and emits a deterministic JSONL manifest per entry (path, type, size, mode, uid/gid, nlink, inode, mtime, SHA-256 for files, symlink targets, and summarized xattrs/ACLs), plus anomalies.csv flagging world-writable/SUID/SGID files, broken symlinks, future timestamps, and duplicate content by hash. Provide a subcommand to diff two manifests and output delta.csv enumerating added/removed/changed items with standardized change reasons for audit.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Create a script that recursively inventories /app/project, extracting normalized metadata for every file, directory, and symlink (path, type, size, mode octal, uid/gid, mtime in UTC ISO 8601, inode+device), computing SHA-256 only for regular files ≤50 MB and recording symlink targets, while skipping paths ignored by the repo’s .gitignore. Write a deterministic, path-sorted JSON Lines manifest to /app/manifest.jsonl and a /app/summary.txt with counts by type, total bytes, detected hardlink groups, and duplicate files grouped by content hash.', 'hard', NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Implement a CLI that recursively audits a directory and writes a JSONL snapshot of each item’s metadata (path, type, size, permissions, owner/group, mtime/ctime, inode, link count, device, xattrs if available) plus a SHA-256 for regular files. When given a prior snapshot, generate a changes report listing added/removed/modified files with old/new metadata and per-extension aggregates, avoiding symlink loops and not crossing filesystem boundaries.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Write a script that recursively audits /app/site and produces /app/manifest.csv in deterministic path order with per-entry path, type, mode (octal), uid, gid, size, mtime (UTC ISO), inode, nlink, and either sha256 for regular files or symlink_target for symlinks. Additionally, generate /app/anomalies.log listing any world-writable entries, files with setuid/setgid bits, and broken symlinks, one issue per line.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Write a script that recursively scans /app/data, producing a manifest (CSV) of every path with type, size, mtime (UTC), permissions (octal), uid/gid, inode, hardlink count, and SHA-256 for regular files, plus symlink targets. Generate a separate report that groups hard-linked files and content-duplicate files (same hash but different inodes) and flags world-writable or SUID/SGID entries.', 'hard', NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Build a command-line tool that fetches all tickets updated since the last run from a GitHub-like REST API using pagination and ETag-based conditional requests, storing normalized records in /app/issues.sqlite. After syncing, compute a summary (new/updated counts, per-label totals, top 3 authors) and POST it as JSON to a provided webhook URL, honoring rate-limit headers with retry/backoff and writing the exact payload to /app/sync_report.json.', NULL, ARRAY['rest', 'api']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Create a command-line script that reads SKUs from /app/skus.csv and an API key from the environment, then queries a paginated inventory/pricing REST API with conditional GETs (ETag/If-None-Match) and backoff for 429 to produce /app/inventory.csv. For any items below the reorder threshold in the input, batch POST a JSON payload to a /reorders endpoint and write the response to /app/reorder_response.json.', NULL, ARRAY['api', 'rest']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Implement a CLI that submits a batch job to a REST API (POST returns 202 + Location), then polls with exponential backoff and ETag/Retry-After handling to download paginated JSON results into a SQLite database with idempotent upserts. Compute daily aggregates and POST an HMAC-signed summary to a webhook, persisting a resume cursor and writing only OK or ERROR to /app/result.txt.', NULL, ARRAY['rest', 'api', 'database']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Write a CLI that reads SKUs from /app/skus.csv, authenticates via OAuth2 to a provided mock REST API, and fetches product metadata through paginated endpoints while respecting rate limits using retries and ETag-based conditional requests. Normalize and deduplicate variants, then POST an idempotent batched inventory snapshot to a submission endpoint and emit /app/run_report.json summarizing cache hits, retries, and per-batch results.', NULL, ARRAY['rest', 'api']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Write a script that reads /app/cities.txt, fetches the last 48 hours of PM2.5/PM10 per city from a public air-quality API (e.g., Open-Meteo AQ) using curl or Python requests with ETag-based caching and retry-on-429, aggregates per-city min/mean/max, and writes /app/aq_summary.csv. Then POST a signed (HMAC-SHA256) JSON summary to a provided local webhook endpoint and print the returned confirmation ID.', NULL, ARRAY['api', 'python']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Against a local S3-compatible endpoint (MinIO) with credentials in /app/.env, implement a CLI-driven sync that uploads /app/dataset to a bucket, applies per-file metadata and content-type from /app/metadata.csv, enforces multipart uploads for files >8 MB, and verifies integrity by re-downloading and comparing SHA-256. Emit /app/report.json listing key, size, content-type, user metadata, ETag, sha256_ok, and 10-minute presigned URLs for the five largest objects.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Build a policy-driven cross-cloud sync tool that mirrors an AWS S3 prefix to a GCS bucket, preserving Content-Type/Cache-Control and applying include/exclude and rename rules from /app/policy.yaml; use checksum-aware diffing that reconciles S3 ETags (including multipart) with GCS CRC32C/MD5 to avoid redundant transfers. Support dry-run and apply modes, persist a local SQLite manifest for idempotency, and write /app/report.json listing created/updated/deleted/skipped objects and any conflicts.', NULL, ARRAY['cloud', 'aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Create a command-line tool that syncs /app/dataset to both an AWS S3 bucket and a GCS bucket, reading include/exclude rules from /app/.syncignore and per-file metadata (Content-Type, Cache-Control, custom tags/labels) from /app/metadata.json. The script must perform a dry-run and then an actual run, preserve metadata, use multipart/resumable uploads, verify integrity via checksums (ETag/MD5 for S3 and CRC32C for GCS), and emit a machine-readable reconciliation report at /app/sync_report.json listing created/updated/skipped objects and any mismatches across clouds.', NULL, ARRAY['aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Create a script that audits and reconciles a Google Cloud Storage bucket prefix to match an AWS S3 bucket prefix: copy missing/outdated objects, preserve Content-Type and user metadata, and delete GCS extras only if not modified in the last 10 minutes. Output /app/audit_report.csv listing key, action (copied, updated, deleted, skipped), source ETag, destination hash, and size, with exponential backoff retries for transient errors.', NULL, ARRAY['cloud', 'aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Write a script that uses AWS CLI against a provided S3-compatible endpoint to perform an idempotent two-way sync between the remote prefix s3://tb-datasets/projects and /app/cache, honoring patterns from /app/.syncignore and verifying integrity via ETag/MD5. Persist object metadata (Content-Type, Cache-Control) to /app/cache/_metadata.json and emit /app/sync_report.txt with counts of added, updated, and removed items; delete locals only when a --prune flag is given.', NULL, ARRAY['aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Create a CLI script that connects to a PostgreSQL database using credentials from /app/db.env, runs a parameterized denormalizing query over customers, orders, and payments to produce per-customer summaries (first/last order, order count, total paid, outstanding balance) with emails masked and reconciliation checks enforced. Export results as partitioned NDJSON files by last_name initial under /app/export/ and write a manifest.json with per-partition row counts, min/max timestamps, and SHA256 checksums.', NULL, ARRAY['postgresql', 'database']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Create a command-line script that connects to a PostgreSQL database using credentials from /app/db.env, queries paid invoices between two dates (joining invoices, customers, and payments), masks PII (email SHA256, phone last 4), normalizes timestamps to UTC ISO-8601, and exports the result as month-partitioned NDJSON gzip files in /app/export sorted by invoice_id. Write a _manifest.json in the same folder with the query inputs, partition file names, row counts, and SHA256 checksums of each gzip.', NULL, ARRAY['postgresql', 'database']);
-- Compact batch 6/29: rows 251-300

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Create a script that connects to a PostgreSQL database via psql and computes per-customer lifecycle metrics over the last 3 years—first_order_at, last_order_at, total_orders, lifetime_gross_revenue, refunds_amount, net_revenue, and avg_days_between_orders—using CTEs, window functions, and UTC-normalized timestamps while excluding test/fraudulent activity. Export the result sorted by customer_id to /app/customer_ltv.csv using COPY with a header and exactly those columns.', NULL, ARRAY['postgresql', 'database']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Implement a terminal-run script that reads credentials from environment variables, connects to PostgreSQL and MongoDB, reconciles active users by email with at least one paid order in the last 90 days, and exports a flattened dataset to /app/output/users_aggregate.csv and /app/output/users_aggregate.parquet with deterministic ordering. Provide a PostgreSQL .sql file to create a materialized view and indexes used by the export, and ensure the script uses server-side COPY for CSV and batched streaming for Parquet to support million-row exports.', NULL, ARRAY['postgresql', 'mongodb', 'sql']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Spin up local Postgres and MongoDB with seed data, then implement a CLI script that joins Postgres orders to MongoDB user profiles by user_id, computes 30/60/90-day LTV and signup-month cohorts per acquisition_channel, and exports results to a partitioned Parquet dataset plus a summary CSV. The script must support incremental runs via a persisted watermark (last_order_ts) and perform idempotent upserts of partitions.', NULL, ARRAY['mongodb']),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Create a Bash script that reads SSH credentials and paths from /app/remote.toml, uploads /app/input/* to a remote server, triggers a Nextflow pipeline there in a detached session, and polls its status until success/failure with a timeout. On completion, download the produced artifacts to /app/output and write /app/run_report.json containing run_id, status, start/end timestamps, and SHA256 checksums for each artifact.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Create a CLI that SSHes into a remote SLURM head node, renders a batch script from a local YAML, submits it with sbatch, and robustly monitors the job by polling squeue/sacct and tailing logs (including retrying if preempted). On completion, scp the results back, verify checksums from a manifest, and write a run_report.json with job ID, final state, runtime, and artifact hashes.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Create a CLI that submits a SLURM job array to process CSV shards listed in /app/manifest.json, capturing the sbatch JobID and polling squeue/sacct until every task completes successfully. When done, fetch each task’s output via scp into /app/results, verify SHA256 checksums from the manifest, and merge the outputs into /app/final.parquet in deterministic shard order; exit non-zero if any task fails or times out.', 'hard', NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Implement a CLI that snapshots a local pipeline directory, syncs it to a remote host over SSH/SCP, launches it in a detached session with a unique run ID, and continuously monitors status/logs with automatic reconnect after SSH drops. On completion or timeout, fetch artifacts and emit /app/run_summary.json containing run ID, start/stop times, exit code, SHA256 of outputs, and a flag indicating if execution was skipped due to idempotent content-hash deduplication.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Remote Pipeline Execution', 'Implement a CLI tool that reads an SSH inventory and a jobs.json describing data partitions, then dispatches a processing script to multiple hosts via rsync+ssh, launches jobs under nohup with a unique run_id, and periodically polls remote status/logs until completion with bounded retries. On success it collects artifacts into /app/results/{run_id} and writes a summary manifest (per-host status, duration, and stderr tail), supporting resume and cancel by run_id.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Build a CLI that converts a folder of Markdown notes (with YAML front matter, [[wiki-links]], reference-style links, and images) into a single self-contained HTML and a PDF: rewrite intra-note links to anchors, deduplicate references, number sections/figures/tables, apply a supplied BibTeX+CSL, and inline all assets. Also emit search_index.json mapping each heading to its text snippet and anchor.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Build a command-line tool that migrates a static HTML site in /app/site to a markdown-first repo: convert each article HTML to Markdown with YAML front matter (title/date/tags), mirror images locally and rewrite internal links to .md. Generate index.md as a site map and report.txt listing broken links and missing assets.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Create a script that compiles a directory of Markdown notes (with YAML front matter and [[WikiLinks]]) into a self-contained HTML site and a JSONL plain-text search index. Resolve {{include:...}} directives, convert wikilinks to relative paths, inline local images as data URIs, and output a broken-link report.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Create a script that converts all Markdown files under /app/pages into self-contained HTML, rendering fenced mermaid and graphviz code blocks to inline SVG (using mermaid-cli and dot) and converting the remainder via pandoc. It must rewrite cross-file links to .html, preserve front-matter metadata as meta tags, and generate a tag index page from YAML front matter.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Markdown/HTML/Text Conversion', 'Create a script that scans /app/posts/*.md (with YAML front matter: title, date, tags), converts each to a standalone HTML article in /app/site via pandoc, preserving code fences and rewriting relative links using a BASE_URL. Also generate an Atom feed at /app/site/atom.xml listing the 20 most recent posts with RFC 3339 UTC timestamps, canonical URLs, and a summary taken from the first paragraph.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Create a command-line script that recursively scans /app/logs (including .gz files), uses regular expressions to parse heterogeneous log lines into structured fields (timestamp in multiple formats, level, message) and extract entities (IPv4/IPv6, emails, GUIDs, request/user IDs). Redact PII by masking emails and Luhn-valid credit card numbers with stable pseudonyms, write redacted_logs.ndjson, and output templates.csv by normalizing messages to regex-derived templates (replace numbers/hex/IDs/IPs with placeholders) with aggregated counts.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Create a command-line script that scans all .txt files in /app/contracts for calendar dates written in diverse styles (YYYY-MM-DD, MM/DD/YYYY, DD Mon YYYY, Mon DD, YYYY, DDth of Month ’YY, and ranges like Jan 3–5, 2024) using regexes that handle ordinal suffixes, commas, stray punctuation, and Unicode dashes. Normalize each concrete date to YYYY-MM-DD, expand ranges into individual dates, and write a CSV to /app/contract_dates.csv with columns file,line_number,original_snippet,normalized_date, sorted by file then normalized_date.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Create a script that scans /app/paper for .tex and .bib files, uses regex to extract citation keys from LaTeX citation commands (handling optional arguments and comma-separated lists), and extracts BibTeX entry keys. Write /app/citation_audit.txt listing ordered unique cited keys, missing citations (cited but absent in .bib), unused .bib entries, and per-file citation counts.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Scan /app/docs for Markdown files and use regex to extract external URLs, bare domain mentions, and email addresses from prose while excluding fenced code blocks and inline code. Also detect reference-style link labels to report undefined and unused labels, writing links.csv (type,value,first_file,count) and ref_audit.txt with the findings.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Pattern Extraction & Regex Matching', 'Write a script that scans all Markdown files under /app/docs and, using only regex-based parsing, extracts every fenced code block (```lang ... ```) with its language tag, 1-based start/end line numbers, and any inline TODO: ... comments inside or on immediately adjacent lines, producing /app/blocks.csv with columns file_path, block_index, language, start_line, end_line, todos (semicolon-separated). Also perform an in-place substitution that wraps any shell code block containing a line matching rm\s+-rf\s+[^;]+ in <!-- DANGEROUS --> ... <!-- /DANGEROUS --> markers without altering code content.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Create a CLI that profiles all CSV/TSV files in /app/data, infers column types, computes per-column quality metrics (null %, distinct count, numeric stats, text length ranges, top values), and flags mixed-type anomalies and candidate ID/date columns. Output a readable /app/profile.md and a structured /app/profile.json summarizing per-file results and a cross-file schema reconciliation section.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Create a command-line script that scans /app/adr for Markdown Architecture Decision Records with YAML front matter (id, title, date, status, supersedes/superseded_by), validates cross-references, and generates /app/adr_index.md. The report must include a chronological table, sections grouped by status, and ASCII supersession chains for each lineage with deterministic ordering.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Generate release notes by parsing a Git repo at /app/repo using Conventional Commits, producing /app/RELEASE_NOTES.md and a machine-readable /app/release.json for the latest tag range (or last 50 commits if no tags). Categorize entries by type and scope, detect BREAKING CHANGE footers, count contributors, and convert issue/PR references (e.g., #123) into links.', NULL, ARRAY['git']),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Implement a script that scans Nginx-style logs in /app/logs/*.log and a release timeline in /app/releases.json to detect outage windows where per-minute 5xx response rate exceeds a given threshold and attribute each window to the nearest release. Generate a Markdown incident report (/app/incident_report.md) with a timeline and a CSV (/app/incident_windows.csv) with start,end,duration,peak_rate,top_errors,release_sha, sorted chronologically.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Report & Document Generation', 'Write a command-line script that scans a Git repository at /app/repo and generates /app/RELEASE_NOTES.md for the tag range specified in /app/range.txt by parsing Conventional Commits and cross-referencing issue/PR titles from /app/issues.json. The Markdown must include grouped sections by type and scope, a BREAKING CHANGES section from footers or ! commits, an alphabetically ordered contributor summary with commit counts, and deterministic ordering within each section.', NULL, ARRAY['git']),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Create a CLI that renders Jinja2 templates in /app/templates using layered variables from /app/config/base.yaml, /app/config/env/*.yaml, and /app/config/secrets.json, and for each locale in /app/locales.csv merges the matching /app/i18n/<locale>.yaml. It must support simple macros for pluralization and date formatting, fail on unresolved placeholders, emit outputs to /app/build/<locale>/ preserving paths, and write /app/build_manifest.json listing each file with its locale and SHA256.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Create a CLI that renders a directory of Markdown pages with YAML front matter through Jinja2 layouts/includes/macros, applying variable precedence of page > section defaults > global defaults > environment and custom filters (datefmt, slugify, markdown). Write HTML to /app/dist mirroring the source tree and emit a manifest.json mapping each source file to its chosen layout and output path.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Create a script that renders all Jinja2 templates under /app/templates to /app/build using layered variables from /app/defaults.yaml, environment variables, and optional per-template .vars.yaml files (per-template > env > defaults), preserving directory structure and failing on undefined variables. Implement custom filters slug (kebab-case) and sha256_file(path under /app/assets), support {% include %}, and write a manifest.json in /app/build listing each output and its SHA-256.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Implement a preprocessor that renders a set of .tmpl files into environment-specific configs by expanding user-defined macros, {{ENV_VAR}} placeholders, and conditional blocks from a vars.yml inventory, with nested include support and cycle detection. Output dev, staging, and prod variants to /app/build and exit non-zero if any placeholder remains unresolved.', NULL, NULL),
('Data Processing & Scripting', 'Text & Document Processing', 'Template Rendering & Macro Expansion', 'Implement a two-stage renderer that processes all .tmpl files under /app/templates: first expand ${VAR} using envsubst limited to a whitelist in /app/allowed_env.txt, then render Jinja2 with values from /app/values.yml and custom filters (slugify, to_kebab, join_path). Write outputs to /app/rendered preserving structure and produce /app/report.json with per-file SHA256 and any unresolved placeholders, exiting non-zero if any remain.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Diagnose and fix a Node.js CLI that crashes with unhandled promise rejections when the upstream API stalls or returns invalid JSON by adding robust async error handling, per-request timeouts, and a retry-with-exponential-backoff fallback. Ensure it surfaces clear messages, writes a partial cached result when available, and exits with a nonzero code on unrecoverable errors; verify using the provided flaky server and integration tests.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Fix a Python CLI downloader that crashes on transient HTTP errors and leaves corrupted partial files by adding robust try/except handling with timeouts, retry-with-exponential-backoff, mirror fallback, and safe atomic writes that can resume from .part files. Verify by simulating failures and confirming the final artifact matches a provided checksum.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Fix a Python ETL loader that crashes on malformed CSV rows, encoding errors, and SQLite constraint violations by adding robust try/except handling, per-row validation, and a dead-letter output. The job should complete with partial success, write rejected rows to /app/bad_rows.csv, and exit zero once all recoverable errors are handled.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Harden a Python CLI that crashes with BrokenPipeError when its output is piped to another process (e.g., head) by adding robust EPIPE/SIGPIPE handling and a clean shutdown path. Verify it emits no stack trace, exits successfully, and does not leave partial lines in the downstream output.', 'hard', ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Exception Handling & Recovery', 'Harden a Python CLI that ingests a directory of JSON Lines files into SQLite; it currently crashes on malformed JSON, oversized records, missing keys, and SQLITE_BUSY/locked errors. Add robust try/except and validation to quarantine bad lines, apply bounded retries with exponential backoff for transient DB errors, fall back to defaults for missing fields, and produce a deterministic /app/ingest_summary.json so the run completes without uncaught exceptions.', 'hard', ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Debug an intermittent crash in a multithreaded C program by using gdb to reproduce the failure, inspect thread states, and set conditional breakpoints and memory watchpoints to trace a use-after-free. Patch the lifetime bug and verify stability by running the provided stress test until it passes consistently.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Diagnose and fix a sporadic SIGSEGV in a multithreaded C ring-buffer logger by reproducing under load, attaching with gdb, and using thread-aware stepping and watchpoints on head/tail indices to find the out-of-bounds write. Patch the bug using proper atomics/locking and verify stability with a stress test.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Reproduce and debug an intermittent segmentation fault in a minimal C HTTP chunked-encoding parser by running it under gdb with a crafted malformed request, stepping through the state machine and inspecting buffer pointers to locate an out-of-bounds write. Implement the fix with proper bounds checks/index corrections, recompile, and verify the parser handles the input without crashing and produces correct output.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Track down an intermittent segmentation fault in a multi-threaded C log processor by using gdb to reproduce, inspect per-thread backtraces, and set watchpoints on a shared ring buffer to catch the corrupting write. Fix the use-after-free in the consumer, rebuild, and verify stability under a provided parallel stress test.', NULL, ARRAY['parallel']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Interactive Debugger Usage', 'Use gdb to diagnose and fix a sporadic crash in a multithreaded C ring buffer that only fails under -O2 by setting thread-aware breakpoints and watchpoints to catch an out-of-bounds write caused by missing synchronization. Implement correct memory ordering or locking and verify stability by running the provided stress harness until it completes without errors.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Diagnose and fix a Rust implementation of topological sort that yields incorrect or nondeterministic orders by mutating a set during iteration and miscounting in-degrees. Implement a correct queue-based Kahn’s algorithm and add a regression test covering cycles, duplicate edges, and deterministic ordering.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Fix a Rust CLI’s Dijkstra implementation that returns incorrect shortest-path distances on graphs with zero-weight and parallel edges due to premature visited-marking and missing decrease-key handling. Correct the relaxation and priority-queue logic, add regression tests, and verify outputs on provided fixtures.', NULL, ARRAY['rust', 'parallel']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Identify and fix a Python topological sort implementation that returns incorrect orders and occasionally fails to detect cycles due to flawed in-degree updates and non-deterministic node selection. Implement a deterministic Kahn’s algorithm with explicit cycle detection and verify stable ordering across repeated runs on provided DAGs.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Logic & Algorithmic Bugs', 'Repair a shortest-path tool that returns wrong Dijkstra distances on graphs with zero-weight edges due to stale priority-queue entries and off-by-one node indexing. Update the algorithm to correctly handle decrease-key via ignoring stale heap nodes and proper indexing so outputs match provided golden results across multiple graphs, including disconnected cases.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'A Rust CLI builds successfully but crashes with SIGSEGV only in --release. Use RUST_BACKTRACE with AddressSanitizer or Miri to locate undefined behavior inside an unsafe block, fix the memory error, and verify cargo test and cargo run complete without crashes.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Diagnose a C++17 CLI tool that compiles but intermittently segfaults on large inputs. Use AddressSanitizer/UBSan and gdb to trace iterator invalidation from std::vector reallocation in a loop, refactor the code to avoid undefined behavior, and verify by running the provided corpus without crashes.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Diagnose and repair a C++ CLI tool that crashes under -O2 with a SIGSEGV due to vector iterator invalidation in a custom deduplication routine. Use AddressSanitizer or gdb to pinpoint the fault, refactor the loop to avoid invalidated iterators, then rebuild with -O2 and verify the program completes on the provided dataset without crashing.', NULL, NULL),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Repair a Rust CLI that fails to compile due to OpenSSL linkage errors and then panics at runtime when a config file is absent by switching TLS dependencies to rustls, fixing Cargo feature flags, and making config loading non-panicking. Verify by building on stable, running cargo test, and executing the binary with and without a config file.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Code Debugging & Error Resolution', 'Runtime & Compilation Errors', 'Repair a flaky multithreaded C11 program that randomly segfaults under -O2 by reproducing the crash, using AddressSanitizer/ThreadSanitizer to locate a data race/use-after-free, and identifying the offending code paths. Implement proper synchronization and lifetime management so the binary runs reliably and passes a provided stress test.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Diagnose a nightly cron job that runs a Python ETL (CSV to Parquet) but fails under cron with ModuleNotFoundError and missing files due to minimal PATH, unset locale, and relative paths. Fix the job by using a virtualenv and absolute paths via a wrapper script, exporting PATH/LANG/TZ, ensuring execute permissions and logging, and verify success by running it under a simulated cron environment.', NULL, ARRAY['python', 'logging']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Diagnose a nightly cron-scheduled CSV-to-JSON ETL that works manually but fails under cron due to CRLF line endings, a bad shebang, and reliance on relative paths/implicit virtualenv activation. Convert line endings, fix executable/shebang and use absolute paths to the venv’s Python, then verify the job runs from cron and writes the expected outputs and logs.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Diagnose why a nightly cron job defined in /etc/cron.d/etl that should transform /app/input/*.csv into /app/out/data.parquet never produces output. Identify and fix cron-specific issues (PATH/virtualenv, working directory/relative paths, stale flock lockfile) and dependency/import errors so the job runs under cron; verify by forcing a run and ensuring the Parquet is generated.', NULL, NULL);
-- Compact batch 7/29: rows 301-350

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Investigate a nightly ETL cron job that succeeds interactively but fails under cron by reproducing the cron environment, inspecting mail/cron logs, and pinpointing issues like missing PATH/virtualenv, % expansion in the command, relative paths, or SSH key permission denials. Fix the job by correcting the cron entry (escaping %, adding absolute paths and required environment), adjusting file permissions, and verifying via a forced run and persisted logs.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Automation & Cron Job Failures', 'Investigate why a nightly cron job that runs /app/etl/run_pipeline.sh succeeds interactively but fails under cron due to missing virtualenv activation, PATH differences, non-UTF-8 locale, and a stale lock blocking retries. Make the job cron-safe by using absolute paths and a venv-aware shebang, exporting LANG/LC_ALL, adding flock-based locking, correcting permissions, and verifying execution/logging when triggered by cron.', NULL, ARRAY['logging']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Data Format & Schema Mismatches', 'Debug a failing ETL that joins newline-delimited JSON to CSV lookups after an upstream change made id string-typed and some nested fields optional. Update the ingest and normalization steps to handle schema drift (type coercion, defaulting missing nested fields, dropping unknown columns) and emit a Parquet file that conforms to the provided JSON Schema.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Data Format & Schema Mismatches', 'Diagnose a Spark job that crashes when reading a partitioned Parquet dataset because certain partitions were written with conflicting types for the same columns (e.g., id as int in some, string in others, differing decimal scales). Create a command-line repair tool that scans partitions, infers a unified schema, rewrites or casts offending files, and verifies the job completes with a stable row count.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Data Format & Schema Mismatches', 'Diagnose and fix a Python CSV-to-Parquet conversion script that crashes because different input shards use inconsistent schemas (missing columns, reordered headers, mixed numeric/string types, and dual date formats). Implement a normalization step that defines a canonical schema, coerces/validates types, fills defaults for absent fields, and produces a single readable Parquet dataset verified by a provided reader script.', 'hard', ARRAY['python']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Data Format & Schema Mismatches', 'Diagnose and fix a failing ETL step where a pandas CSV loader expects comma-delimited UTF-8 with headers id,event_time,amount, but input partitions are mixed: semicolon-delimited ISO-8859-1 with BOM, and some months rename amount to total_amount or omit it entirely. Implement robust delimiter/encoding detection, header alias mapping, and default value/backfill logic, then emit Parquet conforming to a provided schema and verify via a schema-validation script.', NULL, ARRAY['pandas']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Data Format & Schema Mismatches', 'Diagnose and fix a failing Python ETL that ingests NDJSON into a SQLite table where some shards emit objects missing required fields, integers as strings, or an array instead of an object. Implement a loader that validates against a provided JSON Schema, coerces types and fills defaults, quarantines irreparable records to /app/bad_rows.ndjson, and completes the load without errors.', 'hard', ARRAY['python']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'I/O & File Handling Errors', 'A log aggregation ETL that tails /var/log/app.log misses data after rotation and crashes on reading gzipped archives due to stale file handles and incorrect decompression. Diagnose and fix the pipeline to detect log rotation (reopen on inode change), read both .log and .log.*.gz with correct permissions, and emit a complete deduplicated JSONL to /app/out.jsonl for a target date.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'I/O & File Handling Errors', 'Debug a Python ETL that merges CSVs from input/ into a single Parquet but crashes with UnicodeDecodeError and FileNotFoundError because it assumes UTF-8 and only matches *.csv while some inputs are .csv.gz and Windows-1252 or UTF-8-BOM encoded. Implement robust file discovery and decoding (support .csv/.csv.gz, handle BOM and cp1252 fallback) and ensure the output writes to out/ with proper permissions, then rerun to pass row-count and schema checks.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'I/O & File Handling Errors', 'Diagnose and fix a Python log ETL that hangs or fails with UnicodeDecodeError when consuming gzipped records from a named pipe and rotating .gz files because it treats FIFOs like regular files, seeks on non-seekable streams, and assumes UTF-8. Make the reader stream-safe, BOM/encoding-aware, and tolerant of concatenated/partial gzip members, then verify it emits a single UTF-8, LF-normalized output.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'I/O & File Handling Errors', 'Diagnose and fix a file-descriptor leak in a Python ETL that globs and processes thousands of (optionally gzipped) JSON Lines files, causing intermittent ''Too many open files'' errors. Refactor to use context managers and bounded concurrency, then verify the full dataset ingests without EMFILE failures.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'I/O & File Handling Errors', 'Diagnose why a JSONL merge ETL script silently skips inputs and corrupts records when traversing a directory tree containing spaces/newlines in filenames, symlinks, and mixed UTF-8/Latin-1 encodings. Repair the script to robustly discover files and handle .jsonl and .jsonl.gz, safely read/normalize encodings, and write a single deduplicated UTF-8 newline-delimited output to /app/merged.jsonl.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Pipeline Stage Failures', 'Debug a GNU Make–driven ETL where a CSV→JSON→SQLite pipeline intermittently fails because a normalization step writes to a .tmp file and downstream rules consume stale outputs due to misdeclared targets and misordered prerequisites. Fix the Makefile by declaring correct outputs, ordering dependencies (including any order-only deps), and verify that a full run and incremental rebuilds execute the correct stages end-to-end.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Pipeline Stage Failures', 'Debug a Snakemake ETL where a normalization step writes gzipped CSVs while declaring .csv outputs, causing downstream aggregation to miss inputs and the DAG to misorder. Fix the rule I/O patterns, compression settings, and dependencies so the pipeline runs end-to-end and emits the expected final summary.csv.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Pipeline Stage Failures', 'Debug a broken Snakemake-based ETL where a mid-pipeline ''normalize -> join -> partition'' stage fails because the normalize step emits NDJSON while the join expects a JSON array and sometimes runs before normalize completes. Identify and fix misdeclared dependencies and the schema mismatch so the pipeline orders correctly and the final partitioned Parquet output passes a row-count validation.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Pipeline Stage Failures', 'Diagnose a Snakemake ETL where a mid-pipeline rule now outputs gzipped TSV files while downstream rules still expect plain TSV, leading to a no-op aggregation and an empty final report. Align I/O patterns and shell commands across affected rules, rebuild the DAG, and verify the corrected output.', NULL, NULL),
('Debugging & Troubleshooting', 'Data & Pipeline Debugging', 'Pipeline Stage Failures', 'Diagnose and fix a Snakemake data pipeline where a mid-stage rule fails because its declared outputs don’t match the files written by the upstream step (gzip enabled, wrong suffix and temp directory), causing MissingOutputException. Align input/output patterns and compression flags, add any missing dependencies, and run the workflow end-to-end to produce /app/out/summary.csv.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Build Configuration & Toolchain Issues', 'Diagnose a CMake project whose shared plugin fails to link and cannot be loaded at runtime due to missing -fPIC objects and incorrect RPATH/SONAME. Update CMakeLists.txt to enable POSITION_INDEPENDENT_CODE, correct link order, and set a proper INSTALL_RPATH so the host executable can dlopen the plugin and pass the provided test.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Build Configuration & Toolchain Issues', 'Diagnose a CMake-based C++ project that fails to link on a clean environment due to missing transitive dependencies and non-PIC objects in a shared library. Fix CMakeLists.txt to use imported targets with correct PUBLIC/INTERFACE scopes and enable POSITION_INDEPENDENT_CODE, then rebuild and verify by running the provided demo binary.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Build Configuration & Toolchain Issues', 'Diagnose and fix a CMake project that fails to build a shared plugin because a bundled static library is compiled without position-independent code, causing relocation errors at link time. Update the build to ensure all static libraries linked into shared targets are built with -fPIC (e.g., setting POSITION_INDEPENDENT_CODE and propagating flags) and verify the plugin builds and loads with a small test program.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Build Configuration & Toolchain Issues', 'Diagnose and repair a CMake-based C++ project that fails to link a shared library due to non-PIC static dependencies and whose test binary cannot locate the built .so at runtime. Modify CMake to build static libs with POSITION_INDEPENDENT_CODE and set correct RPATH/INSTALL_RPATH so the library links and the tests execute successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Build Configuration & Toolchain Issues', 'Fix a CMake-based C++ project that fails to build shared libraries due to missing -fPIC and unresolved OpenMP symbols when using Clang. Diagnose the errors and update the CMake configuration to enable position independent code and link the correct OpenMP runtime so the project compiles and tests run successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Package Installation Failures', 'Diagnose and fix an npm install failure for the sharp image library where native module build via node-gyp breaks due to missing toolchain and system libraries or incompatible Node ABI/prebuilt binaries. Install the necessary build tools and libvips (or align Node version so prebuilt binaries work), then verify by running a Node script that loads sharp and successfully resizes an image.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Package Installation Failures', 'Diagnose and fix why pip install lxml fails due to missing system headers and build tools (libxml2/libxslt, zlib, compiler). After repair, install lxml successfully and verify by running a provided test script that parses XML without errors.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Package Installation Failures', 'Diagnose and repair a failing npm install of node-canvas that errors with node-gyp and missing libcairo/Pango headers by installing the correct system build tools and configuring pkg-config (and, if necessary, pinning to a compatible Node ABI). Verify the fix by successfully installing the package and rendering a PNG to /app/out.png using a provided test script.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Package Installation Failures', 'Diagnose and repair a pip install failure for pygraphviz on a minimal Linux image where the build cannot locate Graphviz headers/libraries. Install the required system packages and set appropriate env vars so the wheel builds, then verify by generating a PNG from a simple graph.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Package Installation Failures', 'In an Alpine Linux container, diagnose and fix pip install cryptography failing by installing the Rust toolchain and OpenSSL/musl development headers, and configuring the build environment so a source build succeeds. Verify by importing cryptography and generating an Ed25519 keypair in a short Python script.', NULL, ARRAY['container', 'cryptography', 'rust', 'python']),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'System Library & Path Errors', 'Diagnose a Python + GDAL/PROJ setup where importing osgeo.gdal fails with a libgdal.so load error or missing grid files because the shared libraries and data directories aren’t on the search path. Fix by correctly wiring PATH/LD_LIBRARY_PATH/PKG_CONFIG_PATH and GDAL_DATA/PROJ_LIB (or adding an rpath) and verify by importing GDAL and performing a coordinate transform.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'System Library & Path Errors', 'Diagnose and fix a prebuilt CLI in /app/bin/remote-sync that fails with ''error while loading shared libraries: libssl.so.1.1'' due to an OpenSSL ABI mismatch. Provide a compatible libssl/libcrypto and configure the dynamic linker (e.g., rpath/LD_LIBRARY_PATH or ldconfig) so ldd resolves all libraries and the CLI completes its test run.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'System Library & Path Errors', 'Diagnose and repair a Pillow import failure caused by a missing or mislinked system libjpeg (e.g., ''libjpeg.so.X: cannot open shared object file''). Install or relink the correct library and rebuild or reinstall Pillow, then verify JPEG load/save works in a short script.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'System Library & Path Errors', 'Diagnose and repair a prebuilt ELF CLI in /app/bin that fails with ''error while loading shared libraries: libfoo.so.X: cannot open shared object file'' due to a missing or misconfigured runtime library search path. Locate the correct library directory and fix the binary via RPATH (patchelf) or environment (LD_LIBRARY_PATH) so the tool executes a provided command successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'System Library & Path Errors', 'Diagnose why the prebuilt CLI at /app/bin/tool fails to start with missing libssl/libstdc++ errors due to ABI/version mismatches and incorrect library search paths. Repair by vendoring compatible shared libraries and fixing the loader path (RPATH/RUNPATH or LD_LIBRARY_PATH via patchelf or a wrapper) so the tool runs and its self-test completes successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Version Incompatibility', 'Diagnose a Node.js project that fails with a ''module was compiled against a different Node-API/ABI'' error due to a mismatch between the installed Node version and native addon binaries (e.g., sharp/sqlite3). Align versions by switching Node via nvm/asdf or rebuilding addons, then verify the app runs a provided command successfully.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Version Incompatibility', 'Diagnose a Rust workspace that fails to build because the pinned dependencies require a newer Rust toolchain/edition than the installed rustc and cargo. Resolve by activating a compatible toolchain (e.g., via rustup or a directory override) or re-resolving to compatible crate versions, then build and run the tests successfully.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Version Incompatibility', 'Diagnose and fix a Node.js TypeScript project where builds/tests fail under Node 20 due to incompatible versions of TypeScript, ts-node/ts-jest, and Jest (ESM/CJS loader and peer-dependency errors). Align and pin a compatible toolchain or update configs for ESM/CJS correctly, then verify npm run build and npm test succeed.', NULL, ARRAY['typescript']),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Version Incompatibility', 'Diagnose and fix a Node.js project that fails to start due to a native addon compiled for an incompatible Node-API/ABI (e.g., “Module version mismatch” or “Module did not self-register”). Align the Node.js/runtime version and rebuild the addons (or select a compatible prebuilt) so the program runs and the addon loads successfully.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Dependency & Build Troubleshooting', 'Version Incompatibility', 'Diagnose and fix a Rust project’s build failure where the openssl-sys crate is incompatible with the system OpenSSL version (e.g., libssl3 vs libssl1.1). Apply a minimal remedy (pin a compatible crate version, install matching OpenSSL dev headers, or enable the vendored feature), then rebuild and verify the binary performs a successful HTTPS request.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Configuration File Parsing & Validation', 'Diagnose a failing Go HTTP service that crashes on startup due to a YAML configuration using anchors/aliases, merge keys (<<), and ${ENV} placeholders that its parser does not support. Normalize the config by materializing merges, expanding environment variables, fixing type mismatches against a provided JSON Schema, and verify the service boots and passes the health-check script.', NULL, NULL),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Configuration File Parsing & Validation', 'Diagnose and fix a CI pipeline failure caused by a YAML config that mixes tabs, YAML 1.1 booleans (on/off), merge keys/anchors, and unexpanded ${VAR} placeholders that the runner’s parser rejects. Normalize indentation, quote literals per YAML 1.2, inline merged sections, supply safe defaults for env placeholders, and verify with yamllint and a successful dry-run of the pipeline.', NULL, NULL),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Configuration File Parsing & Validation', 'Diagnose and fix a broken Docker Compose v2 YAML that misuses anchors/merge keys and ${VAR} interpolation, triggering schema/type errors on docker compose. Refactor to a valid v2 schema (e.g., via x- extension fields and correct quoting), supply a minimal .env, and verify with docker compose config and a successful service start.', NULL, ARRAY['docker']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Configuration File Parsing & Validation', 'Diagnose why Prometheus fails to start by fixing prometheus.yml and the referenced rules/*.yml: tabs, a UTF-8 BOM, inconsistent indentation, and a stringified numeric scrape_interval cause YAML parsing and schema validation errors. Normalize the files (no tabs/BOM), correct types and rule paths to satisfy the Prometheus schema, and verify with promtool check config and promtool check rules.', NULL, NULL),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Configuration File Parsing & Validation', 'Diagnose why a Prometheus server fails to start due to a malformed prometheus.yml (bad indentation, duplicate scrape_configs, and invalid relabel_configs). Fix the YAML so promtool check config passes and the server can start and scrape a bundled dummy target, verified by a successful GET of /-/ready.', NULL, NULL),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Cross-Platform Environment Differences', 'Diagnose a Python CLI project that runs on macOS/Windows but fails on Linux due to case-mismatched imports and CRLF-terminated helper scripts causing /bin/sh^M errors. Make it Linux-portable by correcting import/module casing and normalizing line endings and path handling, then verify the CLI executes successfully end-to-end.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Cross-Platform Environment Differences', 'Diagnose a Python CLI that crashes with UnicodeEncodeError on Linux but works on macOS due to running under the C/POSIX locale. Generate and set a UTF-8 locale in the container (e.g., en_US.UTF-8), update startup config to export LANG/LC_ALL, and verify it correctly processes non-ASCII filenames.', NULL, ARRAY['python', 'container']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Cross-Platform Environment Differences', 'Diagnose why a Node.js project that builds on Windows/macOS fails in the Linux container due to CRLF shebangs, case-insensitive import paths, and Windows-only path separators in package scripts, and make it fully portable. Verify by normalizing line endings, fixing import path casing, updating scripts to cross-platform commands, and ensuring npm ci && npm run build && npm test complete successfully in the container.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Cross-Platform Environment Differences', 'Diagnose why project entrypoint scripts run on Windows/macOS but fail in Linux containers with errors like ''/usr/bin/env: python\r: No such file or directory'' or ''Permission denied''. Normalize line endings to LF, fix shebangs, set executable bits, and add .gitattributes rules to enforce cross-platform-safe endings, then verify the scripts execute successfully.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Cross-Platform Environment Differences', 'Investigate a Python CLI project that works on Windows but fails on Linux with a python^M shebang error and a ModuleNotFoundError from a case-mismatched import. Fix by converting CRLF to LF, correcting import/file casing, and adding .gitattributes rules to enforce LF line endings and case-consistent paths.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Environment Variable Misconfiguration', 'A Node.js Express app crashes at startup with ''Missing ENV: DATABASE_URL, JWT_SECRET'' even though a .env file exists. Diagnose why environment variables are not loaded in the containerized runtime, load or export them correctly so the app starts, and verify the /health endpoint returns 200.', NULL, NULL),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Environment Variable Misconfiguration', 'A local API test suite times out because HTTP_PROXY/HTTPS_PROXY force localhost traffic through an external proxy and NO_PROXY is misconfigured. Diagnose and fix the proxy-related environment variables (including IPv4/IPv6 loopbacks and custom ports) so requests reach the local service and the test script completes successfully.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Environment Variable Misconfiguration', 'Diagnose and fix a CLI application’s UnicodeDecodeError caused by running under the POSIX C locale with LANG/LC_* unset or misconfigured. Configure and activate a UTF-8 locale and appropriate environment variables so the program can read and print UTF-8 (including emojis) without errors.', NULL, NULL);
-- Compact batch 8/29: rows 351-400

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Environment Variable Misconfiguration', 'Diagnose and fix a CLI tool that crashes on Unicode filenames due to a non-UTF-8 locale by correctly configuring LANG/LC_ALL and generating the required UTF-8 locale in the container. Persist the fix for both interactive and non-interactive shells, then verify the tool processes files with accented characters without errors.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Environment Variable Misconfiguration', 'Diagnose why both curl and a Python requests client fail HTTPS verification against a provided local TLS service, tracing it to a missing CA trust path. Configure SSL_CERT_FILE or SSL_CERT_DIR to point at the bundled CA bundle so both tools succeed, and verify by performing a successful GET request.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Virtual Environment & Container Issues', 'Diagnose a Docker image where a preinstalled Conda environment is not used at runtime, causing imports to fail because the container launches with a non-login shell that never activates Conda. Fix the Dockerfile/entrypoint to reliably run under the intended Conda env (e.g., conda run -n app or sourcing conda.sh) and verify by executing a provided script that imports numpy and prints its version.', NULL, ARRAY['docker', 'container']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Virtual Environment & Container Issues', 'Diagnose and fix a relocated Python virtual environment whose scripts and sys.path still reference an old absolute interpreter path, causing entry points and imports to fail. Recreate or patch the venv so it binds to the container’s Python and current path, then verify by activating it and successfully running the project’s console script and an import/version check.', NULL, ARRAY['python', 'container']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Virtual Environment & Container Issues', 'Fix a Poetry-managed Python app in Docker where `poetry run` fails because Poetry is bound to a stale global virtualenv created with a different Python version outside the container. Reconfigure Poetry to create an in-project venv with the container’s interpreter, rebuild the environment cleanly, and verify the app’s CLI executes end-to-end.', NULL, ARRAY['python', 'docker', 'container']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Virtual Environment & Container Issues', 'In a Docker image, a prebuilt Python virtualenv was moved to a new path, causing console_scripts to fail with ''bad interpreter'' and C-extension imports to break due to a mismatched sys.base_prefix. Diagnose and re-home the venv without reinstalling packages by fixing pyvenv.cfg, rewriting shebangs and activation scripts, cleaning conflicting PYTHONPATH/site-packages, and verify by running a bundled CLI and importing a compiled wheel (e.g., numpy).', NULL, ARRAY['docker', 'python']),
('Debugging & Troubleshooting', 'Environment & Configuration Debugging', 'Virtual Environment & Container Issues', 'In an Alpine-based Docker container, diagnose why importing a pip-installed C-extension fails due to manylinux/glibc vs musl incompatibility. Resolve by ensuring compatible musllinux wheels or required runtime libraries (or switching base image) and verify the fix with a successful import and small test run.', NULL, ARRAY['docker', 'container']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'API Request & Response Issues', 'Diagnose and fix intermittent HMAC-signed API request failures to a local microservice caused by mismatched JSON canonicalization and header normalization between client and server. Update the client to produce a canonical UTF-8 JSON body with stable key order and correct Content-Type/Date headers before signing, then verify end-to-end and add a regression test.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'API Request & Response Issues', 'Diagnose why a POST JSON request to a REST API fails after an HTTP redirect (e.g., 301/302 to HTTPS) because the client changes the method or drops the body, leading to 405/400 responses. Fix the client or invocation to target the canonical base URL or preserve method/body on redirect (use 307/308 semantics), and verify the corrected request succeeds with the expected JSON response.', NULL, ARRAY['rest', 'api']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'API Request & Response Issues', 'Diagnose why a Python CLI using requests gets 401 and 415 from a local OAuth2-protected REST API: the token exchange is incorrectly sent as JSON instead of application/x-www-form-urlencoded and subsequent requests pass the token as a query param instead of Authorization: Bearer. Fix the payload encoding and header usage, then verify by successfully calling a paginated endpoint while honoring Retry-After to avoid 429s.', NULL, ARRAY['python', 'rest', 'api']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'API Request & Response Issues', 'Diagnose why a provided CLI client receives 401 ''signature mismatch'' from a mock HMAC-signed REST API by auditing canonicalization (query param order, header inclusion, body hashing) and timestamp/nonce handling. Correct the signing logic and required headers so POST /v1/payments returns 201, then verify by GETting the created resource and writing its id to /app/payment_id.txt.', NULL, ARRAY['rest', 'api']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Connection & Timeout Errors', 'Diagnose and fix slow HTTP requests caused by the system preferring IPv6 (AAAA) addresses that are unroutable, leading to connection timeouts before IPv4 fallback. Reconfigure address selection, routing, or resolver settings so connections to a dual-stack host complete quickly without timeouts.', NULL, NULL),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Connection & Timeout Errors', 'Diagnose why curl/http clients to a local service on localhost:5000 time out because global proxy settings force localhost traffic through an unreachable HTTP(S) proxy. Remove or correctly scope the proxy configuration (e.g., NO_PROXY, environment vars, tool configs) and verify direct local connectivity works from curl and a sample client.', NULL, NULL),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Connection & Timeout Errors', 'Diagnose why outgoing HTTP requests from a CLI client to a local/internal API consistently time out due to misconfigured proxy environment variables (HTTP_PROXY/HTTPS_PROXY/NO_PROXY) that route localhost/intranet traffic through a dead proxy. Correct the environment and client/server settings so direct connections to 127.0.0.1 and internal hostnames bypass the proxy and succeed, verifying with curl and a simple script.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Connection & Timeout Errors', 'Investigate persistent timeouts when a local microservice calls an internal API and discover they are caused by inherited HTTP(S)_PROXY environment variables pointing to a dead proxy. Reconfigure NO_PROXY or unset the proxy so direct connections succeed, and verify with curl and a test run.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Connection & Timeout Errors', 'Investigate why a client app’s HTTP requests to an internal service stall before timing out: the hostname resolves to IPv6 while the server only binds on IPv4. Fix the mismatch (e.g., enable dual-stack binding or force IPv4 resolution) and verify low-latency success with the provided test script.', NULL, NULL),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Proxy, SSL & Certificate Errors', 'A Python client fails mutual TLS to a Flask service behind an Nginx reverse proxy because the issued certificates lack SAN/extendedKeyUsage and the proxy serves an incomplete certificate chain. Regenerate CA/server/client certs with proper SAN and EKU, configure Nginx to present the full chain and require client auth, update the client trust store, and verify successful requests with curl and a provided test script.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Proxy, SSL & Certificate Errors', 'Diagnose and fix a Node.js service that fails on outbound HTTPS due to an intercepting proxy and missing CA trust while also misrouting internal calls through the proxy. Configure HTTP(S)_PROXY and NO_PROXY correctly and provide the proxy’s root CA to Node (e.g., NODE_EXTRA_CA_CERTS) so external requests succeed via the proxy and internal hosts bypass it, verified by two test fetches.', NULL, NULL),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Proxy, SSL & Certificate Errors', 'Diagnose and fix an Nginx reverse proxy that returns 502 due to a failed TLS handshake with an HTTPS upstream caused by disabled SNI and an untrusted/intermediate-missing CA chain. Update the proxy’s TLS settings (enable SNI, set the correct upstream host, provide a trusted CA bundle) and verify that curling the proxied endpoint over HTTPS succeeds.', NULL, NULL),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Proxy, SSL & Certificate Errors', 'Diagnose and repair a Maven-based Java project that fails to download dependencies through a corporate HTTPS proxy performing TLS interception. Import the provided root CA into the JVM truststore, configure authenticated proxy settings without disabling SSL verification, and verify mvn dependency:resolve succeeds.', NULL, ARRAY['java']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Proxy, SSL & Certificate Errors', 'Fix a broken mutual TLS setup on an Nginx reverse proxy fronting a local API: clients see handshake failures and ''unknown ca'' errors due to an incomplete server certificate chain and verification against the wrong client CA. Rebuild and reference the correct fullchain, configure nginx to trust the proper client CA, issue a client certificate/key, reload, and verify curl with the client cert succeeds while requests without it are rejected.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Service Availability & Health Checks', 'Diagnose a FastAPI service managed by supervisord that never becomes healthy because it binds only to 127.0.0.1 and its /healthz endpoint crashes on missing environment configuration. Reconfigure binding to 0.0.0.0 and make the health check independent of external services, then verify the container healthcheck and curl to /healthz return 200.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Service Availability & Health Checks', 'Diagnose a Nginx→Gunicorn→Flask stack that never passes its health check because Nginx proxies to a nonexistent unix socket, Gunicorn is listening on TCP port 8001, and the /health endpoint hard-depends on a missing database. Reconfigure the upstream to the correct port and make the health route shallow so the service reports healthy and curl to /health returns 200.', 'hard', ARRAY['database']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Service Availability & Health Checks', 'Diagnose a Python FastAPI service that returns 500 on /health only when started by systemd due to an incorrect working directory and missing environment variables. Fix the systemd unit (WorkingDirectory, Environment/EnvironmentFile, ExecStart, dependencies) and any file paths so the service is active and curl localhost/health returns 200.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Service Availability & Health Checks', 'Diagnose an Nginx-proxied FastAPI service whose /health endpoint returns intermittent 502s due to IPv6/IPv4 mismatch (upstream resolving to ::1 while the app binds only to 127.0.0.1). Adjust the app bind address or Nginx upstream to ensure consistent HTTP 200 health checks across restarts.', NULL, NULL),
('Debugging & Troubleshooting', 'Network & Service Debugging', 'Service Availability & Health Checks', 'Diagnose and fix a misconfigured reverse proxy that causes a healthy backend service to fail its /healthz check (e.g., wrong upstream port, missing Host/X-Forwarded headers, or TLS-to-HTTP mismatch). Correct the proxy and service configs and reload them so curl http://localhost/healthz returns 200 with the expected body.', NULL, ARRAY['backend']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'CPU & Memory Profiling', 'Profile a C log-parsing utility that becomes slow and runs out of memory on a large dataset due to quadratic string concatenation and leaked allocations. Use Valgrind (memcheck/massif) or perf/callgrind to pinpoint hotspots, refactor to buffered/streamed processing and proper frees, and verify identical output with markedly lower peak RSS and faster runtime.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'CPU & Memory Profiling', 'Profile a C++ image-processing CLI that becomes CPU-bound and steadily grows memory during a large batch, using perf and Valgrind (callgrind/massif) to pinpoint an O(n^2) hot loop and unbounded container growth. Implement fixes to achieve at least 2x faster runtime and reduce peak RSS by 50% or more, verified by a provided bench.sh baseline/after comparison.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'CPU & Memory Profiling', 'Profile a Go HTTP log aggregator that shows steadily rising memory usage and high CPU under sustained load using pprof (heap and CPU profiles) to uncover a goroutine leak and an O(n^2) JSON concatenation hotspot. Implement fixes (proper context cancellation, bounded channels, and bytes.Buffer-based assembly) and verify with the provided load test that throughput improves and peak RSS remains bounded.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'CPU & Memory Profiling', 'Profile a Python CSV-to-JSON pipeline that pegs a CPU core and steadily increases RSS on large inputs. Use cProfile and tracemalloc to locate an O(n^2) dedup step and an unbounded cache, refactor to a streaming/chunked approach with bounded caching, and verify at least 2× speedup and <200 MB peak memory on a 1M-row dataset.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'CPU & Memory Profiling', 'Profile a mixed Python+C log-processing tool to locate CPU hotspots and memory bloat using cProfile, perf, and Valgrind (memcheck/massif). Eliminate an O(n^2) Python loop and fix a C-side leak so the job completes ≥2x faster with ≥50% lower peak RSS while preserving identical output.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'I/O & Disk Bottlenecks', 'Diagnose a C-based log aggregator that is extremely slow due to 1-byte read() calls and an fsync/flush on every line. Refactor it to use buffered streaming and batched writes (e.g., setvbuf/FILE* or coalesced writev) and verify /app/bench.sh runs at least 5x faster without changing output.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'I/O & Disk Bottlenecks', 'Diagnose a Python SQLite ingestion script that is painfully slow due to per-row transactions causing fsync storms and page cache thrashing. Use strace/iostat to confirm the I/O bottleneck, then optimize by batching transactions and enabling WAL with appropriate PRAGMAs to achieve >5× speedup while preserving identical database contents.', NULL, ARRAY['python', 'database']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'I/O & Disk Bottlenecks', 'Diagnose a Python log-processing CLI that is extremely slow because it writes output one line at a time with fsync after each write and a tiny buffer. Rework it to batch and buffer writes and use atomic rename on completion, remove unnecessary fsyncs, and verify at least a 5x speedup on the provided dataset with byte-for-byte identical output.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'I/O & Disk Bottlenecks', 'Diagnose and fix a severe disk I/O bottleneck in a Python-based SQLite bulk loader that inserts rows individually (autocommit on), triggering per-row fsyncs and tiny writes. Optimize by batching transactions and enabling WAL with appropriate PRAGMAs (e.g., synchronous=NORMAL, page/cache tuning) to achieve a measurable speedup while preserving identical dataset contents.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'I/O & Disk Bottlenecks', 'Diagnose why a Python log-processor that splits records per user is extremely slow due to per-line open/flush/close and tiny writes causing fsync storms. Use strace/iostat to pinpoint the issue, then refactor to maintain buffered file handles and batch writes, verifying identical output and improved runtime.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Network Performance Tuning', 'Diagnose a severe throughput collapse and sporadic stalls across a VXLAN tunnel caused by an MTU black hole (broken PMTUD), using ping with DF and tcpdump to determine the effective path MTU. Apply a fix by setting appropriate interface MTUs and TCP MSS clamping so iperf3 throughput improves at least 3x without packet loss, and write the discovered MTU/MSS to /app/mtu_report.txt.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Network Performance Tuning', 'Diagnose and fix severely limited throughput between two containers on a high-RTT simulated link by identifying TCP window/buffer and congestion-control misconfiguration. Measure with iperf3 and ss/tcpdump, then tune sysctl (enable window scaling, raise rmem/wmem and tcp_rmem/tcp_wmem, switch to BBR with fq, adjust MTU/MSS if needed) and verify higher goodput with fewer retransmits.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Network Performance Tuning', 'Diagnose and optimize a gRPC microservice’s network stack under a simulated 80 ms RTT and 1% packet loss to cut p99 latency by at least 50% without reducing throughput. Apply and verify kernel- and app-level tuning (e.g., congestion control/qdisc, HTTP/2 flow-control windows, TCP keepalive/Nagle, MTU/MSS) using the provided load generator and report.', NULL, ARRAY['grpc']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Network Performance Tuning', 'Diagnose and optimize poor TCP throughput between two local endpoints under simulated WAN latency by measuring with iperf3, inspecting kernel TCP settings, and identifying buffer/congestion-control bottlenecks. Apply sysctl/qdisc tuning (e.g., enable BBR, adjust rmem/wmem and net.core.{r,w}mem_max, set fq qdisc) and verify at least a 2× throughput improvement.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Network Performance Tuning', 'Diagnose why a Python asyncio TCP client/server for log shipping achieves poor throughput and high p99 latency due to tiny writes triggering Nagle/delayed-ACK interactions and lack of connection reuse. Tune by enabling TCP_NODELAY, batching into 64KB frames, increasing socket buffers and enabling keep-alive/pooling, then verify a 5–10× throughput gain and sub-20ms p99 with the provided load generator.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Parallelization & Concurrency Bugs', 'Diagnose a deadlock and throughput collapse in a Rust Tokio-based pipeline where a Mutex is held across await points and a bounded mpsc channel backpressures a blocking file writer. Refactor to avoid cross-await locks and move blocking I/O to spawn_blocking or a dedicated thread pool, then verify no hangs under a provided stress script and achieve at least 3× higher throughput.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Parallelization & Concurrency Bugs', 'Diagnose a throughput collapse in a Java service where CompletableFuture chains run on a fixed-size ExecutorService, causing thread-starvation deadlocks under load. Refactor the scheduling (e.g., non-blocking composition or separate executors for nested tasks) so the load test completes without timeouts and throughput improves by at least 2x.', NULL, ARRAY['java']),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Parallelization & Concurrency Bugs', 'Diagnose and fix a deadlock and goroutine leak in a Go fan-out/fan-in worker pool where misordered channel closes and blocking sends during cancellation stall the pipeline under load. Correct the synchronization (context propagation, channel buffering, and close/drain order) so the provided stress test finishes under the time limit and go test -race shows no leaks or races.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Parallelization & Concurrency Bugs', 'Diagnose and fix a deadlock in a Go-based concurrent log processor where workers hold a mutex while sending to a results channel, creating a circular wait with the aggregator that acquires the same lock. Refactor to avoid holding locks across channel operations (e.g., copy before send or introduce a dispatcher) and verify under a stress harness that prior hangs disappear and throughput improves.', NULL, NULL),
('Debugging & Troubleshooting', 'Performance & Resource Optimization', 'Parallelization & Concurrency Bugs', 'Diagnose and fix an intermittent deadlock in a Rust Tokio service caused by holding a std::sync::Mutex across await points and doing blocking file I/O inside async tasks, leading to hangs under load. Refactor to use tokio::sync::Mutex/RwLock and spawn_blocking for I/O, add cancellation timeouts, and verify via a stress script, cargo test, and tokio-console traces.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Dependency & Environment Audit', 'Diagnose why a Node.js project’s native addon (e.g., sharp or grpc) fails to load with errors like GLIBC_x.y not found or wrong ELF class inside the container. Audit Node ABI vs compiled binaries and system libc/libstdc++ versions, rebuild or pin a compatible binary with node-gyp and verify the addon loads in a minimal script.', NULL, ARRAY['grpc', 'container']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Dependency & Environment Audit', 'Diagnose why the prebuilt /app/server web binary exits on launch with “No such file or directory” and fix it by auditing dynamic library dependencies and environment drift (e.g., missing libsqlite3 due to CGO). Install or rebuild with the correct dependencies (or static linking), verify the /health endpoint responds, and write a brief postmortem to /app/POSTMORTEM.md.', NULL, ARRAY['web']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Dependency & Environment Audit', 'Investigate a prebuilt C++ CLI tool that fails at startup with a “GLIBCXX_x.y not found” error inside the container. Identify the ABI mismatch between the binary and the system libstdc++/glibc, remediate by aligning runtime libraries or rebuilding, and verify the tool runs successfully end-to-end.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Dependency & Environment Audit', 'Investigate why importing a pip-installed C++ extension (e.g., scikit-learn) now fails with GLIBCXX/CXXABI symbol errors after a base image or compiler runtime change by auditing the system’s libstdc++/libgcc versus the wheel’s required ABI. Resolve by aligning the C++ runtime or installing compatible wheels/pins, then verify with a minimal script that imports and exercises the package.', NULL, NULL);
-- Compact batch 9/29: rows 401-450

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Error Reproduction & Isolation', 'Diagnose an intermittent segfault in a C utility that only occurs when compiled with -O2. Reproduce by varying compiler flags and inputs, then reduce the program to a minimal C file that crashes reliably and reveals the underlying undefined behavior.', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Error Reproduction & Isolation', 'Reproduce a flaky ''Address already in use'' failure by isolating the smallest script that deterministically triggers a TCP port reuse race (TIME_WAIT) during rapid server restarts. Build a tiny server/client and use only standard OS tools to narrow the failure to missing socket cleanup/options, yielding a minimal reproducible case.', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Error Reproduction & Isolation', 'Reproduce an intermittent deadlock in a Go HTTP server that occurs only under specific GOMAXPROCS values and concurrent request patterns, then isolate a minimal single-file program and deterministic request sequence that triggers it. Capture the exact environment and commands used (race detector, pprof blocking profile, goroutine dumps) to demonstrate the deadlock reliably.', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Error Reproduction & Isolation', 'Reproduce an intermittent timestamp parsing failure that occurs only during daylight-saving transitions by controlling TZ and the system clock inside the sandbox. Isolate the minimal failing input and code path by iterating across DST boundary timestamps and documenting the exact timezone/offset combination that triggers the error.', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Error Reproduction & Isolation', 'Reproduce and isolate a CLI tool crash that emits ''Broken pipe'' or exits non-zero when its stdout is piped to head/grep -m1, identifying the minimal code path that triggers SIGPIPE/EPIPE under controlled conditions. Implement graceful handling so the pipeline exits cleanly without stack traces or spurious errors.', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Postmortem Documentation & Preventive Measures', 'Investigate a midnight outage where Nginx served 502s after a certbot renewal triggered an automatic reload that pulled in an incomplete vhost via a wildcard include. Diagnose configs/logs, implement a safe reload gate (nginx -t and atomic writes), and produce a postmortem detailing root cause, impact, and preventive measures.', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Postmortem Documentation & Preventive Measures', 'Investigate a production failure where package installs and writes error with ''No space left on device'' despite ample free disk space, trace the root cause to inode exhaustion from a runaway temp/log file generator, and validate via df -i and remediation. Produce a concise postmortem detailing impact, timeline, contributing factors, and preventive measures such as file-count monitoring, log rotation policies, tmp cleanup jobs, and quotas.', NULL, ARRAY['monitoring']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Postmortem Documentation & Preventive Measures', 'Investigate an incident where API requests intermittently returned 401 due to container clock drift causing JWT nbf/exp validation failures around a DST transition. Reconstruct the timeline from logs and metrics, identify the root cause, and produce a postmortem summarizing impact, detection, and preventive measures (time synchronization, validation leeway, targeted monitoring/alerts).', NULL, ARRAY['api', 'container', 'monitoring']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Postmortem Documentation & Preventive Measures', 'Investigate intermittent authentication failures (401/403) traced to JWT validation errors caused by container clock drift and misconfigured timezone/time sync. Write a postmortem detailing timeline, root cause, impact, detection gaps, and concrete preventive measures (time synchronization policy, startup time sanity checks, monitoring/alerts, and validation tolerances).', NULL, ARRAY['container', 'monitoring']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Postmortem Documentation & Preventive Measures', 'Investigate why a nightly data export executed twice and produced corrupted partial uploads by reconstructing the timeline from cron logs, application logs, and a stale PID lock during a DST transition. Write a postmortem detailing root cause, impact scope, contributing factors, and preventive measures (timezone policy, idempotent runs, and robust lock hygiene).', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Traceback & Stack Analysis', 'Diagnose a Java service hang by capturing and analyzing multiple jstack thread dumps to trace a lock-order inversion deadlock (each thread waiting on the other’s monitor) and identify the exact code paths involved. Implement a fix by enforcing a consistent lock acquisition order or using tryLock with timeouts, rebuild, and verify the service remains responsive under concurrent load.', NULL, ARRAY['java']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Traceback & Stack Analysis', 'Investigate a Java service that fails at startup with a NoSuchMethodError originating from org.apache.commons.lang3 in the stack trace, indicating JAR hell from conflicting transitive dependencies. Use gradle dependencies to trace the classpath, add dependency constraints or exclusions to resolve the conflict, then rebuild and verify the service starts and a provided smoke test passes.', NULL, ARRAY['java']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Traceback & Stack Analysis', 'Investigate a Node.js CLI that crashes on startup with "RangeError: Maximum call stack size exceeded"; run with --trace-uncaught and --trace-warnings to obtain full stack traces and pinpoint a circular require/import loop causing infinite recursion. Refactor the modules to break the cycle and verify the program starts and returns the expected output.', NULL, NULL),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Traceback & Stack Analysis', 'Investigate a Python ProcessPool-based task that fails with BrokenProcessPool by reading the exception chain and worker stack traces to find the underlying pickling error caused by a nested/local function. Refactor the callable to be picklable (e.g., move to module scope) and verify the parallel job completes successfully.', NULL, ARRAY['python', 'parallel']),
('Debugging & Troubleshooting', 'Root Cause Analysis & Postmortem Debugging', 'Traceback & Stack Analysis', 'Investigate an intermittently crashing Go HTTP service by analyzing its panic log and full goroutine dump to trace the execution path to a ''concurrent map writes'' failure and pinpoint the offending map and call sites. Implement a minimal synchronization fix and a stress test that runs with -race to verify the panic no longer occurs.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Access Policy & Role Misconfiguration', 'Diagnose and correct a MinIO S3 setup where a data-ingest user cannot list or upload objects due to conflicting bucket and user policies. Apply the least-privilege fix by adjusting the bucket and user policies so that aws s3 ls and multipart uploads succeed only within the intended bucket/prefix.', NULL, ARRAY['aws']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Access Policy & Role Misconfiguration', 'Diagnose and fix a Redis ACL misconfiguration that blocks a non-default user from executing required commands (e.g., SET/GET) on a specific key prefix used by a sample app. Update the ACLs to grant least-privilege access for that user and verify the app completes successfully end-to-end.', NULL, ARRAY['redis']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Access Policy & Role Misconfiguration', 'Diagnose and repair a Kubernetes RBAC misconfiguration that prevents a CI service account from applying resources in a specific namespace. Adjust Roles/ClusterRoles and the corresponding (Cluster)RoleBindings to grant the minimal verbs and scopes required so a provided kustomize/helm deploy completes successfully using only that service account.', NULL, ARRAY['kubernetes']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Access Policy & Role Misconfiguration', 'Diagnose why a data-ingestion script receives 403 AccessDenied when uploading to an S3-compatible MinIO bucket due to a mismatched bucket policy and user credentials. Correct the bucket and user policies to grant least-privilege PutObject/ListBucket on the intended prefix, update the client configuration, and verify the script completes successfully with upload and listing.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Access Policy & Role Misconfiguration', 'Fix a Kubernetes RBAC setup where the ci-deployer ServiceAccount cannot create or patch Deployments in the staging namespace, yet is mistakenly allowed to read Secrets cluster-wide. Diagnose Role/ClusterRole and (Cluster)RoleBinding scope and apiGroup/verb mismatches, then rework the policy to grant least-privilege deploy rights only in staging and no Secrets access; verify with kubectl auth can-i and by successfully applying the provided manifest.', NULL, ARRAY['kubernetes']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Authentication & Authorization Failures', 'Debug an AWS CLI profile that intermittently fails to access an S3 bucket via SSO + AssumeRole (AccessDenied and ExpiredToken errors). Refresh the SSO cache, correct the profile’s role_arn/region, and ensure the role has s3:ListBucket/s3:GetObject/s3:PutObject permissions; verify by listing the bucket and uploading a test object.', NULL, ARRAY['aws']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Authentication & Authorization Failures', 'Diagnose and fix a local PostgreSQL login failure for user ''app'' where URI-based connections are rejected due to pg_hba.conf rule order, SCRAM-vs-MD5 password mismatch, and missing LOGIN/CONNECT privileges. Correct the configuration and role settings, reload PostgreSQL, and verify with psql postgresql://app:app@localhost:5432/appdb -c SELECT 1.', NULL, ARRAY['postgresql']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Authentication & Authorization Failures', 'Diagnose and repair a local PostgreSQL connection that fails with pg_hba.conf rejection and mismatched password method (SCRAM vs md5), preventing a bundled Python app from logging in. Update pg_hba.conf, recreate the user with the correct authentication scheme and role grants, reload Postgres, and verify a psycopg2 script can perform read/write queries.', NULL, ARRAY['postgresql', 'python', 'logging']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Authentication & Authorization Failures', 'Diagnose and resolve npm 401 Unauthorized errors when installing from a local Verdaccio registry by pinpointing bad/expired credentials or scope misconfiguration in ~/.npmrc and registry ACLs. Reauthenticate or correct token scopes/registry settings so installing a private package succeeds from the terminal.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Authentication & Authorization Failures', 'Diagnose why kubectl returns Forbidden and token-related errors using the provided kubeconfig for namespace analytics. Replace the expired/invalid ServiceAccount token and create the minimal Role/RoleBinding needed, update kubeconfig, and verify by successfully applying manifests and listing pods in that namespace.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Environment Secrets & Credential Mismanagement', 'Diagnose a bootstrap script that fails to fetch secrets from a local HashiCorp Vault due to a stale VAULT_TOKEN, missing VAULT_NAMESPACE, and incorrect CA settings. Implement AppRole login using role_id/secret_id files, set VAULT_ADDR/VAULT_CACERT/VAULT_NAMESPACE correctly, and update the script so non-interactive secret retrieval and app startup succeed.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Environment Secrets & Credential Mismanagement', 'Diagnose and fix a CI/container build that fails to fetch private npm packages and Git submodules due to mismanaged tokens (missing/misnamed env vars, malformed .npmrc, and SSH key permission issues). Repair the credential chain and config so npm install and git submodule update complete non-interactively.', NULL, ARRAY['container', 'git']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Environment Secrets & Credential Mismanagement', 'Diagnose and fix a Dockerized Node.js build that fails npm install with 401 due to a missing or mis-scoped NPM_TOKEN for a private registry. Modify the Dockerfile and build command to securely pass the token only to the dependency layer and verify by successfully building and running the app that imports the private package.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Environment Secrets & Credential Mismanagement', 'Diagnose and repair a failing S3 backup in a local MinIO sandbox caused by mismanaged AWS credentials and environment (incorrect AWS_ACCESS_KEY_ID/SECRET, missing region, wrong endpoint URL, and a conflicting default profile). Reconfigure environment variables and ~/.aws files so both aws CLI and a boto3 script can authenticate and list the target bucket without hardcoding secrets.', 'hard', ARRAY['aws']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'Environment Secrets & Credential Mismanagement', 'Diagnose why sops cannot decrypt secrets.enc.yaml in the container because the AGE private key is provided as a base64, single-line environment variable that is never materialized as a key file. Reconstruct the PEM from the env var, set SOPS_AGE_KEY_FILE with correct permissions, and verify decryption by producing /app/secrets.yaml.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'File & Directory Permission Errors', 'A reverse proxy cannot connect to its backend via a Unix domain socket at /run/app.sock due to permission denied on the socket path. Diagnose and correct directory/socket ownership, mode bits, and group/ACL settings so the proxy user can connect, then verify a 200 OK from curl localhost.', NULL, ARRAY['backend']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'File & Directory Permission Errors', 'Diagnose and fix a Unix domain socket connection failing with ''Permission denied'' by auditing the socket file and its parent directory ownership/modes and the creator’s umask. Implement a least-privilege solution (e.g., dedicated group, setgid directory, corrected socket permissions), then verify the client can connect and exchange data successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'File & Directory Permission Errors', 'Diagnose and fix a reverse proxy failing with 502 because nginx cannot access a backend UNIX domain socket (EACCES) due to incorrect ownership/permissions on the socket and its parent directories. Adjust ownership, mode, and directory execute bits or set ACLs, then make the fix persistent by updating the systemd .socket/.service or tmpfiles.d configuration, and verify that curl via nginx returns 200.', NULL, ARRAY['backend']),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'File & Directory Permission Errors', 'Diagnose why a client fails to connect to a local service over a Unix domain socket (/run/app/app.sock) with EACCES and fix the underlying directory and socket ownership/permission and group-membership issues. Verify the repair by enabling the non-root client to connect end-to-end using the provided health-check script.', NULL, NULL),
('Debugging & Troubleshooting', 'Security & Access Debugging', 'File & Directory Permission Errors', 'Diagnose why a client receives ''Permission denied'' when connecting to a Unix domain socket at /app/run/metrics.sock: the socket’s parent directory lacks execute permission for the client’s group and the daemon’s umask creates overly restrictive socket permissions. Implement a least-privilege fix by adjusting directory ownership/modes and the daemon’s umask or group, then verify bidirectional communication over the socket.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Background Service Failures', 'Diagnose a systemd socket-activated service that never starts on client connect due to mismatched unit names, incorrect ListenStream/ListenUnix path, and restrictive socket directory permissions. Correct the .socket/.service units (naming, ExecStart, WorkingDirectory, User), reload and enable socket activation, and verify the daemon spawns on demand and successfully serves a test request.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Background Service Failures', 'Investigate a failing systemd timer + service that should create periodic backups but exits immediately under systemd while the same script runs fine interactively. Diagnose and fix unit misconfigurations (absolute ExecStart, WorkingDirectory, EnvironmentFile, and executable permissions), reload the daemon, and verify the timer produces a backup in /app/backups.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Background Service Failures', 'Investigate a systemd-managed Celery worker that immediately enters a restart loop because it launches before its Redis backend and points to a non-existent virtualenv, producing empty or truncated logs. Repair the unit by correcting ExecStart/WorkingDirectory/Environment, adding Requires= and After= redis-server.service, and verifying the service stays active and processes a sample task.', NULL, ARRAY['redis', 'backend']),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Background Service Failures', 'Investigate a systemd-managed daemon that immediately enters a restart loop because unit hardening (DynamicUser with ProtectSystem=full) prevents creation of its PID/log/state files. Use journalctl and unit inspection to diagnose and then update the unit to provision writable runtime/state paths (e.g., RuntimeDirectory/StateDirectory or ReadWritePaths) so the service starts and stays active, verified by a heartbeat file appearing.', 'hard', NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Background Service Failures', 'Investigate why a systemd timer’s backup.service succeeds when started manually but fails when triggered by the timer due to environment/working-directory differences and reliance on relative paths. Update the unit(s) to use absolute ExecStart, set WorkingDirectory and Environment (PATH/HOME), add logging, then verify the timer fires and completes successfully.', NULL, ARRAY['logging']),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Log Inspection & Anomaly Detection', 'Analyze journald, kernel dmesg, and application/service logs to time-correlate intermittent 5xx outages with OOM-killer events caused by a runaway process triggered by a cron job. Produce a concise root-cause timeline and propose a mitigation (e.g., systemd MemoryMax) to prevent recurrence.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Log Inspection & Anomaly Detection', 'Correlate a recurring 2–3 minute outage by analyzing nginx access/error logs, systemd-journal logs, and cron logs while compensating for a deliberate 5‑minute clock skew between components. Identify that a misconfigured logrotate postrotate script intermittently sends SIGSTOP to the web app and apply a minimal fix so the outage ceases.', NULL, ARRAY['web']),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Log Inspection & Anomaly Detection', 'Diagnose intermittent restarts of a systemd-managed service by analyzing journalctl and kernel logs to detect OOM-killer and memory pressure events time-correlated with the service’s crashes. Implement a mitigation (e.g., adjust MemoryMax or service configuration) and verify stable operation with no recurring log anomalies.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Log Inspection & Anomaly Detection', 'Investigate intermittent TLS handshake failures by scanning journalctl/syslog and application logs for anomalies like “certificate not yet valid” alongside abrupt timestamp jumps, correlating them with NTP desynchronization events. Restore correct time sync (e.g., via systemd-timesyncd or chrony) and verify by re-running a TLS client to confirm error-free handshakes and aligned log timestamps.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Log Inspection & Anomaly Detection', 'Scan syslog, auth.log, and NTP logs (including rotated .gz) to detect a surge in authentication errors and correlate it with a significant NTP time step, pinpointing the drift magnitude and window. Output a concise timeline report naming the triggering event and the impacted services (e.g., sshd, sudo) where errors align with the clock change.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Process Crashes & Core Dumps', 'A Node.js script using the sharp image library crashes with SIGSEGV; enable core dumps and use coredumpctl + gdb (with addr2line) to analyze the core and trace the fault into libvips due to an ABI/version mismatch. Repair by installing compatible libvips/sharp binaries or rebuilding the module, then verify an image resize completes without crashing.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Process Crashes & Core Dumps', 'Analyze a core dump from a Rust CLI that intermittently SIGSEGVs during FFI calls into a C library. Use gdb and addr2line to pinpoint a dangling pointer/use-after-free across the FFI boundary, patch the Rust/C bindings to fix ownership and lifetimes, rebuild, and verify the crash no longer occurs under the stress script.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Process Crashes & Core Dumps', 'Diagnose why /app/bin/ingest crashes with SIGSEGV and leaves /app/core by using gdb to obtain a fully symbolicated backtrace (installing debug symbols if needed) and tracing the fault to a NULL function pointer returned by dlsym due to a missing plugin symbol. Repair the runtime environment (e.g., install the correct .so or adjust LD_LIBRARY_PATH) or patch the loader to handle NULL safely, and verify the program runs to completion without crashing.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Process Crashes & Core Dumps', 'Investigate a Node.js service that segfaults when requiring a native addon, analyzing the generated core dump with gdb to pinpoint an ABI/version mismatch between Node and the .node binary. Rebuild the addon for the correct Node-API/ABI (e.g., via node-gyp or prebuilds), replace the artifact, and verify the service runs and responds to a simple request without crashing.', NULL, ARRAY['api']),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'Process Crashes & Core Dumps', 'Reproduce and analyze a core dump from a multithreaded C service that intermittently segfaults on SIGTERM due to a reentrant, non-async-signal-safe signal handler freeing shared memory twice. Enable core dumps, use gdb to pinpoint the crashing thread and double-free path, refactor the handler to defer work via a self-pipe and enforce single-ownership, rebuild, and verify stability under a stress harness.', NULL, NULL);
-- Compact batch 10/29: rows 451-500

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'System Resource Exhaustion', 'Diagnose a backup job that aborts with "No space left on device" even though df shows ample free space, uncovering inode exhaustion from millions of stale temp/log files. Mitigate by reclaiming inodes and preventing recurrence (e.g., targeted cleanup, log rotation, and moving TMPDIR to a larger filesystem), then rerun the backup to complete successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'System Resource Exhaustion', 'Diagnose intermittent ''EADDRNOTAVAIL'' and ''cannot assign requested address'' errors in a service that spawns many short-lived outbound TCP connections, tracing the issue to ephemeral port exhaustion and sockets stuck in TIME_WAIT. Implement mitigations (e.g., enable connection pooling, tune the ephemeral port range and TIME_WAIT reuse via sysctl in the container), then rerun load to verify stable connectivity.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'System Resource Exhaustion', 'Diagnose intermittent cargo build/test failures in a Rust workspace that end with ''Killed'' or LLVM crashes, caused by OOM from parallel compilation and a nearly full disk from stale build artifacts. Mitigate by freeing disk space, adding a swapfile, and limiting build parallelism, then rebuild and run the tests to confirm stability.', NULL, ARRAY['rust', 'parallel', 'compilation']),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'System Resource Exhaustion', 'Diagnose sporadic ''No space left on device'' errors despite ample free disk by identifying inode exhaustion caused by millions of small files in /var/tmp/app-cache. Implement cleanup and a prevention policy (e.g., tmpfiles.d or a cleanup cron) and verify new files can be created successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'System & Process Diagnostics', 'System Resource Exhaustion', 'Diagnose why package installs and temporary builds fail with ''No space left on device'' by pinpointing which filesystem is full (/var or /tmp) and identifying the largest consumers (journald logs, apt/pip caches, orphaned artifacts). Reclaim space and implement mitigations (logrotate/journald limits, apt clean, TMPDIR relocation) so subsequent installs and builds succeed reliably.', NULL, NULL),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Git & Version Control Issues', 'Diagnose and fix a Git repository that refuses to push due to a detached HEAD, a missing refs/heads/main, and an ''origin'' remote pointing at a non-bare working tree. Reconstruct the branch from reflog/lost-found, restore refs, convert or replace the remote with a bare repo, and verify new commits can be pushed and fetched cleanly.', NULL, ARRAY['git']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Git & Version Control Issues', 'Diagnose and repair a Git repo where refs are lost and the repository is stuck mid-rebase: HEAD is detached, refs/heads/main is missing due to a corrupted packed-refs, and an index.lock prevents most commands. Recover the branch from reflog and dangling commits, clean up the rebase/lock state, and complete a fast-forward push to origin/main.', NULL, ARRAY['git']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Git & Version Control Issues', 'Diagnose and repair a repository with broken Git submodules where .gitmodules contains incorrect paths/URLs and the submodule SHAs are orphaned, causing clone/update to fail. Fix the .gitmodules entries, sync configs, repoint submodules to valid remotes and commits, reinitialize recursively, and verify a fresh clone yields a clean working tree with all submodules checked out.', NULL, ARRAY['git']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Git & Version Control Issues', 'Investigate a repository that fails to clone recursively because a submodule references a non-existent commit and an outdated remote. Fix by updating .gitmodules and the submodule’s origin URL, rewriting the parent commit to point at a reachable submodule revision if necessary, and verify that a fresh --recursive clone completes with a clean working tree.', NULL, NULL),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Git & Version Control Issues', 'Repair a Git repository that fails with ''fatal: bad object HEAD'' due to a corrupted packed-refs file and a missing refs/heads directory. Reconstruct branch and tag refs from reflogs and reachable commits, restore HEAD to main, verify with git fsck, and push the recovered history to a new local bare remote.', NULL, ARRAY['git']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Local vs Remote Environment Divergence', 'Diagnose a CI build that fails because large binary assets are missing while the same repo builds locally, uncovering that Git LFS objects are not fetched in the remote environment. Install and configure git-lfs, update the checkout/build steps to fetch and checkout LFS files, and verify the build completes with the required assets present.', NULL, ARRAY['git']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Local vs Remote Environment Divergence', 'Diagnose a Node.js/TypeScript project that passes locally but fails in CI because CI uses NODE_ENV=production and npm ci --omit=dev, leaving runtime modules incorrectly placed in devDependencies unavailable. Reproduce both environments and then fix the workflow by pinning Node and reclassifying dependencies/build steps so the full test suite passes identically in both.', NULL, ARRAY['typescript']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Local vs Remote Environment Divergence', 'Diagnose why a Python test suite passes locally but fails in the provided CI container due to differing locale and timezone settings (e.g., C vs en_US.UTF-8, UTC vs local), leading to UnicodeDecodeError and timestamp assertion mismatches. Standardize encoding and timezone across environments via environment variables and test/config changes so pytest passes identically in both contexts.', NULL, ARRAY['python', 'container']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Local vs Remote Environment Divergence', 'Investigate Python tests that pass locally but fail in the provided CI-like container due to differences in locale and timezone affecting sorting, casing, and datetime parsing. Identify the mismatch (e.g., POSIX/C vs UTF-8 locale and non-UTC TZ) and reconcile it by standardizing environment settings and hardening code/tests to be locale- and TZ-independent.', 'hard', ARRAY['python', 'container']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Local vs Remote Environment Divergence', 'Investigate a Python project whose tests pass locally but fail in the provided CI container due to differing default locale (C/POSIX vs en_US.UTF-8) and timezone (UTC vs local), affecting collation and datetime formatting. Make the test run deterministic by enforcing explicit locale/TZ and locale-independent sorting/parsing so the suite passes in both environments.', NULL, ARRAY['python', 'container']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Testing & CI/CD Failures', 'Diagnose a Jest/TypeScript test suite that passes locally but fails in CI due to case-sensitive path resolution on Linux (imports reference files with incorrect casing). Correct the import paths and adjust TypeScript/Jest module resolution to enforce strict casing, add a CI check to catch casing mismatches, and verify the pipeline passes.', NULL, ARRAY['typescript']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Testing & CI/CD Failures', 'Diagnose why a pytest suite passes locally but fails in CI where LANG=C and TZ=UTC cause Unicode encoding and time-format assertions to break. Make the pipeline and/or code locale- and timezone-stable (install a UTF-8 locale, set LANG/LC_ALL/TZ, and remove locale-dependent assumptions) and demonstrate a consistent green run.', NULL, NULL),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Testing & CI/CD Failures', 'Investigate a Python pytest suite that passes locally but fails in CI due to hidden timezone/locale assumptions (naive datetime comparisons and locale-dependent formatting). Make tests deterministic by enforcing UTC and a fixed locale in both code and pipeline (e.g., TZ=UTC, LC_ALL=C) and refactoring tests to use timezone-aware datetimes or freezegun, then verify consistent green runs.', NULL, ARRAY['python']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Testing & CI/CD Failures', 'Investigate why tests that rely on large sample assets fail in CI but pass locally, discovering that Git LFS-tracked files are checked out as pointer stubs in the CI environment. Initialize and configure Git LFS in the pipeline, fetch actual binaries, and rerun the suite until all tests pass.', NULL, ARRAY['git']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Testing & CI/CD Failures', 'Reproduce and fix a CI-only failure in a React/Jest snapshot test that passes locally but fails in a headless CI container due to missing system fonts and locale/timezone differences. Identify the mismatches, install/configure the necessary fonts, set deterministic TZ/LC_* and rendering flags, and make the pipeline pass with consistent snapshots.', NULL, ARRAY['container']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Tool Misconfiguration & Dependency Drift', 'Diagnose a JavaScript monorepo where CI fails during yarn install due to a Yarn v1/v4 mismatch and missing Berry metadata (lockfile format drift, absent .yarn/releases, and no packageManager field), causing workspace binaries to be unavailable. Repair by aligning the Yarn version via Corepack, restoring .yarnrc.yml and releases, regenerating a compatible lockfile, and verify by running yarn install and yarn workspaces run test successfully.', NULL, ARRAY['javascript']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Tool Misconfiguration & Dependency Drift', 'Diagnose a Node.js monorepo where CI fails because the pipeline runs npm ci against a pnpm-managed workspace using pnpm-lock.yaml, breaking workspace links and dependency resolution. Reconfigure the workflow to use a pinned pnpm via corepack, install with pnpm in all jobs, regenerate the lockfile if needed, and verify that build and tests complete successfully.', NULL, NULL),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Tool Misconfiguration & Dependency Drift', 'Diagnose a Rust workspace that fails in a simulated CI script due to dependency drift and toolchain mismatch (yanked versions in Cargo.lock, incompatible feature flags, and an unsupported rustc). Repair by pinning a compatible toolchain, updating/patching dependencies and regenerating the lockfile so cargo build and cargo test complete successfully.', NULL, ARRAY['rust']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Tool Misconfiguration & Dependency Drift', 'Diagnose and repair a broken Git pre-commit setup where hooks (e.g., black/isort/flake8/mypy) fail due to Python interpreter mismatches and outdated hook revisions. Update .pre-commit-config.yaml and environment so pre-commit install and pre-commit run -a complete successfully and reproducibly from the terminal.', NULL, ARRAY['git', 'python']),
('Debugging & Troubleshooting', 'Toolchain & Workflow Diagnostics', 'Tool Misconfiguration & Dependency Drift', 'Fix a failing Node.js monorepo build/test workflow that uses pnpm and corepack: the CI script selects an incompatible Node version, ignores the repo’s pinned pnpm, and relies on a stale lockfile that breaks workspaces. Update the workflow and configs (e.g., .nvmrc/engines, corepack enablement, pnpm-lock.yaml/cache keys) so the provided run-ci.sh passes on a clean checkout.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Command Discovery Quests', 'Embark on a Manual Page Scavenger Hunt where each stage hides a token inside the EXAMPLES, ENVIRONMENT, or SEE ALSO sections of specific man/info/tldr entries discovered via apropos and --help. Collect all tokens and use the newly learned subcommands and rarely used flags to build a validating command pipeline that outputs the final passphrase to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Command Discovery Quests', 'Explore a custom git-style CLI called cmdschool whose subcommands are intentionally undocumented except via --help, man pages, apropos, and an interactive cmdschool tutor, and learn to combine preinstalled tools (jq, ripgrep, curl, gpg, imagemagick) to pass five stages. Finish when cmdschool status reports ''graduated'' and write the issued certificate token to /app/results.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Command Discovery Quests', 'Explore a custom lab CLI that discovers subcommands from executables named ''lab-*'' on the PATH; by chaining ''lab help'' and each subcommand''s --help/man pages, deduce the correct workflow to authenticate and extract a hidden secret. Write the recovered secret to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Command Discovery Quests', 'Explore a sandbox full of custom, undocumented CLI tools in /app/tools and hinted only via man/--help to discover how to reconstruct a corrupted, sharded archive. Infer and execute the correct pipeline (index, stitch, parity-fix, decode, decompress, verify) to recover the plaintext and write it to /app/answer.txt.', 'hard', NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Command Discovery Quests', 'Explore a sandbox of unknown CLI tools with self-documented interfaces (e.g., --help, help subcommands, and man pages) to learn their capabilities and compose them into a working pipeline. Use the discovered commands to decode a multipart artifact and write the exact recovered message to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Error Interpretation & Correction Games', 'Broken Build Bootcamp: a staged C project with intentional faults (misconfigured Makefile flags, segfaults, and undefined behavior) driven by a tutor CLI that provides compiler/linker logs, sanitizer traces, and unit-test feedback. Progress by interpreting each failure and patching the Makefile and C sources until every stage reports PASS.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Error Interpretation & Correction Games', 'Interact with a Shell Clinic tutor that presents five stages of broken Bash scripts; running clinic stageN executes tests, shows minimal logs, and offers targeted hints after each failure. Diagnose and fix quoting, word-splitting, pipefail, subshell, and locale/sort bugs until all stages pass, then write the provided success token to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Error Interpretation & Correction Games', 'Interact with a simulated init system where a multi-service stack fails to start due to curated faults (dependency order, bad permissions, missing env vars, and port conflicts). Use logs and status tools to identify and fix each issue until all services are active, then write the final status summary to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Error Interpretation & Correction Games', 'Progress through a multi-level ''Compiler Clinic'' where each stage is a tiny project (Bash, Python, C, SQL) seeded with exactly one failure, from syntax slips to misconfigured env vars. Run a minimal ./judge that reveals only the first error hint per level, iteratively fix issues to unlock the next stage and finish when all levels report ALL GREEN.', NULL, ARRAY['python', 'sql']),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Error Interpretation & Correction Games', 'Repair a staged break-and-teach microservice where each run of ./grade.sh reveals the next failure (dependency conflict in requirements, timezone-induced flaky test, and a CLI parsing error masking a logic bug). Interpret logs and stack traces to apply minimal targeted fixes in code or config until all stages pass and the script writes PASS to /app/result.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Progressive Learning Environments', 'A progressive Git kata inside a pre-seeded repository where a mentor CLI unlocks each level after verifying the repository state. The agent advances by performing real git operations—branching/merging, resolving conflicts, bisecting a failing test, interactive rebasing to rewrite history, and final tag signing—then outputs the mentor’s token to /app/result.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Progressive Learning Environments', 'Advance through a terminal-based Git dojo where each level teaches and verifies a specific operation (branching, merging, rebasing, cherry-picking, reverting, bisecting, submodules) before unlocking the next. After completing all levels, write the verified final commit SHA (LEVEL_FINAL) to /app/answer.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Progressive Learning Environments', 'Build a multi-level NetOps Dojo where each stage exposes a different service/protocol with sparse hints, progressing from DNS and HTTP to raw TCP/TLS and SSH. The agent must use terminal tools (dig, curl, nc, openssl, ssh/scp) to authenticate, parse responses, and collect tokens in order, writing them to /app/flags.txt to unlock subsequent stages.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Progressive Learning Environments', 'Create a terminal-based Bandit Lab where each level runs a simulated multi-armed bandit and the agent must implement a policy that beats a regret threshold to unlock the next level. Later stages introduce non-stationary payouts, delayed feedback, contextual features, and pull-budget constraints with automated evaluation after each run.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Education & Learning Tasks', 'Progressive Learning Environments', 'Progress through a ''Git Time-Travel Academy'': a series of levels that each present a corrupted or tangled repository requiring specific git operations (recovering lost commits, conflict resolution, bisecting a regression, history rewrite to purge secrets, submodules/sparse-checkout) to meet exact verifiable end-states. A terminal proctor script validates artifacts (commit DAG shape, tags, signed commits, message formats) and unlocks the next level upon success.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Debugging Under Constraints', 'Debug a terminal-based roguelike in C that intermittently crashes after a few moves due to a use-after-free in its pathfinding queue; locate the bug using only gdb with at most three breakpoints and fix it by changing no more than five lines in src/queue.c, then survive 300 turns without a crash. The harness limits you to five launches and validates the fix by running under AddressSanitizer and a randomized input script.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Debugging Under Constraints', 'Diagnose and fix a deadlock in a multithreaded C echo server that intermittently hangs under load, using only strace and gdb with a maximum of 12 commands total and editing at most two source lines. Verify the fix by running the concurrent client harness to completion and writing OK to /app/result.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Debugging Under Constraints', 'Given a buggy terminal-based Snake game (C/ncurses) that intermittently segfaults when the snake grows near the top-left wrap, locate and patch the defect using only grep, sed, make, and gdb with at most 10 debugger commands and 3 rebuilds. The fix must allow a provided deterministic input script to reach length 25 without crashing and the program should write the final score to /app/score.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Debugging Under Constraints', 'Perform a constrained git-bisect on a pre-seeded repository: using at most a small fixed number of shell commands and only batch-mode tools (git, make, ./test.sh), identify the first commit that breaks an integration test. Apply a one-line patch to fix the bug so all tests pass, then write the bad commit SHA and the fixed binary’s checksum to /app/answer.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Debugging Under Constraints', 'Repair a minimal in-terminal SQL REPL that returns incorrect results for nested WHERE conditions due to a faulty lexer and precedence handling. Debug using only the built-in ''explain'' and ''trace'' commands with at most five REPL sessions, and fix it by editing only tokenizer.py and eval.py until all transcript tests pass.', NULL, ARRAY['sql']),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Exploit & Reverse Engineering Games', 'Analyze a provided crackme.wasm that validates a passphrase; reverse or patch it using wasm2wat/wasm-objdump and run it (e.g., via wasmtime/node) to trigger the success path and print the hidden flag. Write the exact flag to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Exploit & Reverse Engineering Games', 'Bypass a dynamic license check in a stripped ELF by either reversing libcheck.so to derive a valid key or injecting an LD_PRELOAD shim that overrides the verifier; then run the program to unlock and print the secret. Save the exact secret to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Exploit & Reverse Engineering Games', 'Interact with a CLI vault that issues sequence-based one-time passcodes derived from a predictable PRNG. Infer or reverse engineer the PRNG from observed outputs and generate the correct admin code to unlock the root vault, then write the revealed secret to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Exploit & Reverse Engineering Games', 'Reverse a stripped 64-bit ELF that embeds a tiny stack-based VM and a hidden print_flag() routine; using gdb or radare2, recover the VM instruction set and craft an input/bytecode that pivots execution to the win path despite ASLR and NX. Execute the payload against the service to reveal the flag and write it to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Exploit & Reverse Engineering Games', 'Reverse a stripped ELF implementing a custom WASM-like bytecode VM that loads program.bin and rejects unknown opcodes; interactively recover the instruction set via objdump/gdb and black-box probing. Assemble a crafted bytecode payload that calls the hidden reveal routine to print the secret and write it to /app/results.txt.', NULL, NULL);
-- Compact batch 11/29: rows 501-550

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Live Coding Challenges', 'Create an interactive ''Regex Golf'' challenge where the agent writes a single PCRE regex per round under a character budget to match all include strings and none of the exclude strings, with the harness streaming randomized cases and instant pass/fail diffs. Finish all rounds and write the final regexes, one per line, to /app/answers.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Live Coding Challenges', 'Implement next_state(grid, steps) to simulate a custom Life-like cellular automaton whose birth/survival rules are loaded from /app/rules.txt (e.g., B36/S23) with toroidal wrap-around. An interactive test runner re-executes on save and reports the first failing case until all levels pass, after which the final solution is written to /app/solution.py.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Live Coding Challenges', 'Play an interactive ''jq dojo'' where each round provides JSON input and a target output; you must write a single jq filter that produces the exact output, with immediate diff feedback after each attempt. Pass all levels, including nested reductions, key-based joins across files, streaming inputs, and stable sorting, then persist your successful filters to /app/solutions.json.', NULL, NULL),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Live Coding Challenges', 'Solve a sequence of byte-budgeted I/O puzzles by writing the shortest possible shell/awk/sed or Python one-liners into a live judge that instantly validates against hidden tests and reports progress. Clear all stages under their size caps and output the final passphrase provided by the judge to /app/answer.txt.', NULL, ARRAY['python']),
('Interactive Challenges & Games', 'Interactive Programming & Debugging Challenges', 'Live Coding Challenges', 'Solve an interactive regex-golf ladder where a CLI judge streams sets of positive and negative examples and you submit a single POSIX ERE pattern under a byte-length budget that must match all positives and none of the negatives to advance. Clear 20 levels with cumulative pattern length capped and immediate pass/fail feedback after each submission.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Dynamic Difficulty Adjustment', 'Build a terminal-driven stealth microgame where the agent exchanges JSON messages with a referee, issuing one legal move per turn to reach an exit while avoiding dynamic guard line-of-sight. After each success the referee expands the grid, upgrades guard AI, injects decoys/noise, and the agent must clear three escalating tiers and write the final run transcript to /app/replay.json.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Dynamic Difficulty Adjustment', 'Build an agent that plays an adaptive Unix Pipe Gauntlet: a judge program presents level-by-level data-processing goals and allowed/forbidden tools; each success raises difficulty by tightening constraints (e.g., streaming-only, no temp files, limited forks) and larger inputs. The agent must clear at least 6 consecutive levels without invalid commands and then write the exact pipeline used for each level, in order, to /app/pipelines.txt.', NULL, ARRAY['data-processing']),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Dynamic Difficulty Adjustment', 'Build an agent to play a terminal-based adaptive Mastermind/Wordle hybrid where, after each successful round, the secret’s length, symbol set, and feedback strictness increase; the agent must reach and solve Tier-5 difficulty and export every round’s secret and full guess history to /app/solutions.json.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Dynamic Difficulty Adjustment', 'Create an agent that plays an adaptive ''Protocol Negotiation'' game where a local server alters its text-based handshake, auth, and payload schema each round based on prior success. The agent must probe, infer, and complete three escalating exchanges to obtain and write the final session token to /app/result.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Goal Discovery & Hidden Objective Games', 'Interact with a black-box CLI that accepts short sequences of shell-style actions and returns graded hints while adaptively reshaping its constraints based on your recent attempts. Infer the hidden rule set and produce a minimal sequence that triggers ''EXIT GRANTED'' within a fixed budget, then write both the inferred rule and the winning sequence to /app/solution.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Goal Discovery & Hidden Objective Games', 'Interact with a black-box finite-state machine REPL (/app/fsm_lab) that exposes only a few actions (reset, step A/B/C, probe) and returns minimal cues (opaque state hashes, warmer/colder signals) while its transition rules randomize each run. Infer the hidden acceptance condition and the action sequence to trigger it, then capture the acceptance token printed by the program and write it to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Goal Discovery & Hidden Objective Games', 'Interact with a daemon that watches /puzzle and provides adaptive hints in response to your file operations (create/rename/chmod/symlink/edit/touch). Infer and perform the hidden ritual—a precise sequence of operations—to trigger ''RITUAL COMPLETE'' and write the revealed token to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Goal Discovery & Hidden Objective Games', 'Interact with a shape-shifting oracle CLI whose feedback adapts to your recent commands, signaling only via exit codes, response latency, and log entropy whether you’re getting closer or farther. Deduce that the hidden objective is to achieve a ‘Harmony’ state by simultaneously satisfying three latent conditions (a process with specific args, a file with exact permissions/content, and an open port), then coax the oracle to reveal a token to write to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Goal Discovery & Hidden Objective Games', 'Interact with a terminal mood-daemon that returns only a scalar score in response to your filesystem and process actions, while adaptively changing its hidden preferences if you repeat patterns. Infer the evolving rule set (names, layout, permissions, and timing) to push the score past a threshold that emits a secret phrase, then write the phrase to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Multi-Stage Interactive Scenarios', 'Create an agent that plays an adaptive terminal tower-defense simulator: run waves to probe enemy types, infer resistances from combat logs, and iteratively design a rule-based layout/upgrade plan that counters evolving enemy compositions. Upon defeating all adaptive waves, output the final defense ruleset and a succinct battle report to designated files.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Multi-Stage Interactive Scenarios', 'Design a terminal-driven heist simulation where the agent first conducts reconnaissance to map a procedurally generated facility and infer rotating passcodes from noisy logs, then executes a timed infiltration. The environment adapts to missteps by reshuffling room topology and passcode rules; the agent must extract two target artifacts and POST them to an exfiltration endpoint while saving a full action transcript to /app/heist.log.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Multi-Stage Interactive Scenarios', 'Interact with a terminal-based adaptive market simulator that changes regimes (fees, tick-size, order throttles, volatility) in response to your PnL and risk. In phases, first probe and infer the current rules, then place exactly one legal order per tick to survive all rounds and finish above a target net worth, finally writing your ending equity and inferred parameters to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Multi-Stage Interactive Scenarios', 'Launch an adaptive Forensics Gauntlet CLI that presents a three-stage sequence of artifacts (e.g., pcap, stego-image, packed binary) whose exact types and hints change based on the agent’s prior actions and errors. Extract a token from each stage with appropriate terminal tooling, concatenate them into the final passphrase, and write it to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Meta-Challenges & Adaptive Tasks', 'Multi-Stage Interactive Scenarios', 'Play a terminal-based adaptive heist simulator in three phases: gather recon from noisy CLI sensors and dynamic map feeds, infer guard patterns and rotating lock ciphers, then edit system/cron/firewall configs to schedule a breach and exfil path. The environment reacts to missteps by mutating routes and codes, so the agent must replan and ultimately write the retrieved vault_token to /app/results.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Adversarial Game Environments', 'Build a terminal client for a Tron-style light-cycle duel on a toroidal grid that receives per-tick state over a socket and must emit exactly one legal move under a strict per-turn deadline. To pass, the agent must outlast or trap the AI opponent across a best-of-N series while maintaining a target non-loss rate and zero invalid moves.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Adversarial Game Environments', 'Create a terminal agent that plays an adversarial Battleship variant against a hidden AI where ships can legally drift one cell per turn after each round. With a limited number of probes and optional sonar sweeps, the agent must track probabilistic ship positions, anticipate evasive maneuvers, and achieve a target win rate over a best-of series.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Adversarial Game Environments', 'Create a terminal bot that plays Notakto (misère tic-tac-toe) on three simultaneous 3×3 boards against a provided adversarial engine via a simple text protocol. The bot must issue legal moves under a per-turn time limit, strategically closing boards to force a win across a best-of-five match and log the full transcript.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Adversarial Game Environments', 'Create an agent that plays a fog-of-war Battleship variant against a deceptive AI via a line-based terminal protocol. The AI may relocate damaged ships and emit noisy decoy feedback; the agent must track uncertainty and sink all ships within a fixed shot budget without illegal moves.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Adversarial Game Environments', 'Implement a terminal-based agent that plays Connect Four against a provided adversarial engine over stdin/stdout (newline-delimited JSON), submitting exactly one legal column index each turn whether moving first or second. The agent must avoid losses across a 50-game series and, after the series, write the comma-separated sequence of its chosen columns per game to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Collaborative Simulations', 'Coordinate three autonomous spelunking drones in a fog-of-war cavern using the provided TCP JSON protocol: each drone has limited sensors, battery, and comm range, and some passages require synchronized two-drone actions to unlock. Implement a controller that schedules moves, plans rendezvous to sync maps, avoids collisions, and writes a globally merged ASCII map to /app/cavern_map.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Collaborative Simulations', 'Coordinate three simulated warehouse robots via provided CLI tools and TCP sockets to pick, carry, and deliver items on a 2D grid while avoiding collisions and respecting capacity and battery limits. Plan a joint schedule including charging stops, issue synchronized move/pick/drop commands, and write the final delivery ledger to /app/fulfillment.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Collaborative Simulations', 'Create a CLI agent that cooperatively controls two rescue bots in a partially observable grid city: they must share map info over a limited channel, coordinate to open gates and carry stretchers requiring both, avoid spreading fires, and evacuate all victims. Output the joint action log and final evacuation summary to /app/evac.json.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Collaborative Simulations', 'Implement a terminal controller that coordinates two cooperative agents—Scout and Carrier—over JSON via UNIX pipes in a partially observable 2D mine. Map hazards, schedule pickups, and deliver a target ore quota to base within a strict tick limit without collisions or invalid commands, then write the mission summary to /app/results.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Collaborative Simulations', 'Implement an agent that joins a turn-based, socket-driven cooperative pipe network repair simulation alongside a simulated teammate controlling a disjoint set of tiles. Using a 16-byte-per-turn radio channel, coordinate synchronized rotations and valve toggles to connect all sources to sinks without leaks within 250 turns, then output the final ASCII grid to /app/network.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Negotiation or Dialogue Simulations', 'Implement an agent that conducts multi-attribute contract negotiations (price, delivery window, warranty) via a terminal CLI with three simulated vendors using an alternating-offers protocol, inferring each vendor’s hidden preferences from their counteroffers. The agent must secure at least two agreements under a global budget and write the finalized deals as JSON to /app/deals.json.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Negotiation or Dialogue Simulations', 'Implement an autonomous negotiator that interacts via a stdin/stdout JSON protocol with two simulated counterparts to settle a multi-issue contract (price, delivery, warranty) under a strict 10-round deadline. The agent must adapt to counteroffers, maintain feasibility and constraints, achieve a target utility score, and persist the agreed terms and full transcript to /app/contract.json.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Negotiation or Dialogue Simulations', 'Negotiate a cloud service contract with two simulated vendors over a JSON-lines CLI broker, adapting to counter-offers and surprise constraint changes (budget, latency SLO, and data residency) to reach a Pareto-feasible deal in ≤20 rounds. On success, write the agreed terms as valid JSON to /app/contract.json and the complete dialogue transcript to /app/transcript.log.', NULL, ARRAY['cloud']),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Negotiation or Dialogue Simulations', 'Negotiate a multi-item procurement contract with three simulated vendors over a CLI dialogue protocol, exchanging offers that vary unit price, lead time, and minimum-order constraints. Produce a single purchase order that fulfills the given BOM within budget and deadline while minimizing total cost.', NULL, NULL),
('Interactive Challenges & Games', 'Multi-Agent or Adversarial Simulations', 'Negotiation or Dialogue Simulations', 'Use the provided market_sim CLI to negotiate concurrently with two competing suppliers that bluff, issue time-limited counteroffers, and adapt pricing to your messages. Secure a single bundled contract meeting target quantity, budget, and delivery constraints, then write the agreed terms to /app/contract.json in the required schema.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Environment Navigation Challenges', 'Build an agent that explores a fog-of-war hex-grid rover simulation via CLI, where each move reveals neighbors, drains battery, and encounters dynamic hazards (e.g., shifting dunes or rockfalls). The agent must locate and collect two beacons, then reach the uplink before power depletion, outputting the discovered map and exact move sequence to /app/route.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Environment Navigation Challenges', 'Create an agent that interacts with a terminal simulator to explore a partially observed 2D grid featuring toroidal wrap-around, paired teleporters, and one-way wind tiles that push the agent upon entry. The agent must output an exact ASCII map labeling S, E, walls, portal IDs, and wind directions, and a minimal-length path from S to E that respects these mechanics to /app/solution.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Environment Navigation Challenges', 'Explore a terminal-based grid with hidden paired portals and one-way conveyor tiles under partial observability, inferring teleport linkages and movement dynamics as you go. Reconstruct the exact ASCII map, list portal pairings, and compute the shortest S→E route that respects conveyors and teleports.', NULL, ARRAY['observability']),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Environment Navigation Challenges', 'Interact with a time-evolving grid simulator where hazards drift each turn; using only local scans and actions (up/down/left/right/wait/ping), explore to map the environment, collect two artifacts, and return to S. Write the final ASCII map and the time-indexed action sequence to /app/solution.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Environment Navigation Challenges', 'Navigate a CLI-driven toroidal ocean grid where each move (N/S/E/W/WAIT) is followed by a hidden current vector that displaces the agent, with limited sonar pings revealing local distances to walls and the exit. Infer and map the per-cell current field and chart a route that reliably reaches the extraction gate within a turn limit, then output the final ASCII map and move sequence.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Interactive Physics or Cellular Simulations', 'Interact with a terminal-based 2D n-body gravity sandbox where a probe starts on an escape trajectory; issue one thrust command per tick under a fixed fuel budget to capture into and maintain a near-circular orbit around a specified body for 600 steps. On completion, write the measured semi-major axis, eccentricity, and mean motion to /app/orbit.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Interactive Physics or Cellular Simulations', 'Interact with a terminal-driven Conway’s Game of Life sandbox to synthesize a seed that evolves into a provided 40x40 target bitmap at generation 20. Using only add/remove and step/reset commands, find a seed with ≤200 live cells that yields the exact target and write the seed coordinates to /app/seed.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Interactive Physics or Cellular Simulations', 'Pilot a capsule in a simplified 2D orbital dynamics simulator by issuing discrete thruster burns via a CLI to match position and velocity with a target module before fuel runs out. On successful dock, write the timestamp of docking and remaining fuel to /app/docking_report.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Interactive Physics or Cellular Simulations', 'Pilot a spacecraft in a terminal-based 2D orbital simulator by issuing discrete thrust commands each tick to rendezvous and soft-dock with a moving satellite while conserving fuel. Parse telemetry, predict trajectories under gravity, avoid collision zones, and complete docking within time and distance/velocity tolerances.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Interactive Physics or Cellular Simulations', 'Probe an unknown 2D cellular automaton via a terminal interface that lets you seed patterns, step the simulation, and snapshot states under a fixed global step budget. From these experiments, infer the local update rule and implement a predictor that, given a fresh initial grid and K, writes the K-step future state to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Resource Management Simulations', 'Operate a simulated island microgrid via a terminal CLI—dispatch diesel gensets, curtail wind/solar, and charge/discharge a battery each tick—to meet demand and maintain a spinning-reserve margin without blackouts over a 24h scenario. Minimize total operating cost and emissions while surviving randomized weather dips and a forced-outage event, then write the final KPIs to /app/report.json.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Resource Management Simulations', 'Operate a terminal-based microgrid simulator by dispatching generators and scheduling battery charge/discharge each tick to meet demand under transmission constraints and variable renewable output. Survive 96 ticks without load shedding while minimizing fuel and carbon cost, then write final KPIs (cost, unserved energy, CO2) to /app/results.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Resource Management Simulations', 'Operate a terminal-based power grid where you must schedule generators with ramp/warm-up constraints, shift energy via batteries, and buy/sell on a volatile spot market as demand and weather vary each tick. Survive a multi-day horizon with zero blackouts while minimizing total operating cost under emissions and maintenance limits.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Resource Management Simulations', 'Operate a terminal-driven microgrid where each tick you dispatch generators, charge/discharge batteries, and optionally shed load amid stochastic demand and weather. Keep blackout events below a threshold and total operating cost under a budget for 500 ticks, then write the final cumulative cost and unmet-load percentage to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'Resource Management Simulations', 'Use a terminal microgrid-sim to manage solar, wind, battery, and diesel assets over 48 simulated hours, issuing per-tick dispatch commands to meet demand without blackouts. Minimize fuel use and battery wear amid stochastic weather and output a final KPI summary to /app/report.json.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'System or Network Simulations', 'Operate a simulated multi-AS BGP network via a provided CLI: inspect peering sessions, diagnose a route leak, and implement prefix/AS-path/local-pref policies to restore correct reachability and preferred paths. Validate by triggering a scripted link failure and write the converged RIB snapshot for each router to /app/ribs.json.', NULL, NULL);
-- Compact batch 12/29: rows 551-600

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'System or Network Simulations', 'Operate a simulated three-tank industrial control system via a terminal CLI, reading sensor states and issuing bounded pump/valve commands under latency and sensor noise. Keep all tank levels within target bands for 500 ticks despite disturbances, then write the stability checksum to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'System or Network Simulations', 'Operate a terminal-driven Raft cluster simulator where nodes suffer leader flapping from mis-tuned timeouts and intermittent network partitions. Diagnose logs and adjust per-node settings and links to maintain quorum and consistency across hundreds of simulated ticks.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'System or Network Simulations', 'Pilot a simulated Software-Defined Network: use the provided controller CLI to install and update OpenFlow rules across six virtual switches so tenant networks stay isolated, service A is load-balanced, and failover occurs within 2s during injected link failures. Keep dropped-allowed-traffic under 1% as reported by the monitor and write the final rule set and metrics to /app/results.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Simulation & Virtual Environments', 'System or Network Simulations', 'Using a provided Raft cluster simulator, bootstrap a 5‑node key-value store, write a test key, then induce controlled failures by stopping/restarting nodes to trigger leader elections while preserving linearizable reads. Verify consistency by reading the key from all nodes after recovery and write the observed leader IDs over time to /app/leader_log.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Board & Turn-Based Games', 'Build an agent that plays terminal Battleship against a provided opponent, using probabilistic heatmaps and constraint reasoning to select shots and update an internal belief grid. Run multiple seeded matches, sink all ships, and write the shot sequences and outcomes to /app/shots.json while meeting a shots-per-game efficiency target.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Board & Turn-Based Games', 'Build an autonomous Battleship solver that connects to the provided TCP referee, maintains a 10x10 belief grid, and fires one coordinate per turn using a probability-density hunt/target strategy. Sink all ships within the shot limit without repeating guesses, then output the final reconstructed board to /app/fleet.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Board & Turn-Based Games', 'Create a terminal Battleship agent that connects to a line-based server, places a legal fleet, tracks hits/misses/sunk feedback, and selects shots via a probability heatmap search. It must sink the opponent’s fleet within 60 turns on at least 8/10 random seeds and write the full shot trace to /app/trace.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Board & Turn-Based Games', 'Interact with a terminal-based Minesweeper referee that accepts reveal/flag commands on a hidden 16×16 board with 40 mines. Implement a solver that combines logical deductions and probabilistic tiebreaking to clear the board without detonations, then write the final ASCII board state to /app/board.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Board & Turn-Based Games', 'Play a terminal-based Battleship judge on a 10×10 grid by issuing coordinate probes, interpreting hit/miss/sunk feedback, and adaptively planning to sink the entire standard fleet within a strict shot budget. Upon victory, output the exact shot sequence in order and the reconstructed final board layout to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Interactive Optimization Problems', 'Build an interactive traveling-salesman optimizer that queries a provided HTTP API to reveal individual edge distances at a cost and iteratively proposes route improvements. Under a strict query budget and time limit, the agent must produce a tour beating a baseline length and write the final city order to /app/route.txt.', NULL, ARRAY['api']),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Interactive Optimization Problems', 'Design an agent for a query-limited Traveling Salesman problem: interact with a terminal oracle that reveals pairwise city distances on demand (each query consumes budget), then decide when to stop and output a complete tour. The agent must balance which distances to learn versus constructing a near-optimal route under strict query and time constraints.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Interactive Optimization Problems', 'Interact with a CLI contextual multi-armed bandit simulator: each round you read feature vectors, choose one of K actions, and observe a noisy reward with occasional non-stationary drift. Implement an online policy that maximizes cumulative reward over N rounds and writes the final score and learned parameters to /app/results.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Interactive Optimization Problems', 'Interact with a terminal-based noisy-knapsack simulator: over multiple rounds, optionally probe items (incurring small costs) to reveal noisy weight/value estimates, then choose a subset to pack under a fixed capacity. Design an agent that balances exploration and exploitation to maximize cumulative packed value and write the per-round selections and final total to /app/results.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Interactive Optimization Problems', 'Play a terminal-based nonstationary multi-armed bandit: each turn, choose one of N arms via stdin and receive a noisy reward, with reward distributions drifting over time. Over a fixed horizon, adaptively balance exploration and exploitation to minimize regret, then output total reward and estimated regret to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Probability & Randomized Challenges', 'Build a terminal agent to play a non-stationary multi-armed bandit exposed via CLI/HTTP, choosing among 10 arms with stochastic rewards and occasional distribution shifts. Across 5,000 rounds, the agent must adapt online and complete with sublinear regret while writing final metrics to /app/results.json.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Probability & Randomized Challenges', 'Implement a terminal agent that plays a K-armed bandit via /app/bandit, using only pull and status commands to learn unknown reward distributions under a fixed budget. The agent should employ an online algorithm (e.g., Thompson sampling/UCB) to maximize cumulative reward across randomized seeds and write a summary (seed, total reward, arm statistics) to /app/results.json.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Probability & Randomized Challenges', 'Implement an agent that plays a terminal-based K-armed Bernoulli bandit by reading JSON feedback each round and emitting exactly one arm index to pull, using a probabilistic policy (e.g., Thompson sampling) to balance exploration and exploitation. After a fixed number of pulls, the agent must achieve cumulative regret below a given threshold and write posterior mean estimates for each arm to /app/posteriors.json.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Probability & Randomized Challenges', 'Implement an agent that plays a terminal-hosted multi-armed bandit with unknown, stochastic rewards; within a fixed number of pulls, it must adaptively explore and exploit (e.g., via Thompson Sampling or UCB) to maximize total reward and then report the inferred best arm and its estimated success rate. The agent must handle randomized seeds, noisy observations, and persist state to resume if interrupted.', NULL, NULL),
('Interactive Challenges & Games', 'Strategy & Reasoning Games', 'Probability & Randomized Challenges', 'Play repeated rounds of a generalized Monty Hall game via a CLI where the host’s door-opening policy is unknown and possibly biased; after each reveal, decide whether to stay or switch to maximize win rate. Learn and update a probabilistic model of the host to adapt strategy, then write the estimated host parameters and final win rate to /app/results.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Adventure & Exploration Games', 'Explore a time-loop text adventure mansion whose room connections and NPC locations shift every 10 turns; learn the cycle, intercept three NPCs at their correct windows to collect temporal keys, and unlock the vault. Upon success, write the exact vault phrase to /app/answer.txt and the sequence of commands used to /app/route.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Adventure & Exploration Games', 'Interact with a terminal-based time-loop mystery adventure where the world resets at midnight; learn NPC schedules, gather clues, and chain actions across rooms to obtain three keys and open the lighthouse vault before the reset. Write the exact minimal command sequence that achieves the win to /app/solution.txt and the final vault phrase to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Adventure & Exploration Games', 'Launch a terminal adventure, The Teleporting Labyrinth, where rooms connect via rune-marked pads with non-obvious transitions; explore to learn glyph rules, collect three keystones in the correct order, and unlock the Central Gate. Upon success, capture the exact final inscription shown and write it to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Adventure & Exploration Games', 'Launch the provided Wumpus-style cave adventure and interact via text commands to explore rooms, infer hazards from sensory cues (e.g., breeze/stench), and locate the Wumpus. Kill the Wumpus with limited arrows and escape alive, then write the inferred cave graph and your exact action transcript to the specified output files.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Adventure & Exploration Games', 'Play a terminal-based time-loop mystery where the facility resets every 10 in-game minutes, altering NPC schedules and door states. Across multiple loops, navigate rooms, cache items in persistent lockers, infer patterns from logs, and assemble a 6-symbol code to trigger the final shutdown and escape.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Escape Room & Challenge Scenarios', 'Escape a locked research facility via a simulated command console: probe subsystems, parse scattered logs, and solve layered ciphers (base64, Vigenère, and a custom keypad code) to restore power, calibrate sensors, and enter the final passphrase. The agent must deduce the correct action order and submit the passphrase to unlock the exit and write the confirmation token to /app/escape.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Escape Room & Challenge Scenarios', 'Escape a submarine control room by navigating a filesystem-based map, extracting hidden clues from file metadata (timestamps, POSIX ACLs, and extended attributes) to derive access codes. Use the recovered codes to forge a CRC16-framed command over a provided pseudo-serial socket that unlocks the hatch, then capture the exact ''HATCH OPEN'' response to /app/escape.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Escape Room & Challenge Scenarios', 'Explore a haunted Git repository where branches, tags, and orphaned objects act as rooms, with clues hidden in commit messages, notes, reflogs, and blob deltas. Reconstruct a three-part key using low-level git plumbing and feed it to a provided door CLI to unlock and write the final exit token to /app/exit.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Escape Room & Challenge Scenarios', 'Navigate a git-based escape room where each room is a branch and doors open by resolving crafted merge conflicts that conceal ciphered codes, with clues hidden in commit messages, tags, notes, and diffs. Progress through the repository to reach the final release tag, decode the ultimate passphrase, and write it to /app/answer.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Escape Room & Challenge Scenarios', 'Navigate a terminal-driven escape room with a mock OS and devices (keypad, radio, safe) where clues are hidden across system logs, in-game tools, and a live numbers-station stream. Use shell pipelines to decode layered ciphers (hex -> XOR with a key inferred from artifacts -> base32) and assemble the final passphrase to unlock the exit.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Logic & Math Puzzles', 'Build a cryptarithm (alphametic) solver that reads an expression like SEND+MORE=MONEY from /app/puzzle.txt, finds a valid letter-to-digit assignment with no leading zeros, and verifies the arithmetic. Write the mapping and evaluated equation to /app/solution.json.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Logic & Math Puzzles', 'Implement an agent that interacts with a terminal Minesweeper server (open/flag commands over stdio), deducing safe cells strictly by logic until the board is cleared without guesses or invalid moves. Upon completion, exit gracefully and write the final grid with mines marked as ''M'' to /app/solution.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Logic & Math Puzzles', 'Interact with a terminal-based Nonogram (Picross) game, issuing commands to fill or mark cells using the provided row and column clues to solve a 15x15 puzzle without guessing. Upon completion, capture the final revealed pattern and write the exact ASCII art to /app/nonogram.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Logic & Math Puzzles', 'Interact with a terminal-based Nonogram (Picross) interface that provides row/column clues and accepts commands to mark cells filled/empty and validate lines. Solve the 20×15 puzzle to completion and write the exact final ASCII grid (e.g., ''#'' for filled, ''.'' for empty) to /app/solution.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Logic & Math Puzzles', 'Solve a deterministic terminal Minesweeper instance by interacting with a CLI that prints the board and accepts reveal/flag commands, using only logical inference to avoid detonations. On success, capture the exact victory token the game prints and write it to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Word & Pattern Games', 'Create an agent that plays a terminal Wordle-like CLI, submitting 5-letter guesses, parsing per-letter feedback (G/Y/-), and pruning a candidate list via regex and letter-count constraints to guarantee solving within six tries. Output the ordered guess list and the discovered secret word to /app/guesses.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Word & Pattern Games', 'Create an agent to play a multi-round Regex Golf terminal game: for each round, synthesize a single POSIX ERE that matches all ‘in’ words and none of the ‘out’ words within a length budget by iteratively querying a validator binary. Output the final winning patterns for all rounds to /app/answers.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Word & Pattern Games', 'Interact with a terminal-based Regex Golf engine that provides positive and negative word lists per level; craft a single PCRE regex per level that matches all positives and none of the negatives within a character budget, iterating based on feedback until the level passes. Output the final regex for each level, one per line with its level number, to /app/solutions.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Text-Based Games & Puzzles', 'Word & Pattern Games', 'Solve a terminal ''regex crossword'': fill an N×N grid with A–Z so each row and column matches its provided regular expression. Use the provided /app/regex_oracle to validate partial grids and write the final ASCII grid to /app/solution.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive REPL Mastery', 'Launch gdb on the provided ''vault'' ELF, interactively identify the password check routine, patch the branch condition in-memory to bypass it, run the program to reveal the secret, and write the token to /app/flag.txt. Do not modify the binary on disk; all changes must occur live in the debugger.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive REPL Mastery', 'Load the provided stripped ELF into gdb, set breakpoints to inspect the password-check routine (disassemble/step/examine memory) and recover the required passphrase or live-patch the check to reveal the computed token. Execute the program to its success state and write the exact success banner to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive REPL Mastery', 'Use the Python debugger (pdb) to load a protected script, set breakpoints to inspect the call stack and variables, and live-patch values/functions to bypass randomized checks. Continue execution to reveal the secret token and write it to /app/answer.txt.', NULL, ARRAY['python']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive REPL Mastery', 'Using only the sqlite3 interactive prompt, recover a partially corrupted database with .recover into a clean DB, attach any provided shards, and execute a supplied query to derive the final secret. Use .mode and .output to write the single-line result to /app/answer.txt after verifying PRAGMA integrity_check returns ok.', 'hard', ARRAY['database']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive REPL Mastery', 'Using the GNU Debugger (gdb) REPL, attach to and analyze the provided /app/lockbox binary, intercept its credential check (e.g., via breakpoints on strcmp) to recover the correct unlock token, then rerun the program to reveal the secret and write it to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive Tool Learning', 'Analyze an unknown ELF binary using radare2’s CLI (r2, pdf, afl, aec, izz, etc.) to locate the input check routine and recover the exact passphrase that reveals a secret when the program is run. Run the binary with the recovered input and write the secret output verbatim to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive Tool Learning', 'Use an Enigma-machine CLI emulator by consulting its --help/man to configure rotors, reflector, ring settings, plugboard, and starting positions from /app/settings.txt, then decrypt the ciphertext in /app/cipher.txt. Write the exact plaintext to /app/plain.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive Tool Learning', 'Use radare2 to recover a hidden passphrase from a deliberately obfuscated, stripped ELF in /app/bin using only r2’s analysis and debugging features. Consult r2’s help/man to identify the validation routine, step/trace to derive the correct input, then run the binary with that input and write the exact success banner to /app/flag.txt.', NULL, ARRAY['debugging']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Interactive Tool Learning', 'Using only tshark (learned via --help/man), analyze a provided PCAP to reconstruct an HTTP transfer, extract a base64-encoded secret from the payload, decode it, and write the exact flag to /app/flag.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Shell Puzzle Games', 'Aggregate mixed-format logs (CSV, JSON, Apache) scattered in nested compressed archives and directories, then correlate timestamps and locations using only shell pipelines (find/grep/awk/jq/sort) to invalidate alibis. Identify the single culprit and write their username to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Shell Puzzle Games', 'Excavate a secret phrase hidden across non-obvious Git artifacts—annotated tags, stashes, reflog-only commits, orphaned blobs, and packed-refs—using git plumbing (rev-list, cat-file, for-each-ref) combined with grep/sed/awk. Reassemble the numbered fragments in order and write the final phrase to /app/answer.txt without modifying the repository state.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Shell Puzzle Games', 'Recover a hidden release key by streaming through a labyrinth of nested archives and mixed-format shards, normalizing CSV/TSV/JSONL on-the-fly, key-joining records, and verifying order with checksums using a single shell pipeline. No intermediate files are allowed; write the final key to /app/results.txt.', 'hard', NULL);
-- Compact batch 13/29: rows 601-650

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Shell Puzzle Games', 'Starting at /app/maze/start, traverse a symlink-based filesystem maze where each encountered file contains JSON giving a regex for the next filename plus a token fragment and sequence number. Using only shell tools (find, sed/awk, jq, sort), detect and avoid cycles, collect and order fragments, then base64-decode and gunzip the concatenated string to plaintext and write it to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Shell Puzzle Games', 'Traverse a directory tree containing mixed-format event fragments (logs, JSON, CSV, and filename-encoded timestamps), some nested in unlabeled archives, and normalize their heterogeneous timestamps (RFC3339, epoch ms/nanos, DOY, base36) using shell tools. After deduping by event id and globally sorting, extract one character per event to reconstruct the hidden message and write it to /app/message.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Open lazygit in the provided Git repo and resolve a pending merge by choosing “theirs” for config/app.yml and accepting only the first two conflicting hunks from “ours” in src/core.py, then finalize the merge with the supplied message and exit. After leaving the TUI, write the resulting HEAD commit SHA and the total count of remaining conflict markers (<<< or >>>) in the repo to /app/answer.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Open the provided mixed CSV/JSONL dataset in the VisiData TUI, interactively filter to completed EMEA orders in Q3 2024, join users to orders, group by user, compute total spend, sort descending, and export a 2‑column TSV to /app/report.tsv. All data manipulation must be performed inside VisiData via keyboard-driven operations, then exit cleanly.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Use the tig ncurses Git interface to locate the first commit that introduced a target function signature and determine the first tag that contains it by navigating log, search, and blame views. Record the exact commit SHA and tag name to /app/results.txt as ''SHA TAG'' after exiting the TUI.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Using the lazygit TUI, resolve a prepared merge conflict by selectively staging hunks to restore a passing implementation, commit with the exact message ''merge-fix: OK'', and push to the provided local bare remote. After the test suite passes, write the resulting commit SHA to /app/result.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Using the visidata TUI, open /app/orders.csv and interactively create a pivot grouped by region and product_category for orders in 2023-Q4, calculating total_revenue and order_count, then sort by total_revenue descending and export the result to /app/summary.tsv. Also write the single top row of that pivot (tab-separated) to /app/top.txt.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Build a CLI that ingests a labeled multivariate time-series CSV, slices it into fixed windows, and generates augmented samples using jitter, scaling, time-warp, permutation, and magnitude-warp with deterministic seeds while preserving label alignment. Save the augmented dataset and a per-sample transform manifest, and produce a small DTW-based diversity report.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Build a CPU-only Python CLI that reads WAV files from /app/speech, impulse responses from /app/rir, and noise from /app/noise, and synthesizes an augmented set by convolving speech with a chosen RIR and mixing noise at randomized target SNRs, resampling as needed and peak-normalizing to -1 dBFS without clipping. Save 16-bit PCM WAVs under /app/aug deterministically given a --seed and write /app/aug/metadata.csv listing source paths, chosen RIR/noise, SNR, RT60 estimate of the RIR, output path, and random seed.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Create a CLI that augments multivariate time-series by injecting labeled anomalies—spikes, level shifts, trend changes, and seasonal pattern breaks—under user-controlled prevalence, duration, and SNR, producing /app/aug_series.csv, /app/aug_labels.csv, and a JSONL manifest of per-sample transforms. Include a deterministic seed and a ''validate'' mode that checks non-overlap, exact target prevalence by type, and summarizes augmentation statistics.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Create a Python CLI that ingests multivariate IoT sensor readings from /app/sensor.csv, trains a variational autoencoder on normal windows, then synthesizes a balanced dataset with labeled normal and anomalous sequences by sampling and latent-space perturbations plus jitter, scaling, and time-warp augmentations. Save train/val/test Parquet files (fixed-length windows) and a manifest JSONL capturing augmentation provenance to /app.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Create a Python CLI that performs class-balanced augmentation of a mixed-type tabular dataset by implementing SMOTE-NC from scratch, honoring per-column constraints (numeric bounds, integer-only, allowed categories) and a fixed RNG seed. Output the augmented CSV and a JSON report with pre/post class ratios and drift metrics (KS for numeric, chi-square for categorical), with tests ensuring constraints are met and no target leakage occurs.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a CLI pipeline that ingests a directory of messy CSVs (mixed encodings, delimiters, and locales) and reconciles their schemas, producing a single standardized Parquet dataset and a JSON data-quality report. Handle Unicode NFC and BOM normalization, locale-aware numeric/currency parsing, ISO-8601 UTC datetime normalization, group-wise imputation, MAD-based outlier capping, MinHash de-duplication on a text column, and optional tokenization to fixed-vocabulary IDs.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a CLI pipeline that ingests multiple vendor CSVs of product listings with mixed encodings, locale-specific number/currency formats, and heterogeneous timestamp columns, then standardizes them. Normalize text to UTF-8 NFC, parse dates to UTC ISO-8601, harmonize decimal/thousand separators, convert currencies via a provided FX table, impute missing numeric fields per-category with robust medians, remove outliers via MAD, deduplicate by SKU/title similarity, and write a cleaned Parquet plus a data-quality summary JSON.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a chunked, streaming CLI that consolidates heterogeneous CSV/TSV/JSON sources with mixed encodings into a single cleaned Parquet: auto-detect encoding/delimiters, coerce to a target schema from schema.json, normalize booleans/numerics, parse and UTC-normalize datetimes, impute missing values, cap outliers via MAD, and deduplicate by composite keys. Emit data_quality.json with per-column nulls/outlier counts/type-coercions and bad-record samples, ensuring determinism (seeded), memory safety on multi-GB files, and resilience to BOMs, malformed rows, and embedded newlines.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a streaming CLI that merges multiple CSV shards with mixed encodings and a nested JSON column, infers a unified schema, standardizes datetimes to UTC ISO-8601, normalizes column names, and outputs cleaned Parquet deterministically. Impute missing values (group-wise median for numerics, mode for categoricals, forward-fill within user sessions), cap outliers per group via IQR, tokenize a free-text column while preserving emojis, and generate a data-quality report JSON with validation metrics.', 'hard', NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Create a CLI that consolidates multi-sensor time-series CSVs with differing time zones and sampling rates: normalize timestamps to UTC, auto-detect and correct per-sensor clock drift via alignment to a shared ''sync'' channel, resample to 1 Hz, impute gaps ≤5s, and flag/remove outliers using robust MAD. Save the cleaned dataset to Parquet and emit a JSON audit of imputations, drift corrections, and outlier statistics to support reproducibility.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Build a CLI that performs a purged, group-aware time-series split with an adjustable embargo to prevent leakage, using entity_id as the group and preserving chronological order. It must output train/val/test CSVs that maintain marginal and pairwise label co-occurrence within ±2% via iterative multi-label stratification and emit a JSON report of balance and leakage diagnostics.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a CLI that performs leakage-safe train/val/test splitting for a multi-label text dataset by first clustering near-duplicate documents with MinHash LSH and then applying iterative stratification at the cluster level to match label marginals while assigning whole clusters to a single split. Write split assignments to /app/splits.csv and a duplicate report to /app/dup_report.json with cluster members and pairwise Jaccard estimates.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a Purged Group Time Series Split that creates train/validation/test indices for timestamped events grouped by an entity_id, enforcing a configurable gap and embargo to prevent temporal leakage while approximately preserving class balance via quantile-based stratification. Provide a CLI that reads /app/events.csv and writes split index files plus a JSON report with leakage checks and per-split label prevalence.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a deterministic multi-label, group-aware stratified k-fold splitter with an optional time-based embargo to prevent leakage across folds. Provide a CLI that reads id/group_id/timestamp/labels columns and outputs per-fold splits plus a JSON report of label balance, zero group overlap, and temporal ordering, scaling to millions of rows via streaming.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a spatially stratified data splitter that groups samples by 5-character geohash and assigns whole geohashes to train/val/test to prevent spatial leakage. Maintain class balance for the species label as closely as possible, enforce reproducibility via salted hashing, and write split indices and a summary (per-split label histograms and KL divergence vs overall) to disk.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Build a CLI that performs leakage-safe K-fold target encoding with additive smoothing for multiple categorical columns (including pairwise interactions), producing out-of-fold encodings for train, encodings for test via a holdout, and a serialized mapping for reuse. It must stream-process large CSVs (10M+ rows), be deterministic, and robustly handle missing and unseen categories.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Build a CPU-only Python CLI that loads directed graphs from /app/graphs/*.edgelist and computes reproducible 128-dimensional node2vec embeddings via fixed-seed random walks and skip-gram training, writing /app/embeddings/{graph}.npy and a matching {graph}_nodes.txt listing nodes in row order. Ensure deterministic node ordering across runs, gracefully handle disconnected nodes, and fit any walk/model parameters using only the provided train split to prevent leakage.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Build a Python CLI that performs leakage-safe out-of-fold target encoding and frequency encoding for multiple high-cardinality categorical columns on a time-stamped dataset using chronological folds with smoothing and optional noise regularization. Output transformed train/test Parquet files and a reusable encoder artifact to ensure consistent application on future data.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Create a Python CLI that reads an edge list from /app/graph.csv and computes per-node features including node2vec embeddings and structural metrics (degree, betweenness, clustering coefficient, PageRank), then standardizes and writes a single feature table with a node_id column to /app/node_features.parquet. Ensure deterministic results with fixed RNG seeds and correct handling of isolated nodes and disconnected components.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Implement a leakage-safe categorical target encoder that computes out-of-fold, James-Stein–smoothed means for high-cardinality features (and selected feature crosses), augments with count/frequency stats, and uses a hashed fallback for unseen categories at inference. Provide a CLI that fits on train.csv and transforms both train/test into /app/processed/*.csv in chunked mode with deterministic seeds and memory bounds.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a Mixture-of-Experts Transformer feed-forward block in PyTorch with top-2 gating, capacity-based token dispatch (including overflow handling), and an auxiliary load-balancing loss. Integrate it into a small decoder-only model and provide tests for routing correctness, gradient flow, and equivalence to a dense FFN when num_experts=1.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a PyTorch LoRA adapter system that injects low‑rank adapters into a vanilla Transformer’s attention (Q, K, V, and output) and MLP projections, with per-module rank/alpha settings, freezing of base weights, and precise merge/unmerge of adapters into the backbone. Provide a CLI that trains only the adapters on a tiny language-modeling corpus and verifies identical logits pre/post-merge within a tight tolerance while confirming no non-adapter parameters changed.', NULL, ARRAY['pytorch', 'modeling']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a PyTorch Mixture-of-Experts Transformer block with top-2 routing, per-expert capacity with token dropping, and an auxiliary load-balancing loss, supporting variable sequence lengths and attention masks. Ensure numerically stable, deterministic routing and gradient flow across dtypes on CPU, and include a small CLI to verify balanced expert utilization on synthetic data.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a PyTorch decoder-only Transformer block with grouped-query attention (GQA), rotary position embeddings (RoPE), pre-norm RMSNorm, and an efficient KV cache that supports incremental autoregressive generation with a stable fallback when scaled_dot_product_attention is unavailable. Ensure correct causal masking, mixed-precision stability, variable-length batch handling, and robust cache updates during token appends and sequence reordering.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a minimal decoder-only Transformer in PyTorch that supports rotary positional embeddings (RoPE) and grouped-query attention (GQA) from first principles (no Transformers/timm), with a flag to fall back to standard multi-head attention. Train CPU-only on TinyShakespeare for a short run, then generate a deterministic 200-character sample and save model.pt and a config.json to /app.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a PyTorch MonotonicLinear layer that enforces non-negative weights on selected features and a pairwise monotonicity loss that penalizes order violations, then train an MLP on a tabular regression dataset with specified monotone inputs. Report test MAE and the fraction of monotonicity violations versus an unconstrained baseline.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a PyTorch Soft-DTW loss with numerically stable forward/backward that supports batched variable-length sequences via padding masks, per-sample Sakoe–Chiba windows, and mixed precision. Provide a TemporalAlignment layer that uses the expected alignment matrix to differentiably warp sequences for downstream models.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a batched Soft-DTW loss with padding masks and an optional Sakoe–Chiba band for variable-length time-series, plus a custom temporal attention pooling layer that uses differentiable DTW alignment scores as attention weights. The components must run on CPU/GPU, support mixed precision, and be memory-efficient enough to handle sequences up to length 10k.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a custom TokenDropAttention layer for a tiny Transformer that learns to stochastically drop uninformative tokens via Gumbel-Softmax gates, paired with a sparsity-consistent KL loss enforcing a user-specified target keep-rate with temperature annealing and mixed-precision safety. Train on a synthetic sequence classification dataset and verify via tests that the achieved keep-rate is within tolerance and that accuracy drop versus a vanilla attention baseline stays below a threshold.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a differentiable sorting-based Top-k pooling layer (NeuralSort/SoftSort) and a listwise NDCG surrogate loss in PyTorch to train a ranking model on variable-length item lists with padding masks. The layer and loss must be numerically stable in fp16/fp32, efficient on long lists, and include gradient checks and input validation.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Build a CPU-only PyTorch character-level LSTM language model on TinyShakespeare that stays stable on long sequences via dropout, LayerNorm on recurrent outputs, global-norm gradient clipping, gradient accumulation, and a warmup+cosine learning-rate schedule. Log per-step gradient norms and effective LR, and save a final model plus JSON metrics proving clipping occurred and the scheduler advanced.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Implement a PyTorch character-level LSTM trained on Tiny Shakespeare with two runs: an unstable baseline and a stabilized variant using dropout, gradient clipping, label-smoothed cross-entropy, and a warmup+cosine learning rate schedule. Log gradient norms and NaN/Inf checks, and verify the stabilized run achieves a target perplexity while the baseline shows exploding gradients or divergence.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Retrofit an unstable DCGAN training script to add spectral normalization to the discriminator, an R1 gradient penalty, adaptive global-norm gradient clipping, EMA of generator weights, and a cosine learning-rate schedule with warmup; train on a provided CIFAR-10 subset and write FID and Inception Score to /app/metrics.json, ensuring no NaNs and FID below a set threshold.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Train a compact Transformer for character-level language modeling on long sequences, implementing stabilization features: gradient clipping (by norm and value), warmup+cosine LR scheduling, dropout, label smoothing, and EMA/SWA of weights. Provide a CLI to ablate each component, log per-step gradient norms and NaN/Inf events to /app/stability_log.json, and save the best-perplexity checkpoint.', NULL, ARRAY['modeling']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Write a PyTorch training script for CIFAR-10 that stabilizes an intentionally aggressive setup (large LR, small batch) using mixed precision with dynamic loss scaling, adaptive gradient clipping (AGC), linear warmup plus cosine decay scheduling, and EMA of weights with switchable BatchNorm/GroupNorm and dropout. Tests verify no NaNs/Inf, bounded gradient norms, correct LR schedule behavior, and a validation curve that is smoother and at least as good as a baseline without these techniques.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Build a CLI tool that instruments any PyTorch model with forward/backward hooks to record per-layer activation, weight, and gradient statistics during training, automatically flagging saturation, dead ReLUs, and vanishing/exploding gradients while rendering concise PNG plots and a JSON report. The script should run a short CPU-only training of a small CNN on synthetic data and output /app/reports/{grad_flow.png, activations.png, weights_hist.png, anomaly_report.json}.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Build a Python CLI that loads a PyTorch ResNet-18 and, via forward/backward hooks on a small image batch, saves per-layer activation/gradient histograms (PNGs), a gradient-flow plot, and a dead-ReLU heatmap. Write a debug_report.json summarizing per-layer gradient norms and flagging any vanishing/exploding gradients.', NULL, ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Create a PyTorch script that hooks into a ResNet-18 training on CIFAR-10 to record per-layer activation histograms, activation sparsity, and gradient norms across one epoch, automatically flagging layers with saturation or vanishing/exploding gradients. Produce a self-contained HTML report of these visuals and save Grad-CAM overlays for the top-3 misclassified images.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Create a Python CLI that registers forward/backward hooks on a given PyTorch model to capture per-layer activation histograms, gradient norms, and (if present) attention head entropy/rollout, then writes PNG plots and a single HTML report to /app/vis. The tool should flag vanishing/exploding gradients, NaN/Inf activations, and saturation rates per layer, exiting non-zero if any issue is detected.', NULL, ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Implement a Python CLI that loads a provided PyTorch CNN and sample dataset, registers forward/backward hooks to compute per-layer activation stats (mean/std/sparsity) and gradient norms on a minibatch, and produces a gradient-flow plot plus activation histograms. Save a JSON report flagging dead ReLUs and vanishing/exploding gradients, and write all artifacts (JSON and PNGs) to /app/report so tests can verify detection of an intentionally broken layer.', NULL, ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Build an expert-parallel Mixture-of-Experts layer that shards experts across ranks and routes tokens via top-2 gating with a capacity factor, using all_to_all to dispatch to per-rank experts and recombining to the original order. Provide an unsharded reference and ensure numerical parity and gradient correctness across world_size 1/2/4, with support for dropless/drop policies, variable sequence lengths, and mixed precision.', 'hard', ARRAY['parallel']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement 1D tensor-parallel sharding for GPT-style attention and MLP in PyTorch with custom ColumnParallelLinear/RowParallelLinear layers using torch.distributed (Gloo), including forward/backward collectives (all-gather/reduce-scatter) and model-parallel RNG. Provide a test script that trains a tiny Transformer on synthetic data across 2–4 ranks and verifies numerical parity with an unsharded reference, correct gradient synchronization, and that memory per rank drops below a set threshold.', 'hard', ARRAY['parallel', 'pytorch', 'distributed']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement CPU-only tensor parallelism for a single Transformer block using torch.distributed (Gloo), sharding QKV and MLP weights across two ranks with column/row-parallel matmuls and the necessary all-reduce/concat steps. Provide a script that verifies forward/backward parity with an unsharded baseline to 1e-5 and reports per-rank parameter and optimizer memory to demonstrate sharding benefits.', 'hard', ARRAY['distributed', 'parallel']);
-- Compact batch 14/29: rows 651-700

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement a minimal ZeRO Stage-2 optimizer in PyTorch that shards Adam states and gradients across ranks, using reduce-scatter for gradient partitioning and on-demand all-gather of parameters for forward passes. Train a small model on synthetic data and verify numerical parity with an unsharded baseline, correct save/load of sharded checkpoints, and peak memory reduction for world_size 1, 2, and 4.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement vocabulary-parallel sharding for a tiny Transformer language model in PyTorch by partitioning the token embedding and tied LM head across ranks, producing only local logits and using an all-reduce to compute the global softmax cross-entropy loss. Support world sizes 1, 2, and 4, include sharded checkpoint save/load with resharding across different world sizes, and verify numerical parity with a non-sharded reference model.', 'hard', ARRAY['parallel', 'pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Build a CPU-only PyTorch FSDP training pipeline launched via torchrun (world_size=2–4) that auto-wraps a small Transformer, uses DistributedSampler and gradient accumulation to maintain a fixed global batch, and logs rank-local plus aggregated metrics. Save a sharded checkpoint (model and optimizer) and implement resume-on-different-world_size, verifying determinism by matching validation loss and a consolidated state_dict against a single-process baseline.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Build an elastic PyTorch DDP training launcher that tolerates worker failures and dynamic membership (2–4 ranks), re-forming the process group via a rendezvous backend and resuming from rank-0 sharded checkpoints even when world size changes. The test will start/kill workers mid-epoch; the job must continue training to a target accuracy and emit a single consolidated checkpoint at the end.', 'hard', ARRAY['pytorch', 'backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Launch a CPU-only PyTorch Elastic DDP job (torchrun) with 4 ranks across two localhost ''nodes'' that trains a tiny transformer on synthetic data using the gloo backend, writes sharded per-rank checkpoints, then restarts from the latest checkpoint after simulating a rank failure. Consolidate the shards into a single checkpoint and verify equivalence to a single-process baseline by hashing model parameters.', 'hard', ARRAY['pytorch', 'backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Launch a PyTorch FSDP training job for a small Transformer on synthetic sequence data across 4 local ranks with torchrun, enabling mixed precision and activation checkpointing, and write sharded checkpoints plus a script to consolidate them into a single state_dict.pt. Provide a baseline DDP run and verify that resuming from the sharded checkpoint reproduces validation loss within tolerance and that peak memory per rank is at least 30% lower than DDP.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Launch an elastic Horovod CPU training run (Gloo backend) on synthetic data that tolerates worker restarts and dynamic scaling, preserving optimizer state and the learning-rate schedule across changes. Begin with 2 workers, add a third after 50 steps, then remove one, and produce a metrics.json with per-step throughput and effective global batch size plus a final saved model.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Convert a Hugging Face BERT-base classifier to ONNX, build an INT8 static-quantized variant from calibration data, and serve both in an NVIDIA Triton model repository (ONNX Runtime backend, CPU) with dynamic batching and multiple instances. Implement a concurrent client to benchmark p50/p95 latency and throughput at batch sizes 1/8/32 and assert the INT8 model delivers ≥1.4x throughput with ≤1% accuracy delta, saving a metrics.json.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Deploy a BERT-base encoder in NVIDIA Triton with two TensorRT backends (FP16 and INT8 calibrated from /app/calib), enable dynamic batching and concurrent instance groups, and write a Python client that drives variable-length requests over HTTP to collect p50/p95 latency and throughput. Validate that the INT8 engine achieves at least 1.8x throughput over a PyTorch eager baseline on the same hardware and emit a JSON report to /app/metrics.json.', 'hard', ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Export a BERT-base QA model to ONNX, build both FP16 and INT8 TensorRT engines (with calibration), and deploy them in NVIDIA Triton with dynamic batching and two concurrent instances. Benchmark with perf_analyzer over batch sizes {1, 8, 32} and write a JSON summary of latency/throughput and INT8 vs FP16 speedups to /app/perf_report.json.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Export a HuggingFace BERT-base encoder to ONNX, build FP16 and INT8 TensorRT engines with entropy calibration, and deploy both behind NVIDIA Triton Inference Server with a model repository configured for dynamic batching and multiple concurrent instances. Provide a CLI load generator to send large-batch requests, verify output parity vs. PyTorch within tolerance, and report throughput to demonstrate INT8 speedup over FP16.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Stand up a CPU-only Triton Inference Server hosting an INT8-quantized DistilBERT ONNX model via the ONNX Runtime backend, configured with dynamic batching and multiple instances, then load-test batch sizes {1, 8, 32, 128}. Validate outputs against an FP32 baseline within tolerance and write a JSON report summarizing per-batch throughput plus p50/p99 latency scraped from Triton’s /metrics endpoint.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Build a CPU-only QLoRA fine-tuning pipeline for a small Hugging Face causal LM that can switch between LoRA and prompt-tuning via a CLI flag, training on /app/data/train.jsonl with sequence packing and evaluating perplexity on /app/data/valid.jsonl. Save adapter weights to /app/adapter and a merged full model to /app/merged, and write the final validation perplexity to /app/perplexity.txt.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Fine-tune a small Hugging Face causal LM using LoRA to produce two domain-specific adapters from tiny corpora, then implement on-the-fly weighted adapter merging and a CLI to evaluate perplexity across domains. Verify that each adapter beats the base model on its domain and that online merged logits match an offline-merged checkpoint within a small tolerance.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Fine-tune a small causal LM (e.g., GPT-2) with PEFT to produce two separate LoRA adapters on two distinct domains (e.g., prose and code) in a CPU-only setting. Implement inference-time weighted composition of the adapters to blend or route behaviors, then save individual/composed adapters and a merged variant, and report a metrics JSON comparing cross-entropy/perplexity across domains and blends.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Implement a soft prompt-tuning pipeline for a decoder-only LLM (e.g., distilgpt2) that learns K virtual tokens while freezing all base weights. Provide scripts to train on data/, save/load the prompt embeddings, verify perplexity improvement on a held-out split, and generate completions for prompts.txt into outputs.jsonl with deterministic seeds.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Implement soft prompt tuning for a small causal LM (e.g., distilgpt2) to learn k virtual tokens for reversible date format conversion on a tiny synthetic dataset, keeping the base model frozen. Save the learned prompt vectors separately and provide an inference script that attaches them to the base model to generate outputs for test inputs, writing predictions to /app/date_convert.json.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Build a Python CLI that runs an offline Weights & Biases sweep of 8 trials for a scikit-learn imbalanced binary classifier, performing 5-fold CV per trial and logging per-fold metrics, aggregated results, confusion matrices, and the best model as artifacts under a single grouped sweep. Enable run resumption, and export a self-contained wandb-export.tar.gz with all offline run data to /app/wandb_export plus a best_trial.json summarizing chosen hyperparameters and metrics.', NULL, ARRAY['python', 'logging']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Configure and execute a Weights & Biases hyperparameter sweep in offline mode that trains a scikit-learn model with StratifiedKFold, logging hyperparameters, per-fold AUC/accuracy, ROC curves, and saving the fitted estimator as a W&B artifact for each run. After the sweep completes, programmatically identify the best run by mean AUC and write its run ID, config, and artifact file path to /app/best_run.json.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Create a Python CLI that runs 5-fold cross-validation with hyperparameter search, using MLflow to record a parent run and nested child runs per trial and fold with parameters, metrics, and artifacts (ROC/PR plots, confusion matrices, and serialized models). The CLI must support resuming by skipping completed child runs, produce an aggregated leaderboard.csv, and register the best model with signature and input example in the Model Registry.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Deploy an MLflow tracking server with a SQLite backend and a MinIO S3 artifact store behind an Nginx reverse proxy on localhost, then build a training pipeline that uses nested runs and autologging for both scikit-learn and PyTorch, logs SHAP artifacts, a model signature, and an input example, and registers two models with stage transitions. Add a script that queries MLflow for the best run by a validation metric, downloads its artifacts to run batch inference, logs predictions as a new artifact, and prints the chosen run_id.', NULL, ARRAY['backend', 'pytorch']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Implement a resumable training pipeline that uses MLflow to start a run, log hyperparameters and per-epoch metrics, save checkpoints, then intentionally crashes mid-training. On re-execution it must detect state and resume the exact same run_id (start_run with run_id), continue logging without duplicating epochs, record environment snapshots (pip freeze and git commit) and a learning-curve artifact, and write the run_id to /app/run_id.txt.', NULL, ARRAY['logging', 'git']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Build a local, file-backed model registry with a SQLite metadata store and content-addressable artifact storage, including a CLI to register versions, assign stage aliases (staging/production), and promote or rollback based on evaluation metrics. Provide an inference loader that consumes a lockfile to fetch a pinned version and verifies immutability and lineage.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Create a Python CLI that publishes trained models as OCI artifacts to a local Docker registry (registry:5000), storing model weights and a metadata.json layer with code/data hashes, metrics, and dependency manifest. Support semver tags and aliases (staging/production), integrity verification by content digest, version comparison by a chosen metric, and atomic promote/rollback of the production alias.', NULL, ARRAY['python', 'docker']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Create a schema-aware local model registry that assigns semantic versions (MAJOR/MINOR/PATCH) when registering scikit-learn models by diffing input schema, hyperparameters, and data hashes. Provide a CLI to register, list, and promote versions with stage transitions and a ''champion'' alias, then verify by loading the Production model to score a held-out set.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Start a local MinIO S3 server on localhost:9000 and configure a DVC remote (s3://model-registry) to serve as a centralized model registry for versioned model artifacts and metrics. Train two scikit-learn models, push both versions with semantic tags (e.g., v0.1.0, v0.2.0), implement a promote.py CLI to label Staging/Production in a bucket-hosted registry.json, and demonstrate a rollback to the prior Production version.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Build a Prefect 2.x flow and deployment that orchestrates an end-to-end ML pipeline with ETag-aware data ingestion, feature engineering, model training/evaluation, and conditional promotion; configure retries, result persistence/caching, and a cron schedule, ensuring re-runs skip unchanged tasks. Provide a CLI to register the deployment, start a worker, trigger a run, and emit versioned artifacts plus a run_report.json summarizing cache hits, timings, and the promotion decision.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Build an Apache Airflow DAG that watches /app/data/incoming with a FileSensor, validates new CSVs using Great Expectations, then transforms, trains, and evaluates a scikit-learn model on accepted data, writing artifacts and metrics under /app/outputs/{{ ds }}. The DAG must use the TaskFlow API with XCom to pass artifact paths, support backfilling for the past 7 days, and run on a daily 02:00 schedule.', NULL, ARRAY['api']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Create a Prefect 2 flow and deployment that ingests new CSVs, computes feature drift against a stored baseline, and conditionally runs preprocessing, training, evaluation, and model registration only when drift exceeds a threshold. Enable local caching and retries, add a cron schedule, and persist run artifacts and the chosen model under /app.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Create an Apache Airflow DAG that monitors /app/current.csv, computes data drift against /app/reference.csv with Evidently, and branches to either retrain/evaluate a scikit-learn model or skip retraining when drift is below a threshold. The DAG must be scheduled daily with retries and SLAs, use XCom to pass metrics, persist artifacts to /app/, and implement a file-content hash to cache and skip redundant training.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Create an Apache Airflow DAG that uses dynamic task mapping to run k-fold cross-validation (download/validate data, per-fold training, metric logging, aggregation) and conditionally register the model only if the averaged score exceeds a threshold. Package it for a local Dockerized Airflow setup, schedule it to run daily, ensure idempotent runs, and persist all artifacts/metrics under /app/artifacts.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a Dockerized ML pipeline that achieves bit-for-bit reproducibility by using a conda-lock lockfile, fixing locale/timezone, seeding all RNGs, and pinning BLAS thread counts, while prefetching wheels and dataset artifacts for fully offline execution. Train a scikit-learn model and write both metrics and a SHA256 of the serialized model; rerunning the container (including offline) must produce identical outputs.', NULL, ARRAY['container']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a Dockerized, conda-lock–pinned environment that runs a fully deterministic scikit-learn training pipeline on /app/data.csv, fixing seeds and BLAS threads to yield identical artifacts across runs. The CLI must produce model.pkl, metrics.json, and a reproducibility_report.json capturing package versions, BLAS backend, thread config, and SHA256 checksums of code/lockfile/data, with a make target that verifies bit-for-bit reproducibility.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a Dockerized, fully reproducible ML pipeline using DVC stages (data prep → feature extraction → training) with a conda-lock generated environment and pinned pip extras. Verify that running dvc repro in two clean clones yields bit-for-bit identical model and metrics artifacts and emit a manifest with exact package versions and data checksums.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a fully offline, deterministic ML pipeline using DVC stages (data -> features -> train -> eval) and a Dockerized micromamba environment created from a conda-lock lockfile, training a scikit-learn LogisticRegression on synthetic data with fixed seeds. Provide scripts/targets to run dvc repro and verify reproducibility by asserting identical SHA256 checksums of model and metrics artifacts across two consecutive runs.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Create a Nix flake that defines a hermetic Python 3.11 environment with pinned NumPy, pandas, and scikit-learn, plus a CLI that trains a logistic regression on /app/data.csv with fixed seeds. Add a Makefile target that builds the flake and runs the training twice in fresh pure shells, asserting identical SHA256 hashes for model.pkl and metrics.json to confirm bit-for-bit reproducibility.', NULL, ARRAY['python', 'pandas']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'Build a CPU-only benchmarking CLI that compares FP32, FP16, and INT8 (quantized) variants of a given image classifier on a provided dataset, reporting accuracy, Brier score, and p50/p95 per-sample latency. Output a leaderboard CSV and select the best configuration by maximizing accuracy under a 95th-percentile latency budget, saving the chosen tag to /app/selection.txt.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'Create a reproducible CLI benchmarking harness that trains and evaluates at least three scikit-learn classifiers on every CSV dataset in /app/data using nested stratified k-fold CV, computing accuracy, ROC-AUC, F1, log-loss, Brier score, and calibration error with 95% bootstrap confidence intervals. Aggregate results across datasets with paired Wilcoxon tests and effect sizes to rank models, also measuring CPU inference latency on fixed batch sizes and exporting a single results.json with per-dataset metrics, CIs, significance, and an accuracy–latency Pareto summary.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'Create a reproducible benchmarking harness that trains five scikit-learn classifiers across four built-in datasets with repeated stratified k-fold, logging ROC-AUC, log loss, Brier score, per-sample latency, and model size to a results file. Run a Friedman test with Nemenyi post-hoc to rank methods, emit a critical-difference diagram, and pick a champion model that satisfies latency and size thresholds.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'On an imbalanced binary dataset in /app/data.csv, train two baselines (Logistic Regression and Gradient Boosting) and calibrate each with temperature scaling and isotonic regression using a validation split. Benchmark pre/post-calibration AUROC, AUPRC, accuracy, Brier score, NLL, and ECE with bootstrap 95% CIs, then output a metrics CSV and a JSON naming the best-calibrated variant that keeps accuracy within 1% of the uncalibrated best.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Build a Python CLI that runs repeated StratifiedGroupKFold cross-validation to produce out-of-fold predictions for two classifiers, with options for class-weighting to handle imbalance. Compute ROC-AUC/PR-AUC means and bootstrap CIs, apply the Nadeau–Bengio corrected resampled t-test and McNemar’s test to assess performance differences, and write a summary report to /app/results.json.', NULL, ARRAY['python', 'performance']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Create a Python CLI that loads /app/adult.csv and runs nested stratified 5x2 cross-validation comparing tuned logistic regression and tuned random forest on F1 and PR-AUC, then applies Dietterich''s 5x2cv paired t-test and bootstrap 95% CIs for metric differences, writing /app/fold_predictions.csv and /app/eval_report.json with p-values, effect sizes, and CIs. Use fixed seeds for reproducibility, handle class imbalance, and hard-fail if variance across folds is zero.', 'hard', ARRAY['python']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Create a Python CLI that performs nested, stratified GroupKFold cross-validation (outer k=5, inner k=3) to tune a logistic regression on an imbalanced dataset with strict group-leakage prevention, aggregating out-of-fold macro-F1 and BCa bootstrap 95% confidence intervals. Implement the Nadeau–Bengio corrected resampled t-test to compare the tuned model against a majority-class baseline and report Holm–Bonferroni-adjusted p-values.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Implement a CLI that compares two classifiers on /app/data.csv using nested stratified 5x2 cross-validation, reporting ROC-AUC and F1 per fold. Assess significance with Dietterich’s 5x2cv paired t-test and DeLong’s AUC test, and output a reproducible JSON containing fold scores, BCa bootstrap 95% CIs, and overall p-values.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Implement a CLI that runs Dietterich’s 5x2 cross-validation paired t-test to compare LogisticRegression and RandomForest on the scikit-learn breast cancer dataset with stratification and fixed seeds. Aggregate out-of-fold predictions to compute ROC AUC and Average Precision with bootstrap 95% CIs, and write per-fold metrics, the t statistic, p-value, and a significance verdict to results.json.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Build a CLI that ingests a CSV of binary classifier outputs (y_true, y_score, group) and generates reliability diagrams with bootstrap CIs, ROC/PR curves, and score histograms, saving plots to /app/eval. It must choose a decision threshold that minimizes expected cost from user-specified FP/FN costs, then emit per-group confusion matrices and a summary JSON reporting ECE, MCE, Brier, AUROC/AUPRC, and the worst-affected groups.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Build a CLI that ingests multiclass predictions (logits or probabilities) and ground-truth labels, computes calibration metrics (ECE/MCE, class-conditional ECE, Brier, NLL) with bootstrap confidence intervals, fits temperature scaling on a validation split, and recomputes metrics post-calibration. Output adaptive-binning reliability diagrams, confidence histograms, and per-class normalized confusion matrices highlighting top confusable pairs, and save a concise markdown report linking all figures.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Build a CLI that ingests multiclass softmax predictions and ground-truth labels from CSVs in /app/preds/{train,val,test} (optionally including a ''group'' column) and generates per-group confusion matrices and reliability diagrams. Fit temperature scaling on the validation split, then save before/after ECE and Brier scores, interactive HTML plots, and a JSON metrics summary to /app/report.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Create a Python CLI that evaluates multivariate time-series forecasts against ground truth, computing per-horizon sMAPE, MASE, and prediction-interval coverage, and generates horizon-wise error heatmaps plus overlay plots of predictions vs. actuals for the worst 10 series by MASE. Write a metrics.json and PNG plots to /app/output while streaming to handle >1e6 points with numerically stable aggregations.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Implement a CLI tool that ingests a CSV of predictions (y_true, y_pred, per-class probabilities) plus optional metadata columns and performs comprehensive error analysis: confusion matrix, per-class PR curves, reliability diagram/ECE, and per-slice metrics for each metadata field. Identify the top-3 statistically significant failure slices via bootstrap, then export PNG plots and a JSON report listing highest-confidence false positives/negatives and an optimal threshold per class.', NULL, NULL);
-- Compact batch 15/29: rows 701-750

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Build a CLI evaluator for named entity recognition that parses BIO/BILOU tag sequences, extracts spans, and computes strict and partial-overlap F1, per-entity-type metrics, and micro/macro aggregates. Handle mismatched sequence lengths and invalid tag transitions with robust decoding, output a metrics.json plus a span_errors.csv highlighting boundary and type confusions.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Build a CLI that ingests CSVs of true labels and model logits/probabilities for in-distribution and OOD samples, fits a single temperature on a validation split, and computes accuracy, macro-F1, ROC-AUC, PR-AUC, NLL, Brier score, ECE/MCE (fixed and adaptive bins), and OOD AUROC/AUPR/FPR@95 using MSP and energy scores. Output a metrics.json and per-class.csv plus reliability diagrams and score histograms with strict input validation and numerically stable computations.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Create a CLI that ingests a CSV of multiclass prediction probabilities and ground-truth labels, computes macro/micro F1, top-1/top-5 accuracy, per-class precision/recall, confusion matrix, and calibration metrics (ECE with adaptive binning, MCE, Brier), and writes a JSON summary plus per-class CSV. Include 1,000-sample bootstrap confidence intervals for scalar metrics and save a reliability diagram and normalized confusion matrix as PNGs.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Create a CLI that ingests a Parquet file containing multilabel ground-truth indicators and model score columns, learns per-label thresholds on a validation split to maximize macro-F1, then evaluates the test split with those thresholds. Output a JSON report with per-label precision/recall/F1, micro/macro averages, LRAP, coverage error, Jaccard index, and the chosen thresholds, and write per-label 2x2 confusion matrices to CSV.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Ingest /app/logits.csv (Nx5 logits) and /app/labels.csv (N class ids), fit temperature scaling to calibrate probabilities, and compute top-1/top-5 accuracy, NLL, ECE (15 bins), Brier score, and macro-F1 before and after calibration with 95% bootstrap confidence intervals. Write a JSON summary to /app/report.json and save a before/after reliability diagram to /app/reliability.png.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Build a CPU-only FastAPI inference service that loads a scikit-learn model from /app/model.pkl, performs dynamic batching by aggregating requests for up to 50 ms before a single model call, and hot-reloads the model when the file changes without dropping in-flight requests. Provide a batch_infer.py CLI that reads /app/input.csv, runs identical pre/post-processing for batched predictions, and writes results to /app/preds.csv including a model_version column.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Build a dual-mode inference app around an ONNX image classifier using onnxruntime: a CLI that runs batched CPU inference over all images in /app/images and writes predictions.csv, and a lightweight HTTP server exposing /predict for single-image requests with lazy, thread-safe model loading and unified preprocessing. Implement optional dynamic batching on the server (short timeout window) and enable onnxruntime CPU optimizations for consistent, reproducible outputs.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Export a pretrained torchvision MobileNetV2 to ONNX, then implement an ONNX Runtime-powered FastAPI endpoint for online top-5 ImageNet predictions and a batch script that scores all images in a directory. Include optional dynamic quantization and write JSONL predictions plus latency/throughput metrics for the batch run.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Export a pretrained torchvision ResNet-18 to ONNX and implement a CPU-only FastAPI service backed by ONNX Runtime that supports single-image and dynamic micro-batched inference (≤50 ms window). Provide a CLI for offline batch predictions over a folder, saving top-5 classes per image and recording throughput and p95 latency to /app/metrics.json for both online and batch modes.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Build a CPU-only FastAPI inference server for a torchvision ResNet-18 and implement a background queue that performs dynamic micro-batching (coalesce requests for up to 16 images or 10 ms) with a single forward pass using a TorchScript-compiled model and tuned num_threads. Provide a CLI load generator to compare baseline (no batching) versus micro-batched serving and write p50/p95 latency and requests/sec to /app/results.json.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Build a CPU-only FastAPI service that serves a DistilBERT sentiment classifier via ONNX Runtime, then add three optimizations: static INT8 quantization with calibration, adaptive micro-batching (max batch 16, 10ms timeout), and an LRU cache of tokenized inputs keyed by content hash. Provide a benchmark script that drives concurrent requests and outputs a JSON report comparing p50/p95 latency and throughput before vs after, with a required ≥1.5× throughput and ≥25% p95 latency improvement to pass.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Build an async Python inference server for a small Hugging Face Transformer that adds micro-batching (time-windowed), dynamic int8 quantization, and a tokenizer cache. Provide a replay benchmark that outputs baseline vs optimized p50/p95 latency and throughput to a JSON report.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Containerize a FastAPI inference service for a small Transformer text classifier that implements a background request queue with dynamic micro-batching and an LRU tokenizer cache, and provide an ONNX Runtime int8-quantized variant. Use a load generator to measure QPS and p50/p95 latencies for fp32, int8, and ''int8+batching+cache'' modes, writing a comparison summary to /app/bench.json.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Create a CPU-only FastAPI inference server for a pretrained DistilBERT classifier that implements time-windowed dynamic batching, int8 dynamic quantization of Linear layers, and a content-hash response cache; include a short warm-up and pad inputs to multiples of 8 tokens. Supply a load generator that compares baseline vs optimized builds and outputs p50/p95 latency and throughput, with tests requiring at least 1.5x throughput improvement without violating a p95 latency SLA.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Export a PyTorch sequence model to both TorchScript (scripted) and ONNX with dynamic axes and opset 17, saving weights in safetensors with strictly pickle-free serialization. Provide a CLI that loads the exported artifacts on CPU, runs onnxruntime and TorchScript inference on the same inputs, and asserts numerical parity within a tight tolerance.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Export a scikit-learn Pipeline that includes a custom categorical encoder and a RandomForest model to ONNX by implementing a custom converter and shape calculator, then verify output parity against the original pipeline with onnxruntime on a mixed-type dataset. Save the portable ONNX and a small parity report summarizing numerical differences and opset/ir metadata.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Export the ''sentence-transformers/all-MiniLM-L6-v2'' PyTorch encoder to ONNX with dynamic batch and sequence axes and also produce an int8-quantized ONNX using onnxruntime. Provide a CLI that tokenizes sentences from /app/input.txt, runs PyTorch vs ONNX (fp32 and int8), checks cosine-similarity parity (1e-3/1e-2), and writes a JSON report with model sizes and average latency.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Implement a CLI that builds a variable-length batched BiLSTM tagger in PyTorch using PackedSequence, exports it to TorchScript and ONNX (opset 17) with dynamic batch and sequence length, and provides an ONNX-compatible unpacking path. Validate numerical parity across eager PyTorch, TorchScript, and ONNX Runtime on randomized inputs, saving all artifacts and a sample input under /app/export.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Train a scikit-learn mixed-type Pipeline (ColumnTransformer with numeric StandardScaler and categorical OneHotEncoder feeding LogisticRegression), then export it to ONNX (opset ≥17) with a dynamic batch axis, explicit input dtypes, and embedded class-label metadata. Validate with onnxruntime that class probabilities and predictions match scikit-learn within a tight tolerance across a holdout CSV, and save both the ONNX artifact and a JSON parity/latency report.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Build a FastAPI microservice that serves a scikit-learn model from /app/models/current.pkl with atomic, zero-downtime hot-reloads on file change via a background watcher and readers–writer lock, exposing /predict (batch), /healthz, /readyz, and /promote endpoints. Include a script that swaps in a new model and demonstrates uninterrupted concurrent requests while logging requests and responses to SQLite.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Deploy a FastAPI microservice that serves a small ONNX sentiment classifier via ONNX Runtime with async micro-batching (max batch size and wait time), concurrency-safe tokenization, and pydantic request validation. Provide Dockerfile and startup scripts, expose /predict, /healthz, and /metrics (Prometheus) endpoints, and implement a zero-downtime hot-swap endpoint that atomically loads and switches to a new model file.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Implement a Python gRPC inference server that loads a CPU-only ONNX Runtime model and performs dynamic micro-batching (bounded by batch size and a 50 ms queue window), exposing the gRPC health checking service and a Prometheus /metrics endpoint. Provide a CLI load generator to issue concurrent requests and write a JSON report comparing latency/QPS in batched vs unbatched modes to /app/bench.json.', NULL, ARRAY['python', 'grpc']),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Package a CPU-only ResNet18 into a TorchServe .mar with a custom handler that performs torchvision preprocessing and returns top-5 class probabilities as JSON. Launch TorchServe on 0.0.0.0:8080, register the model, support both single and batched image requests at /predictions/resnet18 with proper 400 errors for invalid inputs, and expose /ping and Prometheus /metrics for health and monitoring.', NULL, ARRAY['monitoring']),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Package a pre-trained PyTorch ResNet18 into a TorchServe .mar with a custom handler that accepts base64-encoded images, applies preprocessing, and returns top-3 labels with probabilities. Launch TorchServe with dynamic batching (e.g., max_batch_size=8), register the model via the management API, issue batched requests, and write a latency/throughput summary to /app/serve_metrics.json.', NULL, ARRAY['pytorch', 'api']),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Add LoRA adapters to a pretrained T5-small and fine-tune on a local summarization dataset in /app/data (train.jsonl, val.jsonl) while freezing all base weights, then export both an adapter-only checkpoint and a merged full model. Verify base weights are bitwise-identical pre-merge, that the merged model matches adapter outputs on a fixed prompt, and that ROUGE-L on val improves over the frozen baseline.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Fine-tune T5-small with parameter-efficient prefix tuning to translate English task descriptions into Bash one-liners on a provided dataset; evaluate exact match and BLEU on a held-out split, and save both the prefix adapter and a merged model plus predictions.json to /app.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Fine-tune a pretrained sentence-transformer (e.g., sentence-transformers/all-MiniLM-L6-v2) on provided positive/negative sentence pairs with a contrastive cosine-similarity loss to adapt it for semantic search. Provide a CPU-only CLI that trains, saves the fine-tuned encoder, and reports MRR@10 and Recall@10 on a held-out query set using exact cosine similarity over corpus embeddings.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Implement parameter-efficient fine-tuning by inserting lightweight adapter modules into a pretrained DistilBERT for domain text classification, training only adapters with discriminative layer-wise learning rates and a slanted triangular schedule. Provide a CLI to train/evaluate, export adapter-only weights, verify the frozen base model hash is unchanged, and support hot-swapping different adapter checkpoints at inference.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Build a CLI that performs multi-objective hyperparameter optimization with Optuna for a PyTorch CNN on Fashion-MNIST, maximizing validation accuracy while minimizing wall-clock time via ASHA pruning and a SQLite-backed study that can be resumed. After the search, pick the Pareto-optimal trial under a 30s time budget, deterministically retrain on the full training set, and export best_config.json, study.db, metrics.json, and best_model.pt.', NULL, ARRAY['optimization', 'pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Build a reproducible Optuna-powered multi-fidelity search (ASHA pruner) that tunes learning rate, weight decay, batch size, and transformer depth for a small text classifier on a provided CSV, persisting the study to SQLite. After the search, retrain the best trial and export the final model along with a Pareto front that balances validation accuracy against measured inference latency.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Build an Optuna multi-objective HPO pipeline that trains a tabular classifier (XGBoost if installed, otherwise scikit-learn GradientBoosting) with stratified K-fold CV to jointly minimize validation log loss and 95th-percentile per-sample prediction latency. Implement pruning, class-imbalance handling, and a dynamic search space conditioned on the chosen estimator, then export the selected Pareto-optimal trial’s hyperparameters, a serialized model, and a small report of the Pareto front.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Create a reproducible, multi-objective Optuna study that tunes a CPU-only PyTorch tabular classifier (layers, width, dropout, learning rate, weight decay, batch size) to simultaneously maximize ROC-AUC and minimize measured inference latency using ASHA pruning within a fixed time budget. Persist the study to a local SQLite DB, export the Pareto-optimal trials to /app/pareto.json, and save the fastest model meeting a target AUC threshold to /app/best_model.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Run a multi-objective Optuna study to tune a CPU-only LightGBM classifier on the UCI Adult income dataset, maximizing ROC-AUC while minimizing p95 inference latency over 10k predictions. Persist the study, write the Pareto front with hyperparameters to JSON, and export the fastest Pareto-optimal model artifact to /app.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Build a PyTorch DDP training harness that runs a small CNN on synthetic data in both single-process and 4-rank torchrun modes, auto-selecting CPU/GPU and gloo/NCCL while pinning one device per rank and using gradient accumulation with optional AMP to keep the effective batch size constant. Log images/sec per rank, global throughput, speedup vs single-process, and (if GPUs exist) per-GPU memory/utilization via nvidia-smi, saving a final JSON/CSV report.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Build a PyTorch training launcher that runs a small CNN under DistributedDataParallel across all available GPUs (falling back to multi-process CPU via gloo), automatically selecting the largest per-device microbatch via OOM-aware binary search and using gradient accumulation to hit a target global batch size. Record per-rank and averaged throughput, communication overlap vs non-overlap timings, and memory usage to a CSV, and support resuming checkpoints when world_size changes.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Implement a CPU-only pipeline-parallel trainer that splits a small Transformer into 2–4 stages across torch.distributed processes with a GPipe-style microbatch schedule, auto-falling back to single-process when world_size=1. Run via torchrun and output a JSON report comparing baseline vs pipeline throughput, per-stage latency, and memory usage.', NULL, ARRAY['parallel', 'distributed']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Implement a PyTorch DistributedDataParallel training launcher that runs on CPU (gloo) but uses CUDA if available, employs ZeroRedundancyOptimizer to shard optimizer state, and supports gradient accumulation with optional mixed precision. It must log per-rank and aggregate throughput and memory usage, save a single rank-0 checkpoint, and verify synchronized weights across ranks after each epoch.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Implement a PyTorch training launcher that uses torchrun to start DistributedDataParallel across all detected GPUs (falling back to multi-process CPU with Gloo), pins each rank to a device, and enables AMP on CUDA. Log per-rank throughput (images/sec) and memory stats to a shared JSONL and aggregate a final /app/throughput.json confirming that multi-GPU total throughput scales over a single process baseline.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Build a semi-supervised pipeline that uses Label Spreading to pseudo-label unlabeled samples, then trains a calibrated LogisticRegression on the union of labeled and high-confidence pseudo-labeled data and benchmarks against a labeled-only baseline. Provide a CLI for labeled fraction, confidence threshold, and seed, and write accuracy/F1/ROC-AUC plus class balance summaries to /app/metrics.json.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Implement a from-scratch Gaussian Mixture Model with EM that supports full/diagonal covariances, K-means++ initialization, and automatic component selection via BIC, then train on a large CSV to output cluster assignments and model parameters. Ensure strong numerical stability (log-sum-exp, covariance regularization) and a streaming-compatible E-step for memory-constrained environments.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Implement an EM-based Gaussian Mixture Model trainer (NumPy only) with diagonal covariances that selects the optimal number of components via BIC and enforces monotonic log-likelihood across iterations. Provide a CLI that reads a CSV, runs multiple random restarts per K, and saves the chosen K, parameters, responsibilities, and cluster labels to /app/out.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Learn a supervised metric using scikit-learn’s Neighborhood Components Analysis on labeled data, then perform k-means clustering in the learned embedding on the full dataset. Save cluster assignments and evaluate clustering quality with adjusted mutual information and silhouette scores to a metrics file.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Train a sparse autoencoder in PyTorch on a numeric tabular dataset with early stopping and L1 activation regularization, saving the encoder to /app/encoder.pt. Freeze the encoder to generate embeddings and train a scikit-learn logistic regression on labels, reporting stratified 5-fold ROC-AUC and writing metrics and artifact paths to /app/results.json.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a PyTorch training loop for a stateful RNN that performs truncated backpropagation through time on a streaming text dataset, correctly carrying/detaching hidden states across chunks with global-norm gradient clipping and LR warmup+cosine decay. Add fully resumable mid-epoch checkpoints that capture optimizer/scheduler/RNG/file offsets/per-stream hidden states, plus early stopping on validation perplexity with top-k best checkpoints.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a PyTorch training loop for a tiny character-level language model on a given corpus with truncated BPTT, gradient accumulation, global-norm gradient clipping, and early stopping on validation loss. Support rotating checkpoints (keep N), exact resume of optimizer/scheduler and RNG state after interruption, and deterministic results when a seed is provided.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a PyTorch training loop that supports mixed-precision (AMP) with GradScaler, gradient accumulation, global-norm clipping, cosine LR scheduling, early stopping on validation loss, and checkpointing that rotates top-2 best models plus the latest. The loop must be fully resumable (optimizer/scheduler/AMP scaler/RNG states) after an external SIGTERM or KeyboardInterrupt, skip NaN/Inf batches safely, and produce deterministic metrics given a seed on a tiny synthetic dataset.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a custom PyTorch training loop for SimCLR-style contrastive learning on a small image dataset with mixed precision, gradient accumulation, per-step gradient clipping, cosine warmup+restarts, temperature annealing, EMA weights, and robust checkpointing (best/last with resume). Validate via k-NN on frozen embeddings each epoch and support early stopping on top-1 accuracy.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a pure-PyTorch training loop for a CNN on a synthetic image dataset with gradient accumulation, automatic mixed precision (CPU/GPU fallback), global-norm gradient clipping, cosine-annealing learning-rate scheduling, and early stopping on a validation metric. Add robust checkpointing that saves and restores model/optimizer/scheduler states, best metric, and dataloader progress to resume mid-epoch or after SIGINT/SIGTERM, producing both latest and best artifacts.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Build a CLI that reads a JSON-specified stochastic GridWorld (walls, terminal states, slip probability) and trains a tabular Q-learning agent with epsilon-greedy exploration and learning-rate decay to learn an optimal policy. Support variable map sizes and multiple maps in a folder, ensure deterministic seeding, evaluate the greedy policy to meet a success-rate threshold, and save both the Q-table and a human-readable policy artifact.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Build a PettingZoo AEC multi-agent predator–prey gridworld with optional message-passing actions and invalid-move masking, register it with Gymnasium, and configure RLlib to train decentralized PPO policies with GAE, vectorized rollout workers, and resume-from-checkpoint support. Evaluate on held-out procedurally generated maps (fixed seed) and require a ≥0.70 predator win rate over 100 episodes while logging training metrics to TensorBoard.', NULL, ARRAY['logging']);
-- Compact batch 16/29: rows 751-800

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Create a custom Gymnasium environment backed by a SimPy discrete-event simulation of a two-station production line with stochastic arrivals and a controllable dispatch/batching policy. Train a Stable-Baselines3 PPO agent on CPU to minimize average job latency, logging learning curves and saving a checkpoint and evaluation report that demonstrates at least a 20% latency reduction versus a fixed-rate heuristic baseline.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Create and register a custom Gymnasium environment ''ThermalControl-v0'' whose dynamics (heat capacity, loss rate, actuator limits, and disturbance sequence) are loaded from /app/dynamics.yaml, and train a Soft Actor-Critic agent with stable-baselines3 (automatic entropy tuning) to keep temperature within a target band via domain-randomized episodes. Save the trained policy to /app/checkpoints and a 100-episode seeded evaluation log (time, state, action, reward) to /app/eval.csv for automated verification.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Install Ray RLlib with PettingZoo and SuperSuit, configure the MPE simple_spread_v3 multi-agent environment, and train a shared-parameter PPO policy using vectorized parallel environments. Export the trained policy to TorchScript and run a seeded evaluation that writes per-agent rewards and coverage metrics to /app/mpe_eval.json.', NULL, ARRAY['parallel']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement a PSRO self-play pipeline for the Rock-Paper-Scissors-Lizard-Spock normal-form game, iteratively training best-response policies via no-regret updates against the evolving meta-strategy. After convergence, compute the mixed Nash distribution and exploitability, and write them to /app/solution.json, passing tests by staying below a specified exploitability threshold.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement a self-play Counterfactual Regret Minimization trainer for Kuhn Poker with a minimal game engine including chance nodes, information-set caching, and average-strategy export. The script should train to a low-exploitability policy verified by a best-response evaluator, be reproducible via seeding, and finish on CPU within tight time limits.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement a self-play training pipeline for Connect Four using OpenSpiel where a PyTorch policy–value network is optimized via AlphaZero-style MCTS. After training, evaluate against a depth-limited minimax baseline over 100 games and save win/draw/loss and Elo metrics to /app/results.json.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement from-scratch CFR+ self-play for Kuhn Poker with a CLI to train for N iterations and compute exploitability via an exact best response. Save the average strategy to /app/kuhn_policy.json and ensure the final policy’s exploitability is ≤0.05 chips.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Train a near-Nash strategy for Kuhn Poker via self-play Counterfactual Regret Minimization (CFR/CFR+) using OpenSpiel (pyspiel), then serialize the average policy and compute exploitability. Produce a policy artifact and a metrics report demonstrating exploitability below a specified threshold.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Augment Gymnasium’s LunarLander-v2 with a configurable multi-objective reward balancing landing accuracy, fuel efficiency, and leg-contact stability, then sweep at least 32 coefficient sets where each trains a lightweight PPO agent for a fixed budget. Compute and save the Pareto frontier and knee-point selection with CSV/plots of episodic return, crash rate, and fuel use to verify the chosen reward balances competing objectives.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Design and evaluate multi-objective rewards for Gymnasium’s Taxi-v3 by augmenting the sparse reward with penalties for illegal pickup/dropoff and a discomfort cost proportional to action changes, implementing both scalarized (lambda-weighted) and Lagrangian-constrained formulations. Train a tabular Q-learning agent across a sweep of coefficients and report the Pareto frontier between episode length and discomfort with a baseline comparison to the original sparse reward.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Implement a Gymnasium-compatible ''Windy Keys-and-Doors'' gridworld and design a potential-based reward shaping function that balances goal completion, energy use, and safety while preserving the optimal policy. Provide a training/evaluation script that compares shaped vs. sparse rewards (e.g., with tabular Q-learning), demonstrating faster learning and fewer collisions, verifying equal optimal returns, and flagging reward-hacking behaviors.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Implement a custom Gymnasium GridWorld with a sparse goal reward, time penalty, and stochastic hazard tiles, then compare two rewards: potential-based shaping via Manhattan-distance potential and a naive per-step progress bonus. Train PPO under both and report success rate, hazard contacts, and path optimality gap to highlight shaping invariance vs reward hacking.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'In a 2D gridworld with a key, a locked door, and a fragile vase, implement and compare three rewards: sparse goal-only, potential-based shaping with step penalty, and a side-effect avoidance term that penalizes action impact relative to an inaction baseline using object-position L1 distance. Train a fixed PPO agent under each reward, detect reward hacking like key pick/drop loops, and produce a summary of goal success, vase collisions, loop frequency, and sample efficiency.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a disk-backed prioritized replay buffer daemon that ingests CartPole-v1 transitions from multiple local producer scripts over TCP, computes n-step returns across episode boundaries, and persists state for crash-safe restart. Provide a client to sample mini-batches with PER (alpha/beta) and importance weights, with a test harness that verifies sampling distribution, deterministic checksums for sampled batches, and replay continuity after restart.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a goal-conditioned 2D grid environment and generate large-scale rollouts to an on-disk replay buffer supporting Hindsight Experience Replay, n-step returns, and prioritized sampling with importance weights. Train a lightweight DQN for several thousand updates from this buffer and output success-rate curves and sampling diagnostics derived from the stored trajectories.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a memory-mapped, Zarr-backed replay buffer for Gymnasium Dict/Box spaces with prioritized experience replay, n-step returns, and sequence sampling for RNN policies (burn-in, overlap, zero-padding). Provide a CLI to run vectorized rollouts to populate the store, then deterministically sample batches under a fixed seed and output a JSON report verifying priority distributions, importance-sampling weights, and crash-safe resume behavior.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a multiprocessing rollout + learner system that generates goal-conditioned episodes and stores them in a segment-tree prioritized replay buffer with n-step returns and on-the-fly Hindsight Experience Replay relabeling. The buffer must support batched append/sample/update with importance-sampling weights, be mmap-backed to handle 10M+ transitions, and a training script should reach a target success rate while validating sampling distribution and bias correction.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a prioritized experience replay buffer using a segment tree that supports n-step returns, sequence sampling for recurrent policies, and per-sample importance weights. Provide a CLI to collect rollouts from parallel Gymnasium environments, persist the buffer to disk in chunked files, then reload to verify sampling probabilities, TD-error updates, and n-step target consistency.', NULL, ARRAY['parallel']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Create a CLI pipeline that attacks a pretrained image classifier with EOT-PGD over random crops, rotations, and Gaussian noise, then reports robust accuracy and worst-case loss. Implement a defense via randomized smoothing with calibrated sigma to compute per-sample certified radii and write a CSV summarizing clean/robust accuracy and certificates for a test subset.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Implement a CPU-only Python CLI that loads a provided CIFAR-10 model, wraps it with a non-differentiable defense (bit-depth reduction plus randomized resizing), and evaluates robust accuracy under FGSM, PGD, and BPDA+EOT-PGD. Save per-attack metrics to /app/robust_metrics.json and 32 adversarial samples per attack to /app/adv/{attack}/, demonstrating that BPDA+EOT meaningfully reduces reported robustness compared to naive PGD.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Train a small CNN on MNIST (CPU) and implement FGSM and PGD-Linf attacks. Add a feature-squeezing detector (bit-depth reduction + median smoothing) and an adversarially trained variant, then report clean/robust accuracy at eps=0.3 and ROC-AUC for detection, saving metrics and checkpoints.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Train a small CNN on MNIST, implement FGSM and multi-step PGD (with Expectation-over-Transformation for stochastic defenses) to craft adversarial examples at multiple L∞ epsilons, and report clean/robust accuracies with saved adversarial image grids. Add PGD adversarial training and a randomized smoothing inference defense, then re-evaluate and output a JSON metrics report plus a file of estimated certified radii for 100 test samples.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Train a small CNN on a CIFAR-10 subset, wrap it with an L2 randomized smoothing certifier (configurable sigma), and compute per-example certified radii with 95% confidence for 200 test images, writing certificates.csv and a summary JSON. Implement an EOT-PGD attack that accounts for the smoothing noise and report robust vs attacked accuracies across L2 epsilons, verifying certified accuracy lower-bounds are not violated.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Build a CLI pipeline that trains a baseline binary classifier on /app/data.csv, computes demographic parity, equal opportunity, and disparate impact for sex and race, then applies Calibrated Equalized Odds post-processing to the model’s probabilities to reduce violations. Write a before/after report with bootstrap confidence intervals to /app/fairness_report.json and save learned group-specific thresholds/weights to /app/thresholds.json.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Build a CLI that loads /app/loans.csv containing features, a binary label y, and a sensitive attribute S, trains a logistic regression baseline, reports fairness metrics per group (demographic parity difference, equalized odds difference, disparate impact), then applies Kamiran–Calders reweighing and a per-group threshold optimizer to enforce equal opportunity within 2% while minimizing accuracy loss. Save before/after metrics and per-group confusion matrices to /app/fairness_report.json, persist the learned thresholds, and repeat the evaluation under a covariate-shifted test split using importance weighting to verify fairness generalization.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Create a Python CLI that loads /app/data.csv with features, label, and protected_attribute, audits a logistic-regression baseline via demographic parity difference, equalized odds difference, and calibration within groups, then mitigates bias using reweighting plus group-wise threshold optimization. Write a before/after fairness report to /app/fairness_report.json, requiring <2% accuracy drop and at least 50% reduction in each measured disparity.', NULL, ARRAY['python', 'optimization']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Create a Python CLI that trains a binary classifier on a tabular dataset, scans all intersectional subgroups (combinations of up to 3 sensitive attributes) to find the worst demographic parity difference and equalized odds violation, and then applies a per-group threshold optimizer to reduce violations below a target while keeping overall AUC within 2%. Output a before/after fairness report (JSON) and save the calibrated, debiased model and the learned group thresholds.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Create a command-line pipeline that trains a credit-approval classifier, computes subgroup fairness metrics (demographic parity, equalized odds, disparate impact), and then applies reweighing plus threshold optimization to mitigate bias. Save a before/after fairness report JSON, per-group decision thresholds, and the mitigated model artifact.', NULL, ARRAY['optimization']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a CLI pipeline that scans a raw text corpus, detects PII (emails, phones, names, addresses) via regex+NER, applies consistent pseudonymization, and produces both a redacted dataset and a provenance ledger with SHA-256 hashes, source URLs, and SPDX license IDs. Then evaluate a fine-tuned text generation model for privacy compliance by running canary extraction and membership inference tests, emitting a JSON report of leakage metrics and pass/fail flags against given thresholds.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a CLI that audits a JSONL training dataset and a local text-generation model for policy compliance by verifying data provenance (required source_url and license in an allowed whitelist) and detecting/redacting PII (emails, phones, addresses, SSNs) with deterministic hashing. Run a fixed red-teaming prompt suite against the model to flag PII leakage and disallowed content, then emit a compliance_report.json with per-rule counts, sample snippets, thresholds, overall pass/fail, and a sanitized_data.jsonl output.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a Python CLI that scans /app/data/{train,test} and /app/logs/inference.log for PII (names via a bundled dictionary, emails, phones, IPs, street addresses, government IDs) and validates that each record has provenance in /app/metadata.jsonl with a source, timestamp, and SPDX license from an allowed list. Automatically redact violations, emit a tamper-evident audit to /app/compliance_report.json with SHA-256 hashes and a redaction map, and exit non-zero if any unredacted PII or disallowed licenses remain.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a streaming Python CLI that ingests a large CSV of free-text training data plus a whitelist of permitted licenses, detects and redacts PII (names, emails, phone numbers, addresses, SSNs) via regex and spaCy NER, and validates each row’s provenance against a provided source-to-license map. Output a JSONL compliance report with SHA-256 hashes of raw and redacted text, per-row PII categories found/redacted, license verdicts, and exit non-zero if any violation exceeds configured thresholds.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Create a terminal CLI that validates a dataset’s licensing and provenance for model training by parsing per-file metadata and SPDX identifiers, verifying cryptographic hashes against a manifest, and checking compatibility with a specified usage policy (e.g., commercial use). The tool should emit a machine-readable compliance report and a symlinked approved/ subset, failing with clear diagnostics for missing, incompatible, or conflicting licenses.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Build a CPU-only CLI that trains a simple MLP on a tabular dataset and computes per-sample feature attributions using LIME, KernelSHAP, and Integrated Gradients, normalizing outputs into a common schema. Export JSONL attributions, a global feature ranking, and quantitative consistency metrics (additivity/completeness checks, Kendall tau agreement, and deletion/insertion AUCs) to predefined paths.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Create a CLI tool that trains a scikit-learn tabular classifier on a provided CSV, computes per-sample SHAP (TreeExplainer) and LIME attributions, and writes unified explanations to /app/explanations.json with global summaries. Evaluate faithfulness by producing deletion/insertion curves that mask features by attribution rank, outputting Kendall rank agreement and AUC metrics to /app/faithfulness.csv with deterministic seeding and memory‑efficient batching.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Create a CPU-only Python CLI that trains a scikit-learn RandomForestClassifier on /app/data/train.csv (binary target column ''y''), computes per-sample feature attributions with SHAP TreeExplainer and LIME TabularExplainer on the test split across 10 random seeds, and quantifies explanation stability via Kendall tau of top-10 feature rankings for each sample. Save per-method global summaries (mean absolute attribution), per-sample top-10 attributions, and a comparison report with median stability and method win-rate to /app/explainability/, and exit with nonzero status if SHAP''s median stability is lower than LIME''s.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Implement a CLI that explains predictions of a pretrained HuggingFace sentiment model on a text file using both SHAP and LIME, repeating each method over multiple random seeds to quantify stability (Kendall tau of token-importance ranks and top-k Jaccard overlap). Write a JSON summary of stability metrics, a CSV of per-example attributions, and HTML token heatmaps for the most unstable cases.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Train a gradient boosting classifier on the UCI Adult dataset and generate local explanations for 20 test instances using both SHAP and LIME. Implement a faithfulness evaluation via deletion/insertion curves over ranked features, report AUC metrics for each method, and save per-instance attributions (JSON) and summary plots to /app/.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Build a CLI tool that ingests a set of NORAD TLEs and a ground-station coordinates file, propagates each orbit with SGP4 over a 24-hour window, and computes visibility passes above a given elevation threshold (AOS/LOS times, max elevation, range). Write per-satellite CSV summaries and an aggregated ICS calendar of visible passes.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Build a Python CLI that reads a time–flux light curve from /app/lightcurve.csv, robustly detrends it, and uses a Box Least Squares search to detect the strongest transiting-planet signal. Write the inferred period, transit epoch, depth, duration, and SNR to /app/transit.json and save the phase-folded, binned light curve to /app/phase.csv.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Implement a CLI tool that ingests asteroid astrometric observations in MPC 80-column format, computes a preliminary orbit (e.g., Gauss) and refines it via weighted least-squares differential corrections. Propagate the fitted state to a target UTC with a two-body Kepler solver and write osculating elements, residual RMS, and a topocentric RA/Dec ephemeris from a specified observatory code.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Implement an exoplanet transit search tool that reads a stellar light curve, detrends it, and runs a Box Least Squares period search over a specified grid with careful, unit-aware time handling. Output the best-fit ephemeris (period, epoch, duration, depth) with detection metrics and a phase-folded, binned curve to standardized files.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Implement an initial orbit determination and refinement pipeline that reads multi-epoch asteroid astrometry (UTC, RA, Dec, observatory) to compute a heliocentric two-body solution using Gauss’ method followed by nonlinear least-squares differential corrections. Output Keplerian elements at a target epoch, a 6×6 covariance estimate, propagated ephemerides, and postfit residual statistics.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Build a CLI tool that reads CF-compliant NetCDF files of monthly precipitation and potential evapotranspiration, computes the 12-month SPEI per grid cell via log-logistic fitting with robust handling of missing values, and writes a CF-compliant NetCDF of SPEI. Additionally, produce a CSV time series of global land-area fraction in drought (SPEI <= -1) for each month.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Build a zero-dimensional energy balance climate model that ingests historical radiative forcing and GMST series, estimates the climate feedback and effective heat capacity via constrained least squares, then projects global-mean temperature through 2100 under provided forcing scenarios. Output the fitted parameters, hindcast skill metrics, and scenario trajectories to standardized files.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Create a Python CLI that reads daily gridded NetCDF temperature and precipitation and computes three ETCCDI indices—TX90p, CDD, and PRCPTOT—per grid cell for a user-specified baseline, honoring CF calendars, masks, and leap years, and writes CF-compliant NetCDF outputs. Also compute area-weighted global and region-mean time series using cell areas and save a summary CSV.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Implement a two-box global energy balance model (mixed-layer plus deep ocean) driven by a provided radiative forcing time series to simulate global-mean temperature and ocean heat uptake. Calibrate feedback, heat capacities, and exchange parameters against historical GMST and OHC data, then output ECS, TCR, fitted parameters, and scenario projections.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Build a Python CLI that loads an SBML metabolic model and uses COBRApy with a GLPK solver to perform FBA, then evaluates growth under multiple media conditions and all single-gene knockouts to identify essential genes. Save growth rates per condition and the essential gene list to standardized JSON/CSV outputs.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Create a BioPython-based tool that reads a coding DNA sequence and a list of restriction enzymes, then computes the minimal set of synonymous mutations needed to remove all listed recognition sites while preserving the protein sequence. Ensure no new listed sites are created, favor host-preferred codons to keep GC content within ±5% of the original, and output the mutated sequence and a CSV change log.', NULL, ARRAY['coding']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Create a Python CLI that loads a metabolic network SBML (/app/model.xml), builds the stoichiometric matrix, and performs flux balance analysis to maximize a specified biomass reaction using linear programming. Validate mass balance and bounds, identify exchange reactions, then write the optimal flux vector and dual shadow prices to CSV files.', NULL, ARRAY['python']);
-- Compact batch 17/29: rows 801-850

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Create a Python CLI that loads a molecular dynamics trajectory of liquid water (XYZ frames plus periodic box dimensions) and computes the O–O radial distribution function g(r), its first minimum, and coordination number (integral to the first minimum), as well as the self-diffusion coefficient from the mean-squared displacement under periodic boundaries. Write g(r) to /app/gofr.csv and a JSON report with the first-minimum position, coordination number, and diffusion coefficient to /app/results.json, verifying normalization and minimum image handling.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Using COBRApy, load the provided SBML metabolic network, configure exchange reactions to model aerobic minimal media with glucose, and maximize biomass to compute the optimal growth rate. Then perform a single-gene deletion screen to identify essential genes under these conditions and write the growth rate and the sorted essential gene IDs to output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Physics & Engineering Simulation', 'Build a 1D compressible gas dynamics solver for the Sod shock tube using a conservative finite-volume scheme (MUSCL-Hancock with HLLC flux and CFL control) from Riemann initial data, and write density/velocity/pressure profiles at specified times. Include a check that total mass and total energy are conserved to within a small tolerance.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Physics & Engineering Simulation', 'Implement a 1D compressible Euler solver to simulate the Sod shock tube using a finite-volume Godunov scheme (e.g., HLLC) with CFL-controlled timestepping and positivity preservation. Output density, velocity, and pressure profiles at specified times and report L1 error against the analytic solution.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Physics & Engineering Simulation', 'Implement a 1D finite-volume solver for the compressible Euler equations to simulate the Sod shock tube using an HLLC Riemann solver with a TVD slope limiter, exposing a CLI to set grid size and CFL and writing CSV profiles of density, velocity, and pressure at a target time. Validate conservation of mass/momentum/energy and achieve a small L1 error versus an exact Riemann solution computed by a provided routine.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Physics & Engineering Simulation', 'Implement a 1D time-dependent Schrödinger equation solver using the split-operator Fourier method to simulate a Gaussian wavepacket scattering from a rectangular potential barrier. The CLI should sweep incident energies, compute reflection/transmission probabilities with norm conservation, and validate against the analytic transmission coefficient.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Physics & Engineering Simulation', 'Implement an Euler–Bernoulli beam finite element solver for a uniform clamped–clamped beam that assembles mass and stiffness matrices and computes the first three natural frequencies and mode shapes. Validate the frequencies against closed-form solutions within 2% and save frequencies and mode shapes to specified output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Matrix & Vector Operations', 'Implement a CLI tool that loads a dense or sparse matrix (CSV or Matrix Market) and computes a rank-k approximation using randomized SVD with configurable oversampling and power iterations, writing U, S, and V^T to standardized output files. Include a verification mode that compares top-k singular values and relative reconstruction error against SciPy’s svds on small matrices, enforcing a ≤1% gap in Frobenius-norm error to the reference.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Matrix & Vector Operations', 'Implement a Python CLI that loads A, B, C from /app/*.npy and solves A X B = C for X using factorization-based linear solves without forming the Kronecker product (e.g., via LU/QR and column/row reshaping), with an option for Tikhonov regularization. Save X to /app/output/X.npy and a metrics JSON including relative residual (target ≤ 1e-8) and simple condition estimates.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Matrix & Vector Operations', 'Implement a randomized SVD (Halko algorithm) with configurable rank, oversampling, and power iterations to compute a low-rank approximation of a large dense or sparse matrix loaded from disk. Compare to a deterministic truncated SVD by reporting singular values, relative Frobenius reconstruction errors, and timings in a results JSON, also saving the U,S,V factors.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Matrix & Vector Operations', 'Implement a randomized low-rank SVD with oversampling and configurable power iterations to compute the top-k singular values and vectors for large dense or sparse matrices via a CLI, benchmarking accuracy and runtime against NumPy/SciPy baselines. Validate orthonormality of U and V and relative reconstruction error, writing standardized outputs to files.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Matrix & Vector Operations', 'Write a Python script that loads a sparse SPD matrix A from /app/A.mtx and a vector b from /app/b.npy, then solves Ax=b using preconditioned conjugate gradients with an incomplete factorization (ILU/IC) or Jacobi fallback. Save x to /app/x.npy and a JSON with iteration count, final relative residual, and wall time, ensuring ||A x − b||2 / ||b||2 ≤ 1e-8.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Numerical Integration & Differentiation', 'Build a Python CLI that integrates systems of ODEs defined by a JSON spec using an adaptive Dormand–Prince 5(4) solver with dense output and event/root detection, automatically switching to an implicit BDF method when stiffness is detected via step-rejection heuristics. The tool should write solution snapshots and event times to files and include a test harness that verifies accuracy and performance against SciPy.integrate on provided nonstiff and stiff problems.', NULL, ARRAY['python', 'performance']),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Numerical Integration & Differentiation', 'Implement a Python CLI that performs 1D integration using tanh-sinh (double-exponential) quadrature with adaptive node refinement and error control to handle endpoint singularities. Use it to evaluate the integral of x^(-1/2) * log(x) over [0, 1] to at least 12 correct digits and write both the result and the function-evaluation count to /app/answer.json.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Numerical Integration & Differentiation', 'Implement a Python tool that loads a parameterized ODE and scalar loss from a problem module, integrates forward to produce state samples at given eval_ts, then computes dL/dθ via a reverse-time continuous adjoint solve with accurate interpolation of the forward trajectory. Output the trajectory and gradient along with a verification report that checks the adjoint gradient against central finite differences within a specified tolerance.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Numerical Integration & Differentiation', 'Implement an adaptive Gauss–Kronrod 21/10 quadrature with a best-first error priority queue that handles infinite limits and endpoint algebraic/log singularities via variable transformations, enforcing absolute/relative tolerances and a cap on function evaluations. The CLI loads an integrand from /app/integrand.py and a JSON spec of intervals and tolerances, then writes the integral, error estimate, and evaluation statistics to /app/output.json.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Numerical Integration & Differentiation', 'Implement an adaptive tanh-sinh (double-exponential) quadrature that handles endpoint singularities and infinite limits, returning integrals to specified absolute/relative tolerances while tracking function-evaluation counts. Provide a CLI that reads multiple integrals and intervals from input, computes results without external integration libraries, and writes both values and convergence traces.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Optimization & Root Finding', 'Build a Jacobian-free Newton-Krylov solver with backtracking line search to find the steady state of the 2D Bratu (nonlinear Poisson) equation on an N×N grid. The CLI should accept N and lambda, converge to a specified residual tolerance using GMRES with simple preconditioning, and write the solution field and an iteration/residual log to disk.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Optimization & Root Finding', 'Build a sparse primal–dual interior-point solver for linear programs in standard form (Mehrotra predictor–corrector with adaptive step and iterative refinement) that loads A, b, c from /app/lp.npz and solves to ≤1e-7 KKT residuals. Write the optimal x, primal/dual residual norms, and objective to /app/output.json, and report infeasible/unbounded cases via a homogeneous self-dual start.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Optimization & Root Finding', 'Build a terminal tool that solves a system of nonlinear equations using a Jacobian-free Newton–Krylov method (Newton-GMRES with backtracking line search), loading residual(x) and an initial guess from /app/problem.py. Stop when the 2-norm of the residual is ≤1e-8 or the time budget is hit, and write the solution vector plus iteration and GMRES stats to /app/solution.json.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Optimization & Root Finding', 'Implement a command-line tool that computes the minimax (L-infinity) polynomial approximation of a given function on [a,b] using the Remez exchange algorithm, with optional weighting. Output the polynomial coefficients, the achieved uniform error, and the final set of alternation points to a results file.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Optimization & Root Finding', 'Implement a primal-dual interior-point solver (Mehrotra predictor–corrector) for dense convex quadratic programs that reads H, f, A, b, G, h, and bound vectors from /app/problem.npz, uses a safeguarded line search with regularization, and drives the KKT residual below 1e-6. Output the primal and dual solutions, final duality gap, and residual norms to /app/output/solution.json.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Random Number Generation & Monte Carlo Methods', 'Build a CLI tool that estimates logdet(A) for large sparse SPD matrices via randomized trace estimation of log(A) (Hutch++ with Chebyshev/Lanczos polynomial approximation), using reproducible seeds and batched probes to deliver a 95% confidence interval within a target relative error. The program must read Matrix Market files, validate against exact small cases, and emit a JSON report with estimate, CI, probe count, and timings.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Random Number Generation & Monte Carlo Methods', 'Create a terminal script that loads a symmetric positive-definite matrix from /app/A.npz and estimates log(det(A)) by approximating trace(log A) via stochastic Lanczos quadrature with Hutchinson (Rademacher) probes using a reproducible RNG seed. The tool should adaptively increase probes to meet a target confidence interval and write the estimate, probe count, and CI to /app/answer.json within a time budget.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Random Number Generation & Monte Carlo Methods', 'Implement a CLI tool that, given a symmetric positive definite sparse matrix A (Matrix Market), estimates trace(log A) via stochastic Lanczos quadrature with Hutchinson probes, supporting Rademacher, Gaussian, and Owen‑scrambled Sobol sequences. The tool adaptively increases probes to hit a target relative 95% CI, reports estimate/CI/probe count/RNG/timing to /app/results.json, and validates on small cases against a Cholesky-based exact computation.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Random Number Generation & Monte Carlo Methods', 'Implement a program that estimates the log-determinant of a large sparse SPD matrix using stochastic trace estimation (Hutchinson with Rademacher probes) combined with Lanczos quadrature, adaptively increasing probes until a 95% confidence interval is within ±1% relative error. Output the estimate, confidence interval, and the number of probes used to a results file.', NULL, NULL),
('Scientific Computing & Analysis', 'Numerical Computation & Linear Algebra', 'Random Number Generation & Monte Carlo Methods', 'Implement a stochastic Lanczos quadrature estimator for trace(log(A)) (i.e., log det A) of a large sparse SPD matrix using Hutchinson probes (Rademacher with optional antithetic pairing), reporting the mean, standard error, and 95% CI as functions of probe count and Lanczos steps. Provide a CLI that loads a Matrix Market .mtx, runs with a reproducible seed, and writes per-probe estimates and the final summary to CSV.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Cluster & Batch Job Management', 'Build a CLI that reads a YAML parameter grid, generates and submits a resilient SLURM array job with a launcher mapping SLURM_ARRAY_TASK_ID to parameters, and monitors progress via squeue/sacct until completion. It must auto-requeue failed indices with increased time/memory, throttle or add dependencies based on pending reasons, and produce a CSV with per-index status, exit code, runtime, and MaxRSS.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Cluster & Batch Job Management', 'Build a SLURM workflow manager that submits a 3‑stage pipeline (preprocess → job array → reduction) with dependency chaining, monitors via squeue/sacct, and automatically requeues only failed array tasks (e.g., preempted or OOM) with adjusted resources and retry limits. It must handle SIGTERM for checkpointing and produce a final JSON/CSV report of exit status, retries, elapsed time, and MaxRSS per task.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Cluster & Batch Job Management', 'Build a SLURM-driven workflow that launches a job array for a parameter sweep, enforces job dependencies (build -> stage data -> array -> postreduce), detects and auto-recovers preempted/failed tasks via sacct, and aggregates per-task elapsed time and GPU allocation into results.json. Provide scripts to submit, monitor, and checkpoint outputs so requeued runs resume without redoing completed work.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Cluster & Batch Job Management', 'Create a SLURM workflow that runs a parameter sweep as a job array reading /app/params.csv, stages per-task inputs to node-local scratch, and records stdout/stderr per index. Implement automatic failure handling that requeues only failed array tasks once with doubled time/memory and a dependent postprocessing job that uses sacct to write state, runtime, and MaxRSS for all indices to /app/output/summary.csv only after all tasks succeed.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Cluster & Batch Job Management', 'Create a SLURM workflow that shards a large input into 200 parts, submits a fault-tolerant job array with per-task time/memory requests that traps SIGTERM to checkpoint and automatically retries failed elements, then resumes from partial results upon requeue. Monitor progress via squeue/sacct and submit an afterok aggregation job that verifies all shards, computes per-task runtime/memory statistics, and writes a consolidated JSON report to /app/summary.json.', 'hard', NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Distributed Numerical Computation', 'Build an MPI-based distributed PageRank solver for sparse graphs that partitions the adjacency matrix by rows, performs power iteration with teleportation and dangling-mass handling via collective reductions, and outputs the top-k ranked nodes and convergence metrics. Provide a CLI to read an edge list, configure alpha/tolerance, verify correctness by matching NetworkX PageRank on small graphs within 1e-6, and report weak-scaling efficiency.', NULL, ARRAY['distributed']),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Distributed Numerical Computation', 'Implement a distributed 2D Poisson/heat equation solver using MPI with domain decomposition and ghost-cell halo exchanges alongside a serial baseline sharing a common CLI to control grid size, iterations, and tolerance. Save residual history and the final field to standardized outputs, validate convergence and agreement with the serial solution within a set error, and demonstrate strong-scaling across process counts.', NULL, ARRAY['distributed']),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Distributed Numerical Computation', 'Implement a distributed Conjugate Gradient solver for large sparse SPD matrices using MPI (mpi4py) with row-wise partitioning and halo exchanges for sparse matrix–vector products; solve Ax=b from provided Matrix Market and NumPy inputs and write the solution residual, iterations, and per-rank timing to files. Validate against a serial SciPy reference on small cases and demonstrate strong scaling across multiple processes.', NULL, ARRAY['distributed']),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Distributed Numerical Computation', 'Implement an MPI-based 3D Poisson solver on a structured grid using domain decomposition and a conjugate gradient method with Jacobi preconditioning, with each rank maintaining ghost cells and performing halo exchanges each iteration. Validate against an analytical solution by reporting L2 error and residual reduction, and write per-rank timing/scaling metrics and a representative solution slice to output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Distributed Numerical Computation', 'Implement an MPI-based 3D Poisson solver on a uniform grid using 3D Cartesian domain decomposition with nonblocking halo exchanges and Jacobi/CG iteration, writing per-iteration residuals, timings, and a central solution slice to standardized outputs. The harness runs at 1, 2, and 4 ranks to validate against a manufactured sinusoidal solution (error threshold), ensure monotonic residual decrease, and assess near-ideal weak scaling.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'GPU & Accelerator Utilization', 'Create a GPU-accelerated 3D FFT-based Poisson solver using CuPy that computes the potential from a density field under periodic boundary conditions, with a CPU fallback using NumPy/SciPy. The CLI should load a .npy volume, compute potential and total energy, verify relative error ≤1e-6 vs CPU on a test case, and print device info and achieved GPU speedup.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'GPU & Accelerator Utilization', 'Implement a 3D FFT-based Poisson solver with periodic boundaries accelerated on GPU using CuPy/cuFFT, supporting batched right-hand sides and single/double precision. Provide a CLI that validates against a manufactured analytic solution and reports accuracy plus speedup versus a NumPy/FFTW CPU baseline.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'GPU & Accelerator Utilization', 'Implement a 3D heat diffusion solver using CuPy with a custom CUDA RawKernel (7-point stencil) alongside a NumPy CPU reference, then run both on provided initial conditions to the same final time and validate the GPU field against CPU within 1e-6 max error while saving the final array, device name, and per-backend timings/speedup. Enforce the stability constraint dt <= dx^2/(6*alpha) and use shared-memory tiling with coalesced accesses in the GPU kernel.', NULL, ARRAY['backend']),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'GPU & Accelerator Utilization', 'Implement a GPU-accelerated 2D Poisson solver on a large grid using a CuPy/Numba-CUDA stencil (Jacobi or Red–Black Gauss–Seidel), and compare its runtime to a NumPy CPU baseline. The script must reach a specified residual tolerance, validate against an analytic solution, and write residual, iterations, and GPU speedup to /app/metrics.json.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Multi-Threaded & Parallel Programming', 'Build a multi-threaded Smith–Waterman local sequence alignment engine that parallelizes anti-diagonals (wavefront) with OpenMP alongside a serial baseline, exposing a CLI to align FASTA pairs and emit alignment score and traceback. Include correctness checks against a known-good implementation and a benchmark that reports GCUPS and thread-scaling.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Multi-Threaded & Parallel Programming', 'Implement a 2D steady-state Poisson solver on a large grid using Jacobi iterations, providing both a single-threaded baseline and a multi-threaded version (OpenMP or multiprocessing). The CLI must accept grid size and thread count, iterate until a residual threshold, and write the final field plus a convergence/timing summary that demonstrates speedup with ≥4 threads.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Multi-Threaded & Parallel Programming', 'Implement a Conjugate Gradient solver for large SPD sparse matrices in CSR format with both serial and OpenMP-threaded paths for SpMV and vector reductions, selectable via a CLI. Load a provided matrix/vector, solve to a tolerance, and output solution, residual norm, and per-iteration timing to validate correctness and speedup across thread counts.', NULL, NULL),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Multi-Threaded & Parallel Programming', 'Implement serial and OpenMP-parallelized Conjugate Gradient for large SPD sparse matrices in CSR loaded from Matrix Market files, with optional Jacobi preconditioning and fused parallel reductions. Provide a common CLI to solve Ax=b, write residual histories and solution summaries, verify the parallel solution matches serial within tolerance, and report timing-based speedups across thread counts.', NULL, ARRAY['parallel']),
('Scientific Computing & Analysis', 'Parallel & High-Performance Computing (HPC)', 'Multi-Threaded & Parallel Programming', 'Implement serial and shared-memory parallel PageRank for a large sparse graph loaded from disk, using thread-partitioned sparse matvec and residual-based convergence. Output the top-ranked nodes and detailed timings to demonstrate speedup over the serial baseline.', NULL, ARRAY['parallel']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Data Versioning & Dependency Control', 'Build a DVC-backed pipeline (local remote) with dvc.yaml stages to fetch, preprocess, and analyze a dataset while pinning Python dependencies with pip-tools to a requirements.lock. Prove reproducibility by switching between two Git tags and using dvc checkout so that metrics.json and output file checksums exactly match each tag’s recorded state.', NULL, ARRAY['python', 'git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Data Versioning & Dependency Control', 'Build a DVC-managed analysis pipeline that versions two dataset revisions in a local DVC remote and runs a metrics script inside a conda-lock pinned environment. The run must write /app/results.json with metrics and provenance (dataset DVC hash, lockfile digest, git commit, script checksum) and reproduce bit-for-bit identical outputs across reruns.', NULL, ARRAY['git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Data Versioning & Dependency Control', 'Create a reproducible time-travel runner that checks out dataset revisions (e.g., tags data/v1 and data/v2) with DVC, resolves the exact environment via conda-lock, executes a Snakemake pipeline, and writes a provenance manifest listing git SHAs, DVC object IDs, lockfile hash, and output checksums. Generate a metric-drift report comparing the two runs and fail if any data or dependency is not pinned.', NULL, ARRAY['git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Data Versioning & Dependency Control', 'Implement a DVC-managed pipeline (preprocess → train → evaluate) with two local dataset versions and lock Python dependencies via pip-tools so reproducing on v1 yields identical artifact and metrics hashes across runs, while switching to v2 triggers only minimal stage recomputation. The run must emit an output manifest with dataset version, DVC stage checksums, and exact pip freeze, and validate at startup that installed packages match the lockfile.', NULL, ARRAY['python']);
-- Compact batch 18/29: rows 851-900

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Data Versioning & Dependency Control', 'Initialize a Git+DVC project tracking /app/data.csv with a local remote and a two-stage, params.yaml-driven pipeline (preprocess -> analysis) that produces metrics.json and commits the resulting dvc.lock. Pin Python dependencies via a generated lockfile (e.g., pip-tools) and run in a fresh venv; then modify the data to create a second version, check out the original DVC tag to reproduce identical metrics and checksums, and write the original data hash and metric to /app/answer.json.', NULL, ARRAY['git', 'python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Build a deterministic three-stage pipeline (preprocess → train → evaluate) that versions data and intermediates with DVC and logs every run to a local MLflow backend, capturing git commit, parameter values, dataset checksums, metrics, and artifacts. Provide a CLI that reads params.yaml (supports sweeps) and writes /app/provenance.json summarizing the latest run’s lineage (stage order, input/output file hashes, DVC stage IDs, MLflow run IDs), reproducing identical metrics on rerun unless inputs or parameters change.', NULL, ARRAY['backend', 'git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Build a small ML experiment pipeline (data → preprocess → train → eval) that uses DVC to version data and stages and MLflow (file backend) to log params, metrics, artifacts, code version, and environment. Provide a CLI to run a parameter sweep and emit a single provenance.json that links each produced model and report to its DVC hash, MLflow run ID, Git commit, code diff, and random seed, enabling exact reruns.', NULL, ARRAY['backend', 'git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Create a DVC-driven workflow (data → preprocess → model) that logs parameters, metrics, and artifacts to MLflow, and emits a provenance.json capturing dataset checksums, code commit, DVC stage graph, environment snapshot, and MLflow run IDs. Provide a CLI to reproduce any past run from only an MLflow run ID by restoring DVC versions and the environment, then verify artifact byte equality and write a reproducibility report.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Create a DVC-tracked pipeline (preprocess → train → evaluate) for a small scikit-learn task, and instrument each run with MLflow to log parameters, metrics, artifacts, git commit, and DVC data hashes. Prove reproducibility by rerunning dvc repro to obtain byte-identical outputs and emit a provenance report linking MLflow run IDs to the exact DVC stage versions used to produce the final metrics.', NULL, ARRAY['git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Create an MLflow Project that runs a parameterized training pipeline on /app/data.csv, logging parameters, metrics, model artifacts, the Git commit SHA, and an input data checksum for each run. Implement a CLI sweep that launches multiple runs and writes the best run_id and its recorded git_commit to /app/answer.txt, with fixed seeding to make reruns reproduce the same metrics and artifacts.', NULL, ARRAY['logging', 'git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Convert a geospatial analysis notebook that reprojects a provided shapefile and computes polygon areas under two CRSs into a headless, reproducible CLI executed via papermill/nbconvert, pinning GDAL/PROJ and setting deterministic env vars (e.g., PROJ network/grid settings and single-threaded BLAS). The run must produce identical CSV/PNG artifacts and a results.json with area summaries and SHA256 hashes across repeated clean-container executions.', NULL, ARRAY['container']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Convert a provided Jupyter notebook into a deterministic, headless analysis pipeline by parameterizing randomness, extracting it to a Python script, and orchestrating execution with a Makefile that builds a fully pinned, hashed environment (e.g., pip-tools) and runs papermill to regenerate results. The pipeline must yield byte-identical CSV/PNG outputs across reruns and emit a manifest.json capturing package lock hashes, data checksums, seeds, and system info for verification.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Create a CLI pipeline that executes /app/analysis.ipynb with papermill using explicit parameters (including a fixed RNG seed), producing deterministic metrics, tables, and plots in /app/outputs and verifying bit-for-bit identical artifacts across two consecutive runs via SHA256 checksums. The run must also emit a locked requirements file and environment manifest (Python, OS, and package versions) alongside the outputs to ensure re-execution fidelity.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Create a CLI that executes /app/analysis.ipynb via papermill with user parameters, seeds all RNGs (random, NumPy, pandas, and torch if present) and forces single-threaded BLAS for determinism, then exports CSV outputs and a deterministic HTML report. The tool must write a manifest with parameters, pip-freeze, and SHA256 checksums of artifacts and provide a verify mode that re-runs and asserts bitwise identity for identical parameters.', NULL, ARRAY['pandas']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Refactor a provided Jupyter notebook with stochastic analyses into a deterministic, parameterized workflow that runs in a locked Python environment and produces byte-for-byte identical outputs on repeated runs. Expose a single terminal entrypoint that pins dependencies, executes the notebook (via papermill or a jupytext-converted script) with fixed seeds and constrained BLAS threads, and writes results.json plus a reproducibility checksum.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a CLI pipeline that performs a seeded analysis on a provided dataset, exports figures and tables, and assembles a fully self-contained HTML report (embedded images, no external assets) plus a manifest with SHA256 checksums and environment metadata. Orchestrate with Makefile/Snakemake so unchanged inputs trigger cached steps and reruns produce byte-identical outputs.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a Makefile-driven pipeline that ingests experiment CSVs, computes grouped summaries with bootstrap CIs, renders figures/tables, and compiles a templated Markdown into a standalone HTML (and optional PDF) report via Pandoc with embedded assets. The workflow must support incremental rebuilds and include a provenance appendix (git commit, CLI args, pip freeze), writing /app/report/index.html and a machine-readable /app/results.json.', NULL, ARRAY['git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a Snakemake pipeline that parameterizes and executes a Jupyter notebook (via Papermill) across rows in a config CSV, aggregates outputs into figures and tables, and compiles a Quarto/Pandoc HTML+PDF report with embedded provenance (input checksums, software versions) and a run manifest. The workflow must be fully incremental (no re-execution on unchanged inputs), produce deterministic artifacts, and write the final reports and exported assets to a versioned /app/report directory.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a Snakemake pipeline that runs a parameterized Monte Carlo study, exports figures and summary tables, and renders a Quarto report to both HTML and PDF. The workflow must record seeds, config, and dependency versions into a manifest to ensure exact reproducibility across runs.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a reproducible, Makefile-driven pipeline that reads a provided CSV, computes summary statistics and bootstrap 95% CIs in Python, saves tables/figures, and renders a self-contained HTML report from a Jinja2 template under /app/report. The build must be deterministic with a fixed seed and incremental (only re-running steps when inputs or templates change).', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Build a CLI that ingests a directory of environmental sensor CSV/TSV files with mixed time zones and units, converts all measurements to SI using declared metadata, aligns timestamps to UTC, resamples to uniform 1-minute intervals, flags/removes outliers, and imputes short gaps. Write a single tidy, columnar Parquet dataset with standardized NaNs and stable column order, plus a JSON file summarizing QC metrics and unit conversions applied.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Build a CLI that ingests a folder of heterogeneous NetCDF climate model files, maps variables to CF-standard names, converts units to SI, reprojects to a target grid, and aligns time onto a unified daily calendar with gap filling. Output a consolidated chunked Zarr store and a manifest CSV documenting provenance, unit conversions, and optional baseline-normalized anomaly fields.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Create a CLI that ingests a directory of daily CF-netCDF climate files, harmonizes units (e.g., K→°C), decodes mixed time units/calendars, merges along time, masks fill values, removes outliers, and resamples to monthly means. Compute 1991–2020 per-gridcell climatology and z-score anomalies, then export a chunked, compressed Zarr dataset with consolidated metadata and a Parquet summary index.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Create a CLI tool that ingests a DICOM CT series, converts pixel data to Hounsfield Units via RescaleSlope/Intercept (handling per-slice calibration), clips to [-1024, 3071], and resamples to 1 mm isotropic voxels while preserving spatial metadata. Write a single 3D NIfTI (.nii.gz) with correct RAS affine and a JSON summary of original/resampled spacings, dimensions, and any slices skipped due to corruption.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Create a Python CLI that ingests a directory of daily CF-NetCDF climate files, harmonizes variable names/attributes, converts units to a specified target, stitches a continuous time axis over a given range, and repairs single-day gaps via flagged interpolation. Compute per-calendar-month climatology over a baseline and standardized anomalies, then write a CF-compliant compressed NetCDF of the cleaned series plus a CSV of the monthly climatology.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Build a Python CLI that ingests a multi-band satellite GeoTIFF (with NIR and Red), computes NDVI with nodata handling, reprojects to EPSG:3857, and writes a colorized XYZ tile pyramid into an MBTiles database with correct bounds and metadata. Additionally, vectorize pixels where NDVI exceeds a threshold into simplified GeoJSON polygons and export a PNG quicklook map.', NULL, ARRAY['python', 'database']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Build a Python CLI that ingests a time-series of multispectral GeoTIFFs and a polygon ROI, decodes per-pixel cloud masks from QA bands, harmonizes projection/resolution, and computes a cloud-free median NDVI composite. Output a cloud-optimized GeoTIFF with overviews, a quicklook PNG with ROI outline and scale bar, and a CSV of per-ROI summary statistics.', NULL, ARRAY['python', 'cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Create a CLI that georectifies an unreferenced aerial image using provided ground control points (pixel ↔ lon/lat), warps to a target CRS, and outputs a tiled, compressed Cloud-Optimized GeoTIFF with correct nodata and overviews. Also produce a quicklook PNG, a GeoJSON footprint of the warped image, and a JSON report of per-point residuals and overall RMSE.', NULL, ARRAY['cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Create a Python CLI that mosaics multispectral GeoTIFF tiles, reprojects to EPSG:3857, clips to polygons in a GeoJSON ROI, computes NDVI from specified Red/NIR bands while honoring nodata and an optional cloud mask, and saves a color-mapped NDVI PNG plus a Cloud-Optimized GeoTIFF. Also compute per-polygon zonal statistics (mean, median, std, pixel count) and write them to a GeoJSON output.', NULL, ARRAY['python', 'cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Create a Python tool that reads a folder of multi-date satellite GeoTIFFs with varying CRSs and resolutions plus an AOI polygon, reprojects and clips them to a common grid, and builds a cloud-robust medoid mosaic across dates using RGB+NIR bands. Write the mosaic as a Cloud-Optimized GeoTIFF, a PNG quicklook with AOI overlay, and a JSON report of per-band mean/std and fraction of valid pixels.', NULL, ARRAY['python', 'cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Build a CLI tool that loads an irregularly sampled light curve from /app/lightcurve.csv (t, flux, sigma), computes a generalized Lomb–Scargle periodogram to identify the top periodicities with false-alarm probabilities, and produces publication-quality periodogram and phase-folded plots. Refine each candidate via weighted sinusoid fitting and write a summary CSV of frequency, period, semi-amplitude, phase, and FAP.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Create a CLI tool that computes a multitaper (DPSS) power spectral density of a 1-D signal and automatically detects narrowband line components above a robust noise floor. Apply notch filtering at detected lines and save the cleaned signal, PSD before/after, and a figure highlighting the peaks removed.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Create a CLI tool that computes power spectral density estimates for multichannel signals using both Welch and multitaper (DPSS) methods, saving PSDs and 95% confidence intervals to CSV plus comparison plots. Validate normalization via Parseval’s theorem by requiring the PSD-integrated variance to match the time-domain variance within 2% and report any channels that fail.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Implement a Python CLI that loads a multichannel time series, applies adaptive notch filtering at mains and harmonics plus a zero-phase bandpass, then computes multitaper PSDs and magnitude-squared coherence; it must detect and report dominant peaks, band powers, spectral entropy, and a coherence matrix to /app/results.json and save publication-quality PSD/coherence plots. Include a test mode that synthesizes known signals to validate peak frequencies within ±0.2 Hz and coherence above 0.9.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Implement a matched-filter detector for transient chirp signals: read a 1D noisy time series and a template waveform, estimate the noise PSD via Welch, whiten both, and compute the matched-filter SNR time series using FFT-based convolution. Write the peak detection time and SNR to a results JSON and save plots of amplitude spectral density (pre/post whitening) and the SNR time series.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Build a Python CLI that loads a gridded NetCDF atmospheric dataset (temperature on lat–lon–pressure), computes a zonal-mean latitude–pressure cross-section and derives the tropopause pressure using the WMO lapse-rate criterion, then plots a publication-quality contour/contourf figure with an inverted log-pressure axis, labeled isotherms, and an overlaid tropopause line using Matplotlib. Save both PNG and SVG figures and export the zonal-mean fields and tropopause profile to specified output files.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Build a Python tool that loads a gridded NetCDF climate dataset, computes a 1981–2010 monthly climatology and anomalies for a chosen year, and generates a publication-quality, three-panel figure: (1) a global Robinson-projection contour map of annual-mean anomaly with coastlines and significance hatching, (2) an equatorial Hovmöller diagram (time vs longitude), and (3) a zonal-mean anomaly profile, all using a shared color normalization. Save both PNG and PDF outputs with consistent, colorblind-safe styling and embedded metadata.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Create a Python CLI that reads a NetCDF surface temperature dataset, computes a 1981–2010 monthly climatology and 2016 anomalies, and produces a publication-quality multi-panel figure: a global anomaly map (Robinson projection with coastlines), a latitude–time Hovmöller, an area-weighted global-mean time series, and an anomaly histogram. Save both PNG and vector PDF with colorblind-safe palettes, labeled colorbars, and consistent typography.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Create a Python script that loads a 2D velocity field (u, v) from /app/velocity.npy, computes speed and vorticity, and generates a two-panel Matplotlib figure: left panel shows a pseudocolor speed heatmap with overlaid streamlines and a sparsified quiver; right panel shows filled vorticity contours with contour lines. Save the figure as /app/flow_figure.png and /app/flow_figure.pdf with labeled colorbars, equal aspect ratio, consistent fonts, and grid-aligned axes.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Write a Python CLI that loads gridded geophysical data (lat, lon, time, variable) from a NetCDF file, computes a 30-year climatology and anomalies for a target month, and generates a 3-panel publication figure: global anomaly map with coastlines, zonal-mean latitude–anomaly cross-section, and a time series with rolling mean and confidence band. Use Matplotlib (and Cartopy for maps) with a symmetric, colorblind-safe diverging colormap centered at zero and fixed normalization across panels, saving the figure PNG and a JSON with summary statistics.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 1D finite-volume solver for the linear advection equation u_t + a u_x = 0 on [0,1] with periodic boundaries, supporting both first-order upwind and second-order MUSCL-Hancock schemes with CFL-controlled time stepping and CLI selection. Validate against the exact traveling-wave solution for a smooth periodic initial condition by outputting solutions at requested times and reporting L1/L2 errors that demonstrate first- vs second-order convergence over at least three uniform grid refinements.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 1D heat equation solver using the Crank–Nicolson scheme with a Thomas tridiagonal solver on a uniform grid with Dirichlet boundaries. Validate by comparing to the analytical solution u(x,t)=exp(-pi^2*t)*sin(pi*x) (alpha=1), reporting max and L2 errors at specified times and demonstrating near second-order convergence under grid refinement.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 2D Poisson solver on the unit square using a geometric multigrid V-cycle with red-black Gauss–Seidel smoothing and second-order finite differences, with CLI options for grid size and cycle parameters. Validate via a manufactured solution (e.g., u(x,y)=sin(pi x) sin(pi y)) to demonstrate O(h^2) convergence and quantify residual reduction per cycle versus a plain Gauss–Seidel baseline.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 2D Poisson solver on the unit square with Dirichlet boundaries using second-order finite differences with at least two solvers (e.g., Gauss-Seidel and Conjugate Gradient) exposed via a CLI. Validate against the analytical solution u(x,y)=sin(pi x)sin(pi y) by grid refinement (e.g., N=32,64,128), reporting L2/L∞ errors and observed order to a results file and failing if order < 1.9 or residual tolerances are unmet.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a Gauss–Legendre quadrature generator using the Golub–Welsch algorithm to compute nodes and weights for arbitrary n (e.g., up to 2048) and provide a CLI to write them to disk. Validate by integrating polynomials up to degree 2n−1 over [-1, 1] and reporting the maximum absolute error against exact values, ensuring it stays below a specified tolerance.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Optimize a naive Python finite-element stiffness matrix assembly for a 2D Poisson problem that performs incremental CSR updates inside nested loops by profiling hotspots and refactoring to a vectorized batch COO (I,J,V) build with a single CSR conversion. Verify numerical equivalence of the assembled matrix and improved end-to-end solve time, and emit a JSON report of timings and speedups.', NULL, ARRAY['python', 'profiling']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile a naive finite-element stiffness-matrix assembly for a 2D Poisson problem on triangular meshes and optimize the hotspot by vectorizing element computations and constructing the global matrix in CSR with preallocated buffers or Numba. Validate numerical equivalence within tolerance across provided meshes and emit timing and speedup metrics.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile a naive nested-loop Python implementation of pairwise Euclidean distances between two sets of 128D vectors and replace the hotspot with a vectorized formulation and/or a Numba/Cython kernel using cache-friendly memory access and preallocation. Validate numerical equivalence (≤1e-8), emit before/after profiles and timing summaries, and achieve at least a 20x speedup on inputs of size Q=5000, D=50000.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile a naive pure-Python 3D heat equation explicit solver stepping 100 iterations on a 128^3 grid, then vectorize the stencil update, optimize memory access, and accelerate with Numba (parallel) to achieve ≥10× speedup while keeping max absolute error ≤1e-6 versus the baseline. Provide a CLI benchmark that runs pre/post-optimization versions, captures timing and max error, and writes a JSON report to /app/bench.json.', NULL, ARRAY['python', 'parallel', 'optimization']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile and optimize a naive 3D 7-point Jacobi solver for the Poisson equation on a uniform grid, transforming a triple-nested loop baseline into a high-performance version via cache blocking/tiling, vectorized memory access, and optional Numba or OpenMP. The CLI should run solves on several grid sizes, verify residual reduction against a reference solution with fixed boundary conditions, and print a profiling report that shows at least a 5× speedup over the baseline.', NULL, ARRAY['performance', 'profiling']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Design and package a Python library for uncertainty propagation offering linearized (Jacobian-based) and Monte Carlo estimators for vector-valued functions, with interchangeable NumPy and JAX backends. Ship a type-hinted API, Sphinx docs with runnable examples, unit tests with finite-difference validation, a CLI that evaluates models from JSON, and a wheel for local installation.', NULL, ARRAY['python', 'api', 'installation']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Develop a Python interval arithmetic library that guarantees enclosure for elementary functions using outward rounding and vectorized NumPy operations, with a clean API, type hints, and Sphinx docs driven by doctests. Include a CLI to evaluate expressions over named intervals and a test suite that verifies inclusion properties and monotonicity on randomized cases.', NULL, ARRAY['python', 'api']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Develop a Python library for lazy linear operators (matrix-free) that supports composition, adjoints, and iterative solvers (CG/LSQR) with NumPy/SciPy backends. Package it with pyproject.toml, Sphinx docs with examples and autodoc, a small CLI to apply/solve from JSON input, and unit tests verifying algebraic identities and solver accuracy.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Develop a Python library that implements polynomial chaos expansion (PCE) for uncertainty propagation with Legendre/Hermite bases, supporting coefficient fitting from samples, surrogate evaluation, and moment estimation. Package it with a CLI to build/evaluate PCEs from CSV data, comprehensive unit tests, and Sphinx-based API/user docs with examples.', NULL, ARRAY['python', 'api']);
-- Compact batch 19/29: rows 901-950

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Develop a typed Python library for uncertainty propagation via Polynomial Chaos Expansions, supporting Gaussian/Uniform inputs, sparse regression and quadrature fitting, and computing means/variances plus first/total Sobol indices. Provide a CLI that loads a black-box model from a Python module and a JSON distribution spec to write a results JSON, and include unit tests and Sphinx docs with API and examples.', NULL, ARRAY['python', 'api']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Testing & Verification Frameworks', 'Build a metamorphic and differential testing framework for FFT/IFFT routines that auto-generates random and structured signals, validates Parseval’s theorem, round-trip fidelity, and the convolution theorem, and cross-checks outputs between numpy.fft and an FFTW-based CLI tool within set tolerances. Integrate deterministic seeding, tolerance budgets, and performance regression guards into a CI pipeline that fails on numerical drift or speed regressions.', NULL, ARRAY['testing', 'performance']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Testing & Verification Frameworks', 'Build a metamorphic and property-based test harness for a 2D Poisson solver that generates randomized periodic RHS fields, asserts exact recovery on single Fourier modes, checks symmetry/conservation invariants, and differential-tests against an independent finite-difference reference with convergence and error thresholds. Provide a CLI to run the suite and a CI workflow that records metrics and fails on tolerance regressions.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Testing & Verification Frameworks', 'Build a pytest + Hypothesis metamorphic testing suite for a time-integration library (explicit/implicit Runge–Kutta) that verifies order of accuracy via step-halving, checks conserved quantities on canonical systems (harmonic oscillator, Kepler), and validates A-stability on the Dahlquist test equation. Provide a CLI to run the suite with numeric tolerance gates, enforce coverage thresholds, and emit JUnit XML and coverage artifacts suitable for CI.', NULL, ARRAY['testing']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Testing & Verification Frameworks', 'Create a pytest + Hypothesis test harness that ingests a finite-difference PDE solver and verifies second-order accuracy via manufactured solutions, conservation and boundary-condition compliance, and monotonic error reduction under grid refinement. The suite must emit JUnit XML and a JSON convergence report and fail if the estimated order falls below 1.8 on any tested problem.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Testing & Verification Frameworks', 'Create an automated test/CI harness that runs a provided 1D PDE solver across successively refined grids on manufactured solutions, computes L2 errors, fits the empirical convergence order, and fails if it falls below a set tolerance. Include metamorphic tests for boundary-condition transformations and a discrete conservation check, and emit JSON and JUnit XML summaries.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Differential Equation Solvers', 'Build a Python CLI that performs adjoint-based parameter estimation for an ODE by integrating both the forward system and its continuous-time adjoint to obtain exact gradients of a least-squares misfit. Apply it to fit the Lorenz system’s (sigma, rho, beta) to a provided noisy trajectory, and report recovered parameters along with adjoint–finite-difference gradient agreement.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Differential Equation Solvers', 'Build a Python CLI that solves the Lane–Emden boundary-value problem y'''' + (2/x) y'' + y^n = 0 with y(0)=1, y''(0)=0 using a shooting method and SciPy''s solve_ivp, seeding near x=0 from a SymPy-derived series expansion. Write the sampled solution and the first zero-crossing radius to output files and report max error versus the closed-form solutions for n=0 and n=1.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Differential Equation Solvers', 'Implement a 1D Fisher–KPP reaction–diffusion simulator using FiPy that, given D and r, initializes a step profile, advances to a traveling-wave regime, and estimates the wavefront speed from threshold crossings over time. Save positions and the speed estimate to outputs and verify the speed is within 5% of the theoretical 2*sqrt(D*r).', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Differential Equation Solvers', 'Implement a 1D heat-equation solver with time-dependent source and mixed (Robin) boundary conditions using second-order finite differences in space and Crank–Nicolson time stepping, solving the per-step tridiagonal system via the Thomas algorithm. The CLI should ingest problem parameters and output snapshots at requested times to CSV and additionally run a mesh-refinement check to confirm ~O(Δx^2 + Δt^2) convergence.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Differential Equation Solvers', 'Implement a Python CLI that solves the 1D Fisher–KPP reaction–diffusion PDE on [0, L] with Neumann boundaries via method-of-lines (finite differences) using SciPy’s stiff integrator from a compact initial condition. Estimate the traveling wave speed from the simulation and verify it is within 5% of the theoretical minimum 2*sqrt(D*r), writing the estimated speed and error to /app/answer.json.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Finite Element & Numerical Methods', 'Implement a 1D discontinuous Galerkin solver for linear advection with periodic boundaries, selectable numerical flux (upwind/Rusanov), and SSPRK time stepping; expose CLI options for polynomial order, CFL, and final time. Output field snapshots and L2 error versus the exact shifted solution, and verify k+1 convergence across mesh refinements.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Finite Element & Numerical Methods', 'Implement a 2D finite element solver for -∇·(k∇u)=f on a Gmsh triangular mesh with mixed Dirichlet/Neumann boundaries, assembling a sparse system and solving with Conjugate Gradient and a basic preconditioner. Validate via a manufactured solution by reporting L2 and H1-seminorm errors across at least two mesh refinements, and write both the nodal field and error summary to output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Finite Element & Numerical Methods', 'Implement a 2D linear-elasticity finite element solver using first-order (P1) triangular elements that reads a Gmsh (.msh) cantilever beam mesh, assembles the global system with mixed Dirichlet–Neumann boundary conditions, and solves for displacements. Output nodal displacements and von Mises stress (VTK/CSV) and write total strain energy and tip deflection to /app/answer.json.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Finite Element & Numerical Methods', 'Implement a Python finite element solver for 2D linear elasticity on an L-shaped domain using P1 triangles from a provided Gmsh mesh, enforcing Dirichlet and traction boundary conditions and computing von Mises stress. Include a Zienkiewicz–Zhu a posteriori error estimator with Doerfler marking to drive three adaptive refinement cycles, writing VTK field outputs and a JSON convergence summary (DOFs, ||u||_H1 error, observed rate).', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Finite Element & Numerical Methods', 'Implement a sparse finite element modal analysis tool for 2D trusses: parse nodes and bar elements with A, E, and ρ, assemble global stiffness and consistent mass matrices, apply fixed DOFs, and compute the first k natural frequencies and mode shapes via eigsh. Write frequencies to an output text file and mode shapes to a mesh-compatible format (e.g., VTK/CSV).', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Parameter Sweeps & Sensitivity Analysis', 'Build a 1D heat-equation simulator (explicit FTCS with Dirichlet boundaries) that sweeps spatial resolution and timestep to explore CFL stability and convergence, comparing against an analytic solution. For each (dx, dt), record stability, L2 error at a fixed final time, and estimate order-of-accuracy across resolutions in a summary CSV.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Parameter Sweeps & Sensitivity Analysis', 'Build a CLI tool that runs a stochastic SIR epidemic simulation (Gillespie SSA) and performs a Sobol global sensitivity analysis over R0, mean infectious period, and initial infected fraction, reporting first- and total-order indices for peak prevalence, time-to-peak, and final size. Use Saltelli sampling with a fixed seed, parallelize simulations, and write indices and summary metrics to CSV/text outputs.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Parameter Sweeps & Sensitivity Analysis', 'Build a Monte Carlo simulator for the 2D Ising model with periodic boundaries, sweeping temperature across a range and multiple lattice sizes to compute ensemble magnetization, energy, specific heat, susceptibility, and Binder cumulant. Estimate the critical temperature by locating the susceptibility peak and Binder cumulant crossing, saving the full per-temperature statistics and Tc estimate to outputs.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Parameter Sweeps & Sensitivity Analysis', 'Build a Python CLI that simulates the Lotka–Volterra predator–prey ODE across thousands of parameter samples (birth, predation, mortality, efficiency) drawn via Sobol or Latin hypercube designs, recording summary metrics such as final populations, peak amplitudes, and oscillation period per run. Compute and save first-order and total Sobol sensitivity indices for each metric, along with a CSV of runs and a JSON report of indices.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Parameter Sweeps & Sensitivity Analysis', 'Implement a Python CLI that integrates the Lorenz ''63 system and, for a grid of (sigma, rho) values at fixed beta, computes the largest Lyapunov exponent to map chaotic vs non-chaotic regions, writing a CSV heatmap and the estimated boundary. Assess numerical sensitivity by rerunning a subset with stricter solver tolerances and reporting deviations in the exponent.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Stochastic or Agent-Based Simulations', 'Build a discrete-event simulator for an M/M/c/K queue with balking at full capacity and exponential reneging, using a fixed RNG seed and automatic warm-up detection before collecting statistics across multiple replications. Output per-replication and aggregated 95% CI estimates for throughput, loss probability, mean queue length, and waiting time to standardized CSV/JSON files.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Stochastic or Agent-Based Simulations', 'Implement a discrete-event simulation of a priority M/M/c/K queue with balking and reneging, reading parameters from /app/scenario.yaml, and run batched replications with fixed RNG seeds to estimate per-class throughput, mean wait, and server utilization with 95% CIs to /app/results.json. Include a test mode that sets K→∞ and a single class to validate against the analytical M/M/c steady-state formulas within a specified tolerance.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Stochastic or Agent-Based Simulations', 'Implement a discrete-event simulator for an M(t)/M/c/K queue with two priority classes (preemptive-resume) and impatient customers (reneging), supporting reproducible random seeds and multi-run parameter sweeps via CLI. For each run, write CSVs with time-series queue lengths and per-class summary metrics (utilization, mean/95th-percentile wait, abandonment rate).', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Stochastic or Agent-Based Simulations', 'Implement a discrete-event simulator for an open Jackson queueing network (2–4 M/M/1 nodes) with configurable arrival/service rates and routing probabilities, running multiple replications with independent RNG seeds to estimate steady-state throughput, utilization, mean queue lengths, and waiting times. For provided test cases, compare simulated metrics and 95% CIs to analytic formulas and fail if discrepancies exceed 5%, writing standardized CSV/JSON outputs.', NULL, NULL),
('Scientific Computing & Analysis', 'Simulation & Modeling', 'Stochastic or Agent-Based Simulations', 'Implement a stochastic chemical kinetics simulator that loads a reaction network from a simple JSON schema and runs both exact Gillespie SSA and an adaptive tau-leaping variant across multiple random seeds. Save ensemble trajectories and checkpointed means/variances, and report agreement metrics between the two methods within specified tolerances.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Descriptive Statistics & Summarization', 'Build a memory-bounded CLI that streams a large (possibly gzipped) CSV to compute per-column descriptive stats (count, mean, std, min/max, skewness, kurtosis) and approximate quantiles (1,5,25,50,75,95,99) using online algorithms (e.g., Welford + KLL), with NA handling and optional group-by by a categorical field. Write a JSON report including equal-width histograms per column and enforce quantile approximation error ≤0.005 while keeping peak RAM ≤200 MB.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Descriptive Statistics & Summarization', 'Build a streaming command-line tool that ingests large CSV shards with columns group, value, weight and outputs per-group weighted descriptive summaries (count, mean, unbiased variance, MAD, IQR), approximate quantiles (p5, p50, p95) via a quantile sketch, and 20-bin histograms using a Freedman–Diaconis rule under a fixed memory cap. Write deterministic summaries to /app/output/summary.csv and /app/output/histograms.json suitable for automated tolerance-based checks.', 'hard', NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Descriptive Statistics & Summarization', 'Implement a streaming CLI that computes per-column (and optional group-by) descriptive stats—count, mean, variance, min/max, skewness, kurtosis—and approximate quantiles (p1, p5, p25, p50, p75, p95, p99) from a large CSV using a memory-bounded quantile sketch (e.g., t-digest/P^2), outputting a compact JSON summary and fixed-bin histograms. The tool must handle missing values and optional weights while operating on datasets larger than RAM.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Descriptive Statistics & Summarization', 'Implement a streaming CLI that reads a potentially multi-GB CSV at /app/data.csv and computes per-group descriptive statistics (count, mean, variance, min, max) and approximate quantiles (p10, p50, p90) using numerically stable online methods (e.g., Welford plus a GK or t-digest sketch), writing a compact JSON report to /app/summary.json. The tool must ignore NaNs, support optional observation weights, and complete under a strict memory cap and time budget.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Hypothesis Testing & Inference', 'Build a CLI that reads a CSV with group labels and a numeric outcome, performs equivalence testing via two one-sided t-tests (TOST) for independent or paired samples with user-specified equivalence bounds, and reports 90% CIs, effect sizes, and decisions. Automatically choose Welch variants when variances differ and apply Benjamini–Hochberg correction when testing multiple endpoints, writing a concise results table to /app/output/results.csv.', NULL, ARRAY['testing']),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Hypothesis Testing & Inference', 'Build a CLI that reads a CSV with outcome, binary treatment, and optional covariates/strata and conducts a stratified permutation test of zero treatment effect using Freedman–Lane residualization. Compute an exact p-value when permutations are enumerable (or Monte Carlo otherwise) and report a 95% confidence interval for the ATE via test inversion to a JSON file.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Hypothesis Testing & Inference', 'Build a CLI tool that performs permutation-based ANCOVA (Freedman–Lane) to test a binary treatment effect while adjusting for a continuous covariate, optionally within blocking factors. Output the permutation p-value, partial R², and 95% bootstrap CI, and apply Benjamini–Hochberg correction if multiple outcomes are tested.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Hypothesis Testing & Inference', 'Create a CLI tool that ingests per-study effect sizes and standard errors, runs a random-effects meta-analysis (DerSimonian–Laird with optional Hartung–Knapp adjustment), and tests for overall effect and heterogeneity (Cochran’s Q, I²). Write combined estimates, p-values, per-study weights, and a leave-one-out influence summary to standardized output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Hypothesis Testing & Inference', 'Create a script that ingests stratified 2x2 contingency data (A/B by success/failure with a stratum identifier), performs a Cochran–Mantel–Haenszel test to estimate a common odds ratio with 95% CI, and reports its p-value. Additionally run the Breslow–Day test for homogeneity across strata and write the common OR, CI, and both p-values to an output JSON file.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Regression & Curve Fitting', 'Build a CLI tool that fits an errors-in-variables Deming regression for method-comparison data, estimating the error variance ratio from replicate measurements and optionally applying Huber M-estimation to orthogonal residuals for robustness. Output slope, intercept, their 95% CIs (bootstrap), the variance ratio estimate, and a CSV of fitted values and orthogonal residuals.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Regression & Curve Fitting', 'Build a CLI tool that fits piecewise linear (segmented) regression with an unknown number of change-points on a noisy 1D dataset, selecting the number and locations via BIC-penalized dynamic programming (e.g., PELT). Save breakpoint positions, segment slopes/intercepts, fitted values and residuals, and bootstrap confidence intervals for parameters to standardized output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Regression & Curve Fitting', 'Build a terminal script that reads /app/series.csv (time,y) and fits a piecewise linear regression with 1–3 unknown changepoints under Huber loss using dynamic programming (or equivalent), selecting the segment count by BIC. Output /app/results.json with changepoint times, segment slopes/intercepts, BIC per model, and residual diagnostics.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Regression & Curve Fitting', 'Create a CLI tool that fits a bi-exponential decay model y(t)=a1*exp(-k1 t)+a2*exp(-k2 t) using the variable-projection method: optimize k1,k2 via nonlinear search while solving a1,a2 by linear least squares at each step, enforcing a1,a2>=0 and k1,k2>0. Report parameter estimates, bootstrap 95% CIs, and predicted values at specified eval times without using high-level curve-fitting helpers.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Regression & Curve Fitting', 'Fit Planck''s law to a measured spectral radiance dataset (wavelength vs intensity), jointly estimating temperature and a gray-body emissivity factor with bounds and optional instrument-response correction from a calibration file. Save parameter estimates with bootstrap confidence intervals and residual diagnostics to standardized output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Time Series Analysis', 'Build a CLI tool that ingests an irregularly timestamped sensor series, resamples to hourly, and fits a Basic Structural Model (local level + local trend + 24-hour seasonality) using a from-scratch Kalman filter/smoother with maximum-likelihood estimation of noise variances while natively handling missing points. Output the decomposed components, the seasonally adjusted series, and 48-hour forecast quantiles (5/50/95%).', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Time Series Analysis', 'Create a CLI that ingests an irregularly sampled multivariate time series, estimates dominant seasonal periods via Lomb–Scargle and multitaper spectral analysis, then fits a seasonal state-space/SARIMAX model with Fourier terms to forecast a specified horizon. The tool must impute missing values, run rolling-origin backtesting, and write forecasts with 80/95% intervals and per-horizon error metrics to standardized output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Time Series Analysis', 'Create a CLI tool that ingests an irregularly sampled time series with gaps, detects dominant seasonal periods via a Lomb–Scargle periodogram, and performs robust STL decomposition after appropriate resampling/imputation. Fit an ARIMA model to the seasonally adjusted component to generate a 7-day forecast with 95% intervals, and write the detected periods, decomposition components, and forecast to standardized CSV outputs.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Time Series Analysis', 'Create a CLI tool that ingests an irregularly sampled univariate time series with optional exogenous variables, resamples to a target frequency, applies a Box–Cox transform, and selects a SARIMAX model via stepwise AIC under stationarity/invertibility constraints. Perform multi-fold rolling-origin backtesting and output n-step forecasts with 95% intervals, residual Ljung–Box diagnostics, selected (p,d,q)(P,D,Q)s, and MASE/sMAPE metrics to standardized files.', NULL, NULL),
('Scientific Computing & Analysis', 'Statistical Analysis & Data Modeling', 'Time Series Analysis', 'Implement a CLI that ingests an irregularly sampled univariate time series, estimates dominant seasonal periods via Lomb–Scargle spectral analysis, and performs STL decomposition using those periods. Fit a SARIMA model to the deseasonalized component to generate 30-step forecasts with 95% intervals, saving detected periods, decomposition components, and forecasts to disk.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Bayesian Parameter Estimation', 'Build a CLI tool that performs Bayesian inference for a stochastic SIR model using ABC-SMC to estimate transmission and recovery rates from observed daily case counts with an adaptive tolerance schedule. The program outputs weighted posterior samples and posterior predictive simulations for a fixed forecast horizon as standardized CSV files.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Bayesian Parameter Estimation', 'Build a CLI tool that performs hierarchical Bayesian calibration of a Michaelis–Menten kinetics model across multiple temperatures, linking Vmax(T) via an Arrhenius law to jointly infer activation energy, pre-exponential factor, Km, and per-experiment noise with MCMC. Write posterior parameter summaries and posterior predictive trajectories with 95% credible intervals for each experiment to designated output files.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Bayesian Parameter Estimation', 'Calibrate an SIR epidemic ODE model to noisy incidence data using Bayesian inference (e.g., PyMC/NumPyro with NUTS), estimating transmission and recovery rates and R0. Run multi-chain MCMC to produce posterior credible intervals and posterior predictive trajectories and save a concise summary artifact.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Bayesian Parameter Estimation', 'Create a Python tool that fits a stochastic SIR model to a provided daily incidence CSV using Particle Marginal Metropolis–Hastings with a bootstrap particle filter, jointly inferring β, γ, initial I0, and a reporting rate under Negative Binomial observation noise. Output posterior samples and 95% credible intervals for parameters plus posterior predictive incidence trajectories to standardized JSON/CSV files.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Bayesian Parameter Estimation', 'Implement a CLI tool that calibrates an SIR ODE model to noisy daily incidence via Bayesian inference (PyMC NUTS), estimating beta, gamma, initial infections, and a reporting rate under a Negative Binomial likelihood. The program must run MCMC, compute R0 and posterior predictive trajectories with coverage metrics, and write diagnostics and posterior summaries to standardized output files.', NULL, NULL);
-- Compact batch 20/29: rows 951-1000

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Error Analysis & Confidence Intervals', 'Create a Python CLI that loads mean and covariance for parameters [a,b,c] of the model y(x)=a*exp(b x)+c and a grid of x, and computes 95% confidence bands for y(x) via both first-order delta-method propagation and Monte Carlo sampling (Cholesky, fixed seed). Write the bands and propagated standard errors to /app/output/bands.csv and a summary JSON, and verify the two methods’ bounds agree within 3% at each x.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Error Analysis & Confidence Intervals', 'Create a Python script that estimates a 95% confidence interval for the half-life parameter in an exponential decay experiment by fitting a non-linear model with heteroscedastic Gaussian noise and performing both profile likelihood and parametric bootstrap, then propagates the parameter uncertainty to predicted counts at specified times. Load data from /app/data/decay.csv and write a JSON summary with the CI endpoints, bootstrap distribution diagnostics, and predicted interval bands to /app/output/results.json.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Error Analysis & Confidence Intervals', 'Fit a nonlinear Michaelis–Menten model to concentration–rate data and compute 95% confidence intervals for Vmax and Km using both an asymptotic (Fisher information/delta) method and a residual bootstrap. Propagate uncertainty to a prediction at a specified substrate level and write point estimates and interval bounds to a standardized results file.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Error Analysis & Confidence Intervals', 'Fit a nonlinear Michaelis–Menten model with weighted least squares to enzyme kinetics data, then compute 95% profile-likelihood confidence intervals for Km and Vmax. Propagate parameter uncertainty to a specified substrate level to produce a 95% prediction interval for the reaction rate and emit all interval endpoints in a results file.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Error Analysis & Confidence Intervals', 'Implement a CLI tool that propagates uncertainty in Steinhart–Hart temperature estimation: given correlated coefficient estimates (A,B,C with covariance) and a CSV of resistance readings with standard uncertainties, compute 68%/95% confidence intervals for T using both the delta method and Monte Carlo sampling with correlation, and write per-sample intervals plus a JSON summary. Include a simulation mode that generates synthetic datasets from a known ground truth to estimate empirical coverage for both methods.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Sensitivity Metrics & Ranking', 'Build a CLI tool that computes first- and total-order Sobol indices (Jansen estimator with Saltelli sampling) for a stochastic black-box model f(x, seed) in /app/model.py under independent Uniform priors, using replicate runs and bootstrap CIs to produce a ranked list of influential parameters. The script must be robust to NaNs/infs in model outputs, respect a configurable time budget, and write both the indices with 95% intervals (JSON) and the parameter ranking (TXT).', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Sensitivity Metrics & Ranking', 'Create a CLI tool that loads a black-box model f(x) from /app/model.py (optionally stochastic via a seed argument) and computes first- and total-order Sobol’ indices using a Saltelli design with Owen-scrambled Sobol sequences and common random numbers under a fixed evaluation budget. Output a CSV of indices with bootstrap 95% CIs and a parameter ranking by total-order effect, and include an automated self-test that recovers Ishigami indices within ±0.02.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Sensitivity Metrics & Ranking', 'Create a CLI tool that loads a black-box model from /app/model.py and input distributions from /app/inputs.json, then computes first-order and total Sobol indices via Saltelli sampling and performs Morris screening for comparison. Output a ranked CSV of parameters by influence with bootstrap 95% confidence intervals, and save the exact sample matrices and RNG seed to /app/artifacts.npz for reproducibility.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Sensitivity Metrics & Ranking', 'Create a Python CLI that loads a black-box model f(x) from model.py and parameter bounds from bounds.json, then computes Sobol first- and total-order indices via Saltelli sampling alongside derivative-based global sensitivity measures (DGSM) via finite-difference gradients. The tool should adaptively increase samples until 95% bootstrap CI widths for S_i fall below a threshold and write indices, CIs, and a consolidated parameter ranking to /app/results.json.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Sensitivity Metrics & Ranking', 'Implement a Python CLI that loads a differentiable JAX model f: R^d -> R, computes derivative-based global sensitivity measures (DGSM) via automatic differentiation on quasi–Monte Carlo samples, and converts them into provable upper bounds on total Sobol indices using Poincaré constants for Uniform/Normal inputs. Write a JSON file with per-parameter DGSM, total-effect bounds, and a ranking by the bounds.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Uncertainty Propagation', 'Build a CLI that propagates uncertainty in a deterministic SIR ODE model with uncertain β, γ, and I0, computing the distributions of peak infected fraction and time-to-peak via both Monte Carlo (with variance reduction) and non-intrusive polynomial chaos. The tool must read input distributions from a config file and output summary statistics and method agreement diagnostics.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Uncertainty Propagation', 'Build a CLI tool that loads a black‑box Python model and a JSON of input uncertainties (marginals plus optional correlation) and propagates them to outputs using both Latin Hypercube Monte Carlo and a sparse Polynomial Chaos Expansion. Report mean, variance, 5th/95th percentiles, and KL divergence between methods, and save the fitted PCE surrogate for reuse.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Uncertainty Propagation', 'Build a polynomial chaos expansion (total order 3) using Smolyak sparse Gauss-Hermite quadrature to propagate a 5-D Gaussian input uncertainty through a provided black-box function, returning mean/variance estimates and a surrogate evaluator. Compare the PCE estimates against a Sobol low-discrepancy Monte Carlo reference and report relative errors and sample efficiency.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Uncertainty Propagation', 'Create a script that reads a JSON file describing correlated lognormal uncertainties for R and C in an RC circuit, then propagates them through the step-response V(t)=V0*(1-exp(-t/(R*C))) to estimate the mean and variance of V(t) over a given time grid using both Monte Carlo sampling and a third-order polynomial chaos expansion. Write per-time statistics and the maximum absolute discrepancy between the two methods to /app/results.json with a fixed random seed for reproducibility.', NULL, NULL),
('Scientific Computing & Analysis', 'Uncertainty Quantification & Sensitivity Analysis', 'Uncertainty Propagation', 'Implement a non-intrusive polynomial chaos pipeline that reads a YAML describing independent input distributions and a black-box model CLI, then builds a sparse Legendre/Hermite PCE via stochastic collocation to propagate uncertainty and estimate the output mean, variance, and 5th/95th percentiles. Validate the PCE by comparing its moments against a fixed-seed Monte Carlo baseline and report relative errors.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Cipher Cracking & Weak Encryption Analysis', 'Crack a corpus of hex-encoded messages encrypted with the same unknown repeating-key XOR by estimating key length via normalized Hamming distance and recovering the key using English frequency scoring. Decrypt all files and write the plaintext of a specified target message to /app/output/target.txt.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Cipher Cracking & Weak Encryption Analysis', 'Exploit AES-CTR nonce reuse across many hex-encoded ciphertexts to reconstruct the keystream via multi-time-pad techniques (pairwise XOR and crib-dragging) and decrypt a designated target message. Write the exact recovered plaintext to /app/secret.txt and the keystream to /app/keystream.hex.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Cipher Cracking & Weak Encryption Analysis', 'Exploit AES-CTR nonce reuse across multiple ciphertexts by using XOR analysis with known file headers and crib-dragging to recover the keystream and decrypt a target message containing SECRET{...}. Write the recovered secret to /app/secret.txt without modifying other files.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Cipher Cracking & Weak Encryption Analysis', 'Recover plaintexts from multiple files encrypted with the same repeating-key XOR (many-time pad) by estimating key length with Hamming distances, crib-dragging using known file headers, and reconstructing the key to decrypt all messages.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Cipher Cracking & Weak Encryption Analysis', 'Recover the seed and decrypt files that were XOR-encrypted with a keystream from Python’s Mersenne Twister (random.Random) seeded by a 32-bit Unix timestamp, using magic headers (e.g., PNG/ZIP) and file mtimes to constrain the search. Implement a CLI that identifies the correct seed, reconstructs the keystream, decrypts all files losslessly, and outputs the recovered seed plus SHA-256 hashes of plaintexts.', NULL, ARRAY['python']),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Password Hash Cracking', 'Build a command-line cracker that parses a mixed credential dump containing unsalted MD5, SHA-1, and NTLM hashes alongside bcrypt entries, auto-detects the hash type per record, and recovers plaintexts for only the weak/unsalted hashes using a provided wordlist plus simple mutation rules. Output cracked username→password pairs in deterministic order to /app/output/cracked.json and leave bcrypt entries untouched.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Password Hash Cracking', 'Extract mixed-format password hashes from a legacy CMS SQLite dump, fingerprint each hash type (unsalted MD5, apr1-crypt, and phpass), and crack only accounts labeled tier=low using a hybrid wordlist+mask attack derived from a provided policy hint file. Output a validated username:plaintext list and a summary indicating which algorithms were cracked and which were skipped.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Password Hash Cracking', 'Given /app/data/users.txt containing username:hash entries in mixed formats (unsalted MD5 hex, SHA-1 hex, and MySQL 4.1 double-SHA1), write a script that auto-detects each scheme and cracks passwords using the provided dictionaries plus a ?d?d mask for numeric suffixes. Output recovered pairs as username:password, sorted lexicographically by username, to /app/cracked.txt.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Password Hash Cracking', 'Given a mixed-format hash dump at /app/hashes.txt containing LM+NTLM (unsalted), raw MD5, and SHA-1, create a workflow that auto-detects each hash type, cracks them via dictionary+rules and targeted mask attacks (including LM half-cracking and recombination), and verifies candidates. Write username:password pairs to /app/cracked.txt and a brief per-hash-type success summary to /app/stats.txt.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Password Hash Cracking', 'Recover plaintext passwords from a SQLite database where each user’s hash is the hex-encoded MD5 of lowercase(email)+":"+password (no salt). Reverse-engineer the format from a provided hint script, then use a dictionary+mask attack to crack all accounts and write user_id,email,password to /app/cracked.csv.', NULL, ARRAY['database']),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Protocol Analysis & Exploit Detection', 'Analyze SMTP traffic to detect a STARTTLS stripping downgrade that exposes credentials, reproduce the attack using standard CLI tools, and write the recovered username:password to /app/creds.txt. Reconfigure the mail server to require TLS (reject plaintext AUTH without STARTTLS) and verify via a captured session that downgrade attempts no longer leak credentials.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Protocol Analysis & Exploit Detection', 'Analyze a TLS-enabled internal service and an accompanying PCAP to detect reuse of a static TLS session ticket encryption key. Extract the ticket key from the service (e.g., config/env/memory) and use it to decrypt the captured session, recovering a secret and writing it to /app/recovered_secret.txt.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Protocol Analysis & Exploit Detection', 'Analyze a reverse-proxy setup (HTTP/2 frontend to HTTP/1.1 backend) to detect H2-to-H1 request smuggling caused by improper forwarding of Connection/TE headers. Craft a single HTTP/2 request to smuggle an internal GET /admin/flag to the backend, extract FLAG{...}, and write it to /app/flag.txt.', NULL, ARRAY['frontend', 'backend']),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Protocol Analysis & Exploit Detection', 'Build a CLI that parses a provided pcap of SMTP/IMAP/POP3 sessions to detect STARTTLS downgrade/misuse and plaintext credential exposure. For each session, report server capabilities, whether TLS was negotiated, and any extracted usernames/passwords in a machine-readable summary.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Protocol Analysis & Exploit Detection', 'Given a TLS 1.2 packet capture and the server’s RSA private key, decrypt and reconstruct the HTTP conversation to extract a leaked bearer token. Replay the token against the local service to obtain a protected secret and persist the result to a specified file.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Reverse Engineering Binary Artifacts', 'Analyze a stripped ELF that reconstructs an AES-128 key at runtime from scattered lookup tables and uses it to decrypt an embedded data blob. Recreate the key derivation and decryption in a standalone script to recover the original JSON config and write it to /app/config.json.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Reverse Engineering Binary Artifacts', 'Analyze a stripped x86-64 ELF that embeds a custom bytecode virtual machine for license verification; extract the bytecode and reimplement the VM in Python to compute the correct license without running the binary, writing it to /app/license.txt.', NULL, ARRAY['python']),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Reverse Engineering Binary Artifacts', 'Analyze a stripped x86-64 ELF that validates input using a custom stack-based bytecode VM and simple anti-debug checks. Reconstruct or emulate the VM to recover the exact passphrase that yields ''ACCESS GRANTED'' and write it to /app/flag.txt.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Reverse Engineering Binary Artifacts', 'Reverse engineer a stripped x86_64 ELF that decrypts a payload using AES-CTR, where the key and nonce are constructed at runtime from several obfuscated constants. Recover the exact key/nonce and write a CLI tool that decrypts /app/secret.enc to /app/secret.txt without modifying the binary.', NULL, NULL),
('Security & Cryptography', 'Applied Cryptanalysis & Reverse Engineering', 'Reverse Engineering Binary Artifacts', 'Reverse-engineer a stripped Linux ELF that embeds a WebAssembly module inside a custom section and validates input by interpreting that WASM; locate and extract the module, reconstruct the check algorithm from its bytecode, and compute the input that makes the program reveal FLAG{...}. Write the recovered flag to /app/secret.txt and note the byte offsets used for the extraction.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Multi-Factor & Token-Based Authentication', 'Configure OpenSSH in the sandbox to require dual authentication: Ed25519 public key plus a TOTP code derived from /app/mfa_seed.txt via keyboard-interactive PAM. Implement a non-interactive client script that computes the current TOTP and successfully scp’s /secure/flag.txt to /app/result.txt as proof of access.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Multi-Factor & Token-Based Authentication', 'Configure an OpenSSH server to require dual authentication: a CA-signed user certificate and a TOTP code via PAM. Generate a user keypair, sign it with a provided SSH CA, seed a TOTP secret for the test user, and prove that SSH access succeeds only with a valid cert plus current OTP while plain keys or incorrect codes are rejected.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Multi-Factor & Token-Based Authentication', 'Implement a minimal OAuth 2.0 device authorization flow that issues RS256 JWT access tokens backed by a rotating JWKS, where user approval requires a TOTP code generated from a provisioned per-user secret. Provide a CLI client that enrolls the TOTP secret, completes the device flow while handling authorization_pending/slow_down, and accesses a protected API; tests validate TOTP windowing, key rotation via kid, and strict audience/issuer checks.', NULL, ARRAY['api']),
('Security & Cryptography', 'Authentication & Access Control', 'Multi-Factor & Token-Based Authentication', 'Patch a vulnerable web API that currently accepts JWTs with alg=none and HS/RS key confusion by enforcing strict RS256 verification against a local JWKS endpoint, validating iss/aud/exp, and implementing key rotation with cache invalidation. Provide CLI scripts that mint a valid token to access a protected endpoint and demonstrate that forged tokens (none, HS-using-public-key, wrong aud/iss, expired) are rejected, writing results to /app/verification.txt.', NULL, ARRAY['web', 'api']),
('Security & Cryptography', 'Authentication & Access Control', 'Password Management & Hashing', 'Create a CLI shadow-upgrader that reads a users.json containing mixed legacy password formats (plaintext, SHA-1 hex, salted SHA-256, bcrypt), authenticates a batch of login attempts, and transparently rehashes verified passwords to Argon2id with per-user random salts and a file-based pepper (/app/pepper.key) in PHC format. Use constant-time verification, enforce target Argon2 parameters, write updates atomically with upgraded_at timestamps, and leave unverifiable entries unchanged.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Password Management & Hashing', 'Create a CLI tool that migrates a legacy user database containing mixed bcrypt/SHA-1 password hashes to Argon2id with per-user salts and a global pepper from an environment variable, encoding hashes in PHC string format. Implement transparent verification that accepts old hashes, rehashes on successful login using constant-time comparisons, and outputs an audit report listing upgraded accounts and any remaining non-compliant entries.', NULL, ARRAY['database']),
('Security & Cryptography', 'Authentication & Access Control', 'Password Management & Hashing', 'Given a SQLite users.db containing mixed password schemes (md5, sha256-crypt, and bcrypt), implement a CLI that verifies logins against the current scheme and transparently upgrades accounts to Argon2id in PHC format with per-user random salts and tuned parameters. Provide a dry-run audit mode that reports counts by scheme and flags records requiring rehash due to weak algorithms or sub-threshold cost factors.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Password Management & Hashing', 'Implement a login-and-migrate utility that validates users from a legacy SQLite users.db storing unsalted SHA-1 password hashes using constant-time comparison, and on successful authentication rehashes the password to Argon2id with per-user 16-byte random salt and a global pepper from /run/secrets/pepper, storing a PHC-formatted hash and schema version. Output a machine-readable /app/migration_report.json summarizing upgraded accounts, skipped/locked users, and the Argon2 parameters used.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Password Management & Hashing', 'Refactor a legacy Flask-style auth service that stores unsalted SHA-1 password hashes to use Argon2id with per-user random salts and configurable memory/time parameters, including automatic rehash-on-login when parameters are outdated. Provide a CLI to bulk-migrate users by validating credentials from a provided login_attempts.csv, updating hashes in-place, and emit an /app/audit.json listing any accounts that could not be migrated.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Role-Based & Policy Enforcement', 'Configure PostgreSQL with role-based and row-level security: create tenant-scoped roles, enable RLS on a table using tenant_id policies, and expose a masked view so analysts can only SELECT non-PII while ingesters can INSERT but not read. Verify cross-tenant reads are denied and admins retain full access via psql tests.', NULL, ARRAY['postgresql', 'security']),
('Security & Cryptography', 'Authentication & Access Control', 'Role-Based & Policy Enforcement', 'Configure an SSH-based Git service that enforces role-based access using authorized_keys restrictions and server-side hooks: create ci and maintainer roles where ci can only perform read-only git-upload-pack on /srv/repoA.git, while maintainer can push to repoA.git but not to repoB.git. Disable interactive shells and all forwarding features for both roles, and verify that unauthorized operations are denied while allowed ones succeed.', NULL, ARRAY['git']),
('Security & Cryptography', 'Authentication & Access Control', 'Role-Based & Policy Enforcement', 'Create Unix roles via groups ''analyst'' and ''operator'' and enforce POSIX ACLs (including default ACLs) so analysts have read-only access to /app/data/reports (present and future files) while operators can modify /app/data/ops but cannot read reports. Lock down sudoers so only operators may run `/usr/bin/systemctl restart metrics-agent.service` without a password and cannot execute any other command or escalate to a shell, and provide a script that proves both allowed and denied behaviors.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Role-Based & Policy Enforcement', 'Implement UNIX RBAC for a project workspace by creating dev, qa, and ops roles, enforcing a permission matrix with POSIX ACLs (including default ACL inheritance), and adding a sudoers.d rule that lets only ops run a specific appctl restart command without enabling shell escapes or env-based escalation. Provide a verifier that impersonates sample users to confirm read/write/execute behavior, ACL inheritance on new files, and denials for unauthorized sudo or file operations.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Role-Based & Policy Enforcement', 'Provision role-based access on a Linux host by creating dev, ops, and audit roles with UNIX groups, POSIX ACLs, and sudoers.d policy. Enforce that devs can write to /srv/app/releases but cannot restart services, ops may only sudo systemctl restart app@* without shell escapes or env-based escalation, and auditors can read /var/log/app but not secrets, with default ACLs applied to new files.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Session Management & Revocation', 'Augment a JWT-based API to use short-lived access tokens and refresh-token rotation with reuse detection backed by a persistent token-family store; presenting any previously rotated refresh token must revoke the entire family and add active access-token JTIs to a denylist. Provide an admin CLI/endpoint to revoke all sessions for a user and verify with curl that compromised tokens are rejected across multiple app processes while other users remain authenticated.', NULL, ARRAY['api']);
-- Compact batch 21/29: rows 1001-1050

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Security & Cryptography', 'Authentication & Access Control', 'Session Management & Revocation', 'Configure an OpenSSH service to support key-based session revocation by adding a revoke-key command that, given a public key or fingerprint, updates the RevokedKeys file, reloads sshd, enumerates and terminates any active sessions established with that key, and emits a machine-readable report. Verify that future logins with the revoked key are refused while unaffected users remain connected.', NULL, NULL),
('Security & Cryptography', 'Authentication & Access Control', 'Session Management & Revocation', 'Extend a JWT-based FastAPI authentication service to implement session-level revocation using per-token jti entries in Redis with TTL and refresh-token rotation with reuse detection that triggers immediate user-wide logout. Provide a CLI/endpoint to revoke all sessions for a username and verify that revoked access tokens return 401 while new logins succeed, with revocation instantly propagated via Redis pub/sub.', NULL, ARRAY['redis']),
('Security & Cryptography', 'Authentication & Access Control', 'Session Management & Revocation', 'Implement cluster-wide session revocation for a demo API by adding a PostgreSQL-backed denylist keyed by JWT jti with expiration matching token exp, and wiring LISTEN/NOTIFY to invalidate in-process caches immediately. Provide a CLI revoker that accepts a token or jti and proves the revoked session is rejected instantly across processes while other active sessions remain valid.', NULL, ARRAY['api', 'postgresql']),
('Security & Cryptography', 'Authentication & Access Control', 'Session Management & Revocation', 'Implement refresh-token rotation with reuse detection and a centralized, Redis-backed revocation list for a JWT-based API, propagating revocations to all workers via pub/sub. Verify that using a stolen refresh token revokes its entire chain and that rotating the JWKS signing key invalidates only old-key tokens while unaffected sessions continue.', NULL, ARRAY['redis', 'api']),
('Security & Cryptography', 'Cryptographic Operations', 'Digital Signatures & Verification', 'Create a CLI that canonicalizes JSON per RFC 8785 (JCS) and generates/verifies detached Ed25519 signatures over the canonical bytes using local PEM keypair files. Verification must accept semantically identical JSON regardless of whitespace or key order and exit non-zero if any value changes.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Digital Signatures & Verification', 'Create a CLI that canonicalizes a directory into a Merkle root (including path, mode, size, and SHA-256 for every file) and produces a detached Ed25519 signature for that root. Implement a verify subcommand that, given the public key and signature, detects any tampering and prints a deterministic list of mismatched paths or ''VERIFIED'' on success.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Digital Signatures & Verification', 'Create a terminal CLI that signs artifacts with Ed25519 producing detached .sig files and verifies downloads against a local trust store using a configurable threshold policy (e.g., require 2-of-3 maintainer signatures). Output a JSON report detailing key IDs verified, failures, and overall status, and support key rotation by marking old keys as retired while still validating past releases.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Digital Signatures & Verification', 'Implement a CLI tool that verifies OpenPGP cleartext-signed messages (RFC 4880) with correct canonical text handling (CRLF normalization, dash-escaping, and trailing whitespace) against a provided public keyring. The tool scans /app/messages for .asc files and outputs a per-file validity report to /app/verification.json.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Digital Signatures & Verification', 'Recover a secp256k1 ECDSA private key from two signatures that reused the same nonce k using the provided message digests and (r,s) pairs in /app/signatures.log, and output the key in hex to /app/privkey.hex. Use the recovered key to produce a DER-encoded detached signature for /app/release.bin at /app/release.bin.sig, and include a verify.py that prints VERIFIED when the signature checks against the derived public key.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Encryption & Decryption', 'Build a CLI that performs envelope encryption of a directory using a hybrid RSA-OAEP (recipient key) + AES-256-GCM (data) scheme in a streaming fashion (tar -> encrypt) to produce a single archive.enc with a minimal JSON header. Provide a decrypt command that uses the recipient’s RSA private key to recover the data key, verify integrity, and reconstruct the directory byte-for-byte, failing on any tag or SHA-256 manifest mismatch.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Encryption & Decryption', 'Create a CLI that performs hybrid envelope encryption for a directory: each file is encrypted with a fresh AES-256-GCM key and nonce, and the key is wrapped for multiple recipients using RSA-OAEP (SHA-256), emitting a per-file JSON manifest with wrapped keys, nonce, and tag. Implement a decrypt mode that accepts any matching recipient private key, verifies tags before writing, reconstructs paths and permissions, and aborts on any authentication failure.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Encryption & Decryption', 'Implement a hybrid envelope-encryption CLI that recursively encrypts a directory using per-file random AES-256-GCM keys, wrapping each key with an RSA-2048 public key and preserving permissions/mtimes via a JSON manifest. Provide a decrypt/verify command that uses the RSA private key to restore files byte-for-byte and detect tampering via AEAD tag checks and SHA-256 hashes.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Encryption & Decryption', 'Implement an envelope decryption tool that processes all *.enc files under /app/data: each file begins with a base64 JSON header containing an RSA-OAEP-wrapped AES-256-GCM key and 12-byte nonce, followed by raw ciphertext and tag. Use the PEM private key at /app/keys/priv.pem to unwrap keys, decrypt outputs to /app/dec preserving directory structure, and write a decrypt.log listing any files that fail authentication.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Encryption & Decryption', 'Use OpenSSL CMS to implement multi-recipient envelope encryption of a file with AES-256-GCM using both an RSA and an EC certificate, then demonstrate that either private key can decrypt while tampering triggers authentication failure. Automate key/cert generation, encryption, per-recipient decryption, and produce the recovered plaintext and an audit log of verification steps.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Hashing & Integrity Verification', 'Build a CLI that creates a chunked SHA-256 Merkle-tree manifest for a directory (e.g., 4 KiB chunks), emitting a root hash and inclusion proofs, and a verifier that can validate a specific file or chunk without re-hashing the entire dataset. The verifier must pinpoint tampered chunks and output a minimal diff (paths and chunk indexes) with expected vs actual hashes.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Hashing & Integrity Verification', 'Build a command-line verifier that, given a directory of fixed-size file chunks and their inclusion proofs plus a claimed root hash, validates a Merkle tree over the chunks (SHA-256 by default) and identifies any tampered chunks. Write the zero-based indices of all failing chunks to /app/bad_chunks.txt and exit non-zero if the computed root does not match.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Hashing & Integrity Verification', 'Implement a CLI that computes a deterministic Merkle tree over /app/data (leaf = SHA-256 of file contents with executable-bit encoded; internal = SHA-256(left||right) in stable lexicographic path order) and verifies the root against /app/expected_root.txt. Output /app/verification_report.json listing any files or links violating integrity (hashing symlink targets, handling Unicode and hardlinks, skipping sockets/FIFOs).', 'hard', NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Hashing & Integrity Verification', 'Implement a CLI that generates a SHA-256 Merkle tree manifest for a directory (capturing file sizes, permissions, and symlink targets) and outputs the root hash to a JSON file. Provide a verify mode that loads the manifest to report added/removed files, metadata changes, and chunk-level byte-range differences for large files, honoring ignore patterns in a .integrityignore.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Hashing & Integrity Verification', 'Implement a CLI tool that computes a deterministic SHA-256 Merkle tree over all regular files in /app/data (sorted paths), outputs the root to /app/output/root.txt, and can emit and verify inclusion proofs for any file. Use it to validate a provided set of proofs and write the paths of any tampered files to /app/output/tampered.txt.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Key Generation & Management', 'Create an offline GPG primary certification key and add dedicated signing, encryption, and authentication subkeys; export a revocation certificate and relocate the primary key to an “offline” store. Implement a rotation that replaces the encryption subkey, updates the public keyring, and proves functionality by signing, encrypting, and decrypting test data using only the subkeys.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Key Generation & Management', 'Design a JWKS-based JWT signing key rotation workflow: generate an initial Ed25519 signing key and JWKS, issue a token with a kid, then rotate by adding a new key, switching issuance, and finally retiring the old key. Provide commands/scripts to validate both old and new tokens during the overlap window and only the new token after finalization, writing verification outcomes to an output file.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Key Generation & Management', 'Establish an internal SSH Certificate Authority that issues, revokes, and rotates user and host certificates via shell scripts. Validate by configuring sshd to trust the CA, publishing a KRL, rotating to a new CA key with dual-trust during migration, and demonstrating that only valid, non-revoked certificates can authenticate.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Key Generation & Management', 'Set up an SSH Certificate Authority that issues both user and host certificates, configure a local sshd to trust the CA, and prove access requires a signed user cert and a valid host cert signature. Rotate the CA by generating a new key, re-signing credentials, publishing a Key Revocation List for the old CA/host certs, and demonstrate that old certs are refused while new ones succeed.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Randomness & Entropy Generation', 'Build a user-space entropy combiner and CTR-DRBG that gathers timing jitter and kernel getrandom(), applies SP 800-90B on-line health tests to raw samples, and derives output via HKDF + AES-CTR. Provide a CLI that emits N bytes, supports reseed, blocks until a configurable entropy threshold is met, and refuses to reuse state across fork unless reseeded.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Randomness & Entropy Generation', 'Implement a CLI entropy-mixer that collects bits from getrandom(), optional RDRAND, and CPU timing jitter, combines them via HKDF-SHA256 into a seed, and drives a ChaCha20-DRBG with fork detection and periodic reseeding to emit N bytes. Provide a health subcommand that performs basic randomness sanity checks and refuses to run if only a single untrusted source is present.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Randomness & Entropy Generation', 'Implement a ChaCha20-based DRBG CLI seeded exclusively via Linux getrandom, with a timing-jitter collector for periodic reseeding and online health tests (FIPS continuous test and SP 800-90B repetition/adaptive proportion). Generate 512 KiB of random data to /app/out.bin and a JSON health report with reseed events and PASS/FAIL to /app/health.json.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Randomness & Entropy Generation', 'Implement a user-space ChaCha20-DRBG that seeds from getrandom(2), a persistent seed file, and timer jitter mixed via HKDF-SHA256, and atomically commits reseed state. Provide a CLI that emits N random bytes and a JSON health report (sources, reseed_count), and passes a compromise-and-recover test for forward/backward secrecy.', NULL, NULL),
('Security & Cryptography', 'Cryptographic Operations', 'Randomness & Entropy Generation', 'Implement a user-space entropy collector that samples CPU timing jitter and other non-blocking sources, conditions them with SHA-256, and seeds a ChaCha20-based CSPRNG with a CLI to emit N bytes to /app/output/random.bin. Include basic health tests (repetition count and adaptive proportion) and persistent reseed state to avoid output reuse across restarts.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'File & Memory Forensics', 'Analyze a Linux RAM dump to locate a shared object mapped from a deleted path or memfd that indicates code injection. Carve the ELF from the dump, save it to /app/output/evil.so with its SHA-256, and report the hosting PID and the suspicious VMAs involved.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'File & Memory Forensics', 'Analyze a Linux memory image to locate a ChaCha20-Poly1305 key and nonce left in a suspected process’s heap, reconstruct them from little-endian 32-bit words, and decrypt /app/capture.enc into /app/plaintext.out. Record the PID and virtual address where the key was recovered in /app/findings.txt.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'File & Memory Forensics', 'Analyze a Linux process memory dump to recover an in-memory OpenPGP private key (ASCII-armored or binary), reconstruct and import the key, and decrypt a provided ciphertext to validate the extraction.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'File & Memory Forensics', 'Given a Linux process core dump and an AES-CTR–encrypted blob, recover the 32-byte AES key from memory artifacts (e.g., key schedule structures or contiguous hex bytes) and use it to decrypt the blob to plaintext. Validate success by matching the plaintext’s checksum against a provided reference.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'File & Memory Forensics', 'Given a Linux process core dump produced during an active ransomware run, carve the ChaCha20-Poly1305 master key and per-file nonces from memory and use them to decrypt all samples under /app/encrypted to /app/recovered. Output a machine-readable incident report including the recovered key material, originating PID/command line, and hashes of the decrypted files.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Log Analysis & Intrusion Detection', 'Analyze BIND9 query logs and system auth logs to detect DNS tunneling via long high-entropy subdomains and elevated NXDOMAIN rates, then identify the tunneling domain, originating client IP, and time window. Write findings to /app/output/report.json and provide a CLI that prints the top 5 suspicious FQDNs with counts based on your heuristics.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Log Analysis & Intrusion Detection', 'Analyze Suricata EVE JSON and SSH auth logs to detect a DNS TXT-based tunneling session followed by lateral movement via successful SSH login from the same source IP. Output the exfiltration domain and compromised username to /app/findings.json.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Log Analysis & Intrusion Detection', 'Correlate nginx access.log, /var/log/auth.log, /var/log/cron.log, and syslog to detect a webshell-driven intrusion: identify the initial exploit request and source IP, enumerate compromised accounts via SSH, and reconstruct the timeline through persistence installation and data exfiltration. Write the attacker IP(s), first compromise timestamp, compromised usernames, path and SHA256 of the dropped payload, and the exfil destination to /app/incident_report.txt.', NULL, ARRAY['installation']),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Log Analysis & Intrusion Detection', 'Implement a terminal tool that ingests DNS resolver logs and network flow summaries to detect DNS tunneling exfiltration via heuristics (label entropy, query length uniformity, periodicity) and correlate suspicious domains to source hosts. Output a ranked alert list, a chronological incident timeline, and a JSON file of IOCs (domains, client IPs, first/last seen, estimated bytes).', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Log Analysis & Intrusion Detection', 'Parse DNS resolver logs (e.g., BIND/Unbound) to detect DNS tunneling by flagging high-entropy, long subdomains with abnormal query/NXDOMAIN rates. Attribute offending client IPs and reconstruct the exfiltrated payload by decoding base32/base64 subdomain chunks into /app/exfiltrated.txt.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Malware Behavior Analysis', 'Create a terminal-based workflow that unpacks a multi-stage Linux dropper (bash + ELF), statically recovers its C2 configuration by deobfuscating an XOR+Base64 blob, and dynamically confirms behavior by tracing syscalls and outbound DNS/HTTP. Output a JSON report listing decrypted C2 domain(s), beacon interval, persistence mechanisms (e.g., ld.so.preload or cron edits), files touched, and any exfiltration paths.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Malware Behavior Analysis', 'Inspect an obfuscated Linux ELF dropper that persists via /etc/ld.so.preload and exfiltrates data using DNS TXT queries. Recover and decode its embedded C2 domain (XOR+base64) to /app/iocs.txt and cleanly remove the persistence without breaking legitimate binaries.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Malware Behavior Analysis', 'Perform hybrid (static and controlled dynamic) analysis of a suspicious Linux ELF dropper that conceals its configuration via XOR+Base64 and establishes persistence via a systemd user service, using tools like objdump, strings, strace, and a local network namespace to observe behavior safely. Recover and save the decrypted config, enumerate all IOCs (C2 endpoints, file paths, service names), and produce a concise YARA rule that identifies the sample while minimizing false positives.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Malware Behavior Analysis', 'Perform static and controlled dynamic analysis of a packed Go-based Linux ELF dropper that decrypts its configuration at runtime. Without contacting external hosts, recover and write its decoded C2 endpoints, mutex/campaign ID, and installed persistence artifacts (e.g., systemd unit/timer names or crontab entries) to /app/iocs.txt, and extract the embedded payload to /app/payload.bin.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Malware Behavior Analysis', 'Statically and/or dynamically analyze an obfuscated Linux ELF dropper at /app/samples/dropper to recover its behavior and configuration, including persistence mechanism and C2 endpoints. Decrypt the embedded config (XOR key stored in the .note.sec_key section) and write the persistence artifact path, targeted exfiltration globs, and C2 domain:port as JSON to /app/iocs.json.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Timeline Reconstruction', 'Correlate Apache/Nginx access logs, SSH auth logs, Zeek HTTP/conn logs, and a small pcap with mixed time zones to reconstruct the compromise timeline from first exploit to data exfiltration. Output a UTC-normalized CSV of events and a brief report identifying attacker IPs, exploited endpoint, privilege escalation, persistence, and exfiltration channel.', NULL, NULL),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Timeline Reconstruction', 'Correlate Linux auth logs, web server access logs, Docker daemon/container JSON logs, and shell history with partial clock skew to reconstruct a unified UTC timeline of a short-lived breach from initial credential reuse through containerized payload execution to data exfiltration and a log-tampering attempt. Produce a machine-readable CSV including timestamp, event label, actor/IP, and source file, noting any time-skew adjustments applied.', NULL, ARRAY['web', 'docker', 'container']),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Timeline Reconstruction', 'Create a CLI that parses rotated nginx access logs, SSH/auth logs, and Linux auditd logs from multiple hosts, auto-normalizes timestamps (timezones, DST changes, and a known 7-minute skew), and reconstructs a single UTC incident timeline from initial web hit to privilege escalation. Output a CSV with sequence numbers, unified timestamps, actor (IP/user), correlated PIDs, and flags for suspected timestomping when file MAC times conflict with process execution events.', NULL, ARRAY['web']),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Timeline Reconstruction', 'From /app/artifacts (kube-apiserver audit logs, containerd logs, application auth logs, and a crontab snapshot), reconstruct a minute-by-minute timeline of a Kubernetes pod compromise that began with a leaked CI/CD token, escalated via an over-privileged service account, established cron-based persistence, and exfiltrated data to an external host. Output a strictly chronological /app/output/timeline.json with fields [timestamp, source, action, subject, details] and tag the events of initial access, privilege escalation, persistence, and exfiltration.', NULL, ARRAY['kubernetes']),
('Security & Cryptography', 'Forensics & Incident Analysis', 'Timeline Reconstruction', 'Reconstruct a minute-by-minute timeline of a supply-chain compromise by correlating auditd, apt/dpkg logs, pip metadata, systemd journal, and netflow/pcap artifacts from initial package install to persistence and exfiltration. Output a sorted timeline.csv and a findings.txt naming the malicious package, the first persistence write path, and the exfiltration destination IP.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Exploit Simulation & Payload Delivery', 'Against a sandboxed Flask web app, identify a server-side template injection sink in a preview endpoint and craft an obfuscated Jinja2 payload that writes /tmp/mark_ssti.txt and exfiltrates http://127.0.0.1:8081/flag to /app/result.txt despite a naive WAF blocking common tokens. Success is validated by the presence of the marker file and the exact flag content.', NULL, ARRAY['web']),
('Security & Cryptography', 'Network & System Penetration Testing', 'Exploit Simulation & Payload Delivery', 'Assess a misconfigured Docker Engine API exposed on localhost by deploying a minimal containerized payload that mounts the host filesystem read-only to retrieve a designated secret. Copy the secret to /app and then cleanly remove all created containers, images, and networks to restore the environment.', NULL, ARRAY['docker', 'api']);
-- Compact batch 22/29: rows 1051-1100

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Security & Cryptography', 'Network & System Penetration Testing', 'Exploit Simulation & Payload Delivery', 'Exploit a Shellshock vulnerability in a CGI-enabled web service by sending a crafted HTTP header that executes a controlled payload to create /app/pwned.txt and print the effective UID. Then harden the server (e.g., CGI config or shell update) and verify the payload no longer executes.', 'hard', ARRAY['web']),
('Security & Cryptography', 'Network & System Penetration Testing', 'Exploit Simulation & Payload Delivery', 'Exploit a misconfigured JWT verification flow vulnerable to HS/RS algorithm confusion by crafting a forged admin token to access a protected API and read /app/flag.txt. The task validates payload delivery by confirming elevated access and can be extended with a code/config hardening fix.', 'hard', ARRAY['api']),
('Security & Cryptography', 'Network & System Penetration Testing', 'Exploit Simulation & Payload Delivery', 'Exploit an unsafe YAML deserialization endpoint in a Python microservice that uses yaml.load on user input to achieve remote command execution. Craft and deliver a payload from the terminal that writes /tmp/pwn1, /tmp/pwn2, and /tmp/pwn3 and verifies their creation.', NULL, ARRAY['python']),
('Security & Cryptography', 'Network & System Penetration Testing', 'Port Scanning & Enumeration', 'Design a command-line workflow that uses masscan for fast discovery and nmap (with select NSE scripts) for deep enumeration to build a complete inventory of open TCP/UDP ports and service metadata across a provided 172.20.0.0/24 sandbox network. The tool should fall back to netcat banner grabs when NSE is inconclusive and write a normalized JSON report to /app/scan.json plus a brief human summary to /app/REPORT.md.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Port Scanning & Enumeration', 'In a dual-stack (IPv4/IPv6) sandbox with multiple interfaces, perform comprehensive TCP/UDP scanning to enumerate open ports, service versions, and TLS SNI/ALPN-based virtual hosts using nmap, masscan, and netcat. Extract a port-knocking sequence from a discovered banner to temporarily expose a hidden management port and produce a structured report of all findings.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Port Scanning & Enumeration', 'Perform TCP and UDP scans against the target to find open ports, fingerprint services (and versions) on non-standard ports, and detect protocol mismatches such as SSH speaking on 8080 or HTTP wrapped in TLS on 8443 using banner grabs and NSE scripts. Produce a machine-readable inventory (including port, transport, service, version, HTTP titles, and TLS certificate SHA-256 fingerprints) at /app/scan/report.json.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Port Scanning & Enumeration', 'Scan node1 and node2 for open TCP and UDP ports using masscan or nmap, then perform service enumeration (nmap -sV/-sC plus banner grabs with netcat/curl) to capture service names, versions, HTTP titles, TLS certificate CNs, and SSH key fingerprints. Produce a deduplicated, host-and-port-sorted JSON report at /app/scan_results.json summarizing all discovered endpoints and metadata.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Port Scanning & Enumeration', 'Scan the provided lab subnet to discover an HTTP service on a non-standard TCP port, an SNMP agent on UDP 161, and a custom binary service on an unknown port. Use nmap (including relevant NSE scripts) and netcat/openssl to enumerate and write the HTTP title, SNMP sysName, and the custom service’s banner to /app/enum.txt.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Post-Exploitation & Persistence Analysis', 'After a simulated compromise, build a CLI triage tool that hunts for stealthy Linux persistence (systemd timers/units, cron spools, SSH authorized_keys options, and /etc/ld.so.preload), disables any malicious entries safely, and emits a machine-readable remediation report. Validate by proving the planted persistence no longer triggers while legitimate services remain functional.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Post-Exploitation & Persistence Analysis', 'After an authorized compromise of a Linux host, enumerate and analyze multiple persistence footholds including a masqueraded systemd unit, an SSH authorized_keys entry with a forced-command, and a cron job carrying a base64-encoded payload. Decode each mechanism, map it to its on-disk artifacts and startup triggers, generate a JSON report with indicators of compromise, and disable the persistence so it does not survive reboot.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Post-Exploitation & Persistence Analysis', 'Analyze a compromised Linux host that employs layered persistence—a PATH-hijacked systemctl shim, a systemd service drop-in masquerading as a core daemon, an /etc/ld.so.preload hook, and a cron self-healer using immutable files—to identify, validate, and eradicate each mechanism. Produce a machine-readable IOC report (paths, hashes, service/timer names, environment changes) and a verification script that proves persistence does not return after service restarts.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Post-Exploitation & Persistence Analysis', 'Investigate a compromised Linux host to enumerate and validate stealth persistence mechanisms (e.g., a malicious systemd drop-in, cron.d @reboot job, LD_PRELOAD via /etc/ld.so.preload, and an SSH authorized_keys forced-command backdoor), safely neutralize each, and produce a machine-readable report at /app/persistence_report.json. Ensure legitimate services and user access remain unaffected while removing only malicious artifacts.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Post-Exploitation & Persistence Analysis', 'On a compromised Linux host, locate and neutralize multiple persistence mechanisms (a masqueraded systemd service, an LD_PRELOAD backdoor, and a cron-based beacon) without disrupting legitimate services, then produce /app/findings.json detailing each artifact, removal steps, and verification evidence. Confirm remediation by showing the backdoor no longer injects into sshd and that no malicious systemd timers or cron entries remain.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Privilege Escalation Simulation', 'As a low-privileged user, enumerate scheduled tasks and exploit a root cron backup that runs tar on a world-writable directory by abusing wildcard/--checkpoint-action injection to execute a root command. Confirm escalation by reading /root/flag.txt into /app/result.txt and creating a root-owned marker file in /tmp.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Privilege Escalation Simulation', 'Escalate from an unprivileged user to root by abusing an NFS export mounted in the sandbox that was configured without root_squash, demonstrating creation and execution of a setuid helper on the share. After obtaining root, apply and verify the remediation by correcting the export configuration and filesystem permissions so the escalation no longer works.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Privilege Escalation Simulation', 'Identify and exploit a misconfigured Linux capability on a helper binary (e.g., cap_setuid+ep) to escalate privileges and read a root-only secret. Output the exact contents of /root/flag.txt to /app/result.txt without altering system state beyond what’s required.', NULL, NULL),
('Security & Cryptography', 'Network & System Penetration Testing', 'Privilege Escalation Simulation', 'Simulate a privilege escalation via Python library path hijacking of a root-run systemd timer. The agent must find a group-writable plugin directory used by a backup script, plant a malicious module to execute as root on the next run, create /root/owned.txt as proof, and then revert the change.', NULL, ARRAY['python']),
('Security & Cryptography', 'Network & System Penetration Testing', 'Privilege Escalation Simulation', 'Starting from an unprivileged user, enumerate the host to discover a root cron job that uses tar against a writable directory and exploit tar’s wildcard/--checkpoint-action behavior to execute code as root and write a proof file. Then remediate by correcting permissions and hardening the backup command to use absolute paths and safe options so the escalation path is closed.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Container & Cloud Hardening', 'Build a CLI that hardens a Docker Compose project by enforcing least-privilege defaults (non-root user, read-only filesystem, dropped capabilities, no-new-privileges, seccomp/AppArmor profiles, CPU/memory limits, and read-only mounts unless whitelisted) and rewrites compose.yaml in place. Provide a verification script that launches the stack and uses docker inspect to confirm each service satisfies the required constraints.', 'hard', ARRAY['docker']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Container & Cloud Hardening', 'Harden a Docker Compose application by rewriting its services to run as non-root with read-only root filesystems, drop all capabilities (add back only what’s required), enable no-new-privileges, apply a strict seccomp profile, and set memory/PIDs limits. Validate that the app still functions while attempts to perform privileged actions (e.g., mounting, ptrace, writing to root-owned paths) fail within the containers.', 'hard', ARRAY['docker']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Container & Cloud Hardening', 'Harden a Docker Compose-based microservice by running containers as non-root, dropping unnecessary capabilities, enabling no-new-privileges, read-only root filesystems, strict seccomp/AppArmor profiles, and segmented networks with egress allowlists. Implement a policy-as-code linter (e.g., OPA/conftest) that validates the Compose file against these controls, and ensure the project passes the checks.', 'hard', ARRAY['docker']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Container & Cloud Hardening', 'Harden a Dockerized service to run rootless with user-namespace remapping, read-only rootfs and tmpfs overlays, minimal capabilities (e.g., only NET_BIND_SERVICE), no-new-privileges, custom seccomp/AppArmor profiles, and strict ulimits/cgroup constraints while preserving functionality. Provide a docker-compose setup that blocks 169.254.169.254 and non-whitelisted egress, denies hostPath mounts and Docker socket access, and verifies the container cannot write outside its volume or invoke ptrace/keyctl/mount syscalls while health checks pass.', 'hard', ARRAY['docker', 'container']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Container & Cloud Hardening', 'Harden a local Kubernetes cluster by enabling Pod Security Admission at ''restricted'' on a target namespace and adding OPA Gatekeeper constraints requiring runAsNonRoot, readOnlyRootFilesystem, seccompProfile RuntimeDefault, and all capabilities dropped; verify by applying both noncompliant and compliant workloads and writing an enforcement report to a file. Ensure privileged, host* options, and hostPath mounts are rejected while the compliant Deployment runs successfully.', 'hard', ARRAY['kubernetes', 'security']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Filesystem & Permission Security', 'Audit a containerized service for insecure file permissions and symlink misuse: find world-readable secrets and an uploads directory that permits symlink-based writes, then harden by correcting ownership/modes, adding sticky bits where needed, and enforcing least-privilege with POSIX (default) ACLs for the service user. Demonstrate remediation by showing unprivileged users can no longer read the secrets or redirect writes via symlinks.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Filesystem & Permission Security', 'Audit a multi-user workspace to detect world-readable secrets and symlink-based write-escape hazards in shared paths (e.g., /tmp and a build cache), then implement a fix script that enforces least-privilege permissions and blocks symlink traversal. The solution must harden SSH/GPG key directories and files, set sticky-bit and ownership on shared directories, and ensure artifact writes cannot escape the intended workspace.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Filesystem & Permission Security', 'Build a hardening utility that audits and remediates PATH hijacking and temp-dir permission issues: locate world-writable or non-root-owned directories in $PATH, detect user-writable shadow binaries preceding system utilities, and ensure /tmp and /var/tmp have the sticky bit. It must fix ownership/permissions or remove unsafe entries, reorder PATH safely, and emit a before/after compliance report proving that no PATH entry is writable by non-root and that core tools resolve to system binaries.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Filesystem & Permission Security', 'Identify and remediate a symlink race in a root-run log rotation script that compresses and moves files from /var/log/app, where an attacker can swap a log for a symlink to clobber or exfiltrate arbitrary files. Harden the rotation by rejecting symlinks/hardlinks and using O_NOFOLLOW-safe moves, then tighten permissions (logs 600, log dir 700) and emit a change report to /app/rotation_hardening_report.json.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Filesystem & Permission Security', 'Implement a policy-driven filesystem hardener that reads a YAML policy of expected owners, groups, and modes for specified paths, then audits and enforces them while stripping unsafe SUID/SGID bits and setting sticky bits on designated temp directories. It must detect and refuse symlink/hardlink escapes, correct insecure world-readable/writable permissions on secrets, and emit a machine-readable before/after report of all changes and blocks.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Network Security Configuration', 'Configure a stateful nftables firewall implementing a three-packet port-knocking sequence that temporarily opens SSH (port 22) for the knocking source IP, with automatic timeout and rate-limited logging of denied traffic. Provide idempotent scripts to install and persist the rules, and verify that incorrect sequences or scans never open the port and that access reverts after the timeout.', NULL, ARRAY['logging']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Network Security Configuration', 'Harden a Linux host with nftables by implementing a default-deny, stateful firewall that permits inbound HTTPS on 8443 to a sample service, rate-limits SSH (5/s, burst 10), blocks all IPv6, and enforces an egress allowlist (DNS/HTTPS) for both the host and a container bridge subnet. Provide a persistent ruleset and a verification script that proves container isolation, denied outbound ports, and rule survival after a ruleset reload.', 'hard', ARRAY['container']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Network Security Configuration', 'Harden the host by replacing permissive rules with an nftables default-deny policy that allows only inbound TCP 8443 and rate-limited SSH from 10.42.0.0/16, permits established/related traffic, and restricts egress to DNS, NTP, and HTTPS while logging first-hit drops to /var/log/nft-blocks.log via ulogd. Ensure the rules persist across reboot via /etc/nftables.conf and verify with connectivity tests that allowed paths work and blocked paths fail.', 'hard', ARRAY['logging']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Network Security Configuration', 'Implement host egress hardening using nftables: redirect all outbound TCP/80 to a local transparent proxy on 127.0.0.1:3128, allow only DNS and HTTPS to the Internet, block cloud metadata IP 169.254.169.254 and RFC1918 destinations, and set default-drop for everything else. Include a verification script that demonstrates allowed and blocked paths and writes a concise report to /app/firewall_report.txt.', 'hard', ARRAY['cloud']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'Network Security Configuration', 'Using nftables, implement a default-deny egress policy that only allows DNS to the system resolver and HTTPS to a domain whitelist managed as an IP set resolved from /app/allowed_domains.txt, with all other outbound connections rate-limited logged and dropped. Persist the rules in /etc/nftables.conf and provide /app/update_whitelist.sh to refresh the IP set so curl to allowed domains succeeds while others are blocked.', NULL, NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'System & Service Hardening', 'Harden a provided systemd service via a drop-in override that runs it as a dedicated user and applies systemd sandboxing (NoNewPrivileges, CapabilityBoundingSet, ProtectSystem=strict, PrivateTmp, ReadOnlyPaths, RestrictAddressFamilies, SystemCallFilter) while preserving its core UNIX-socket functionality. Tests confirm the service still works but cannot read /etc/shadow, write outside its working directory, bind TCP sockets, or retain elevated capabilities.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'System & Service Hardening', 'Harden a provided systemd unit for a demo HTTP service to enforce least privilege by using DynamicUser, dropping all capabilities, enabling NoNewPrivileges, sandboxing with ProtectSystem=strict/ProtectHome/PrivateTmp, restricting writable paths, and applying a strict SystemCallFilter. Demonstrate the service still functions on localhost while reads of /etc/shadow, execution of new binaries, and writes outside the allowed directory are denied, then output the final unit and a validation log to /app/result.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'System & Service Hardening', 'Harden a provided systemd-managed file-sync service by applying systemd sandboxing and least-privilege features (DynamicUser, capability bounding, filesystem and network/address-family restrictions). Verify the service still syncs files while being unable to write outside /var/lib/sync, read /etc/shadow, spawn a shell, or bind to privileged ports.', 'hard', NULL),
('Security & Cryptography', 'Secure Configuration & Hardening', 'System & Service Hardening', 'Harden a root-running Python web service by converting it into a sandboxed systemd unit that uses an unprivileged DynamicUser, drops capabilities, sets NoNewPrivileges, enforces ProtectSystem=strict/PrivateTmp, applies RestrictAddressFamilies/IPAddressDeny, and a seccomp SystemCallFilter. The service must still serve on localhost:8080 and write only to /var/lib/app, while tests verify it cannot read /etc/shadow, spawn a shell, or make outbound network connections.', 'hard', ARRAY['python', 'web']),
('Security & Cryptography', 'Secure Configuration & Hardening', 'System & Service Hardening', 'Install and harden OpenSSH by creating a non-root deploy user, disabling root and password authentication, enforcing Ed25519 key-only access with modern Ciphers/MACs/KexAlgorithms, and restricting logins to that user via AllowUsers. Prove hardening by successfully logging in to localhost with the key while root/password logins fail, and write the final sshd_config and test results to /app/output.', 'hard', ARRAY['logging']),
('Security & Cryptography', 'Secure Software Development', 'Input Validation & Sanitization', 'Harden a CSV export CLI to prevent spreadsheet formula injection by neutralizing fields that begin with =, +, -, @, or a tab and removing embedded CR/LF control characters from untrusted input. Preserve delimiter/quoting behavior, field order, and whitespace so output is identical except for safely neutralized cells.', 'hard', NULL),
('Security & Cryptography', 'Secure Software Development', 'Input Validation & Sanitization', 'Harden a Node.js CLI that shells out to ImageMagick using user-provided format, size, and filename by replacing string-based exec with safe argument arrays and strict schema validation. Enforce a whitelist of formats, bounded numeric dimensions, and sanitized filenames to prevent command injection while preserving legitimate conversions and exact output naming.', 'hard', NULL),
('Security & Cryptography', 'Secure Software Development', 'Input Validation & Sanitization', 'Harden a Node.js CSV import/export pipeline to prevent spreadsheet formula injection by validating cell content, normalizing Unicode, and prefix-escaping dangerous leading characters (=,+,-,@) while preserving numeric/date parsing and encoding. Rebuild the project and confirm that benign CSVs round-trip unchanged.', 'hard', NULL),
('Security & Cryptography', 'Secure Software Development', 'Input Validation & Sanitization', 'Patch a Python backup CLI that currently builds a shell command like "tar cf - <path> | {compressor} > out.tar.{ext}" from user input, allowing command injection and path traversal. Refactor to use subprocess with argument lists, whitelist compressor choices (gzip/bzip2/xz), canonicalize and restrict the source path to an allowed base directory, and enforce length/charset limits while preserving normal backup behavior.', NULL, ARRAY['python']),
('Security & Cryptography', 'Secure Software Development', 'Input Validation & Sanitization', 'Refactor a FastAPI + SQLite microservice that currently builds SQL from query parameters to enforce strict validation and sanitization using parameterized queries and Pydantic validators (types, length limits, regex whitelists), returning 400s for disallowed input. Include a CLI verifier that exercises endpoints to prove SQLi payloads are neutralized while valid requests still return correct results.', NULL, ARRAY['sql']),
('Security & Cryptography', 'Secure Software Development', 'Secrets & Key Management in Codebases', 'Convert all plaintext secrets in the repo to SOPS-encrypted files using age, adding a .sops.yaml policy and scripts to encrypt/decrypt/rotate while modifying the app to load secrets from environment or decrypted files. Rewrite Git history to remove the original cleartext and install a pre-commit hook that rejects new secret leaks; the harness verifies no secret bytes remain in blobs, packs, or tags and that the hook blocks a seeded leak.', NULL, ARRAY['git']),
('Security & Cryptography', 'Secure Software Development', 'Secrets & Key Management in Codebases', 'Create a Git-aware remediation tool that scans the working tree and full history for hardcoded credentials (provider patterns + entropy), migrates them to environment-driven config backed by a local dotenv/JSON store, and rewrites code to use lookups. The script must purge secrets from history, install a pre-commit hook to block future leaks, and emit a machine-readable report verifying no secrets remain and the app runs with injected env vars.', 'hard', ARRAY['git']),
('Security & Cryptography', 'Secure Software Development', 'Secrets & Key Management in Codebases', 'Identify hardcoded OAuth client secret and JWT signing key in a Node.js service, migrate them to Docker Compose secrets and a .env file with properly generated replacements, and refactor the code and compose config to consume them securely. Add a git pre-commit hook that blocks future secret leaks and demonstrate the app still authenticates and issues tokens using the rotated keys.', 'hard', ARRAY['docker', 'git']),
('Security & Cryptography', 'Secure Software Development', 'Secrets & Key Management in Codebases', 'Refactor a Node.js Express authentication service that currently hardcodes an HS256 JWT secret to use RS256 keys with a local JWKS endpoint and automated key rotation, loading private keys from a protected file path or env-injected secret. Update the app and tests so tokens include a kid, the public JWKS rotates without downtime, and no secrets remain in the repository or Docker build context.', 'hard', ARRAY['docker']),
('Security & Cryptography', 'Secure Software Development', 'Secrets & Key Management in Codebases', 'Scan the repository (including full Git history) to locate hardcoded secrets, rewrite history to purge them, and migrate the values into a SOPS-encrypted secrets.yaml secured with an AGE keypair while refactoring code to read from environment variables. Verify by showing the application runs using decrypted env at runtime and that a secret scanner reports no findings in the working tree or history with only the encrypted file tracked.', 'hard', ARRAY['git']),
('Security & Cryptography', 'Secure Software Development', 'Secure Build & Dependency Management', 'Audit a Python package that includes a Rust extension (built with maturin) using pip-audit and cargo-audit, then apply minimal semver-safe upgrades via constraints.txt and Cargo.toml updates/patches to eliminate all high/critical advisories without altering CLI behavior. Rebuild the wheel, regenerate lockfiles/SBOM, ensure tests pass, and write a JSON report of remediated CVEs to /app/vuln_report.json.', NULL, ARRAY['python', 'rust']),
('Security & Cryptography', 'Secure Software Development', 'Secure Build & Dependency Management', 'Build a CLI that scans a monorepo’s Python (pip) and Node (npm) dependencies with OSV/pip-audit/npm audit, applies minimal non-breaking upgrades to eliminate high/critical CVEs, and regenerates lockfiles with hash/integrity pins. It must emit pre/post CycloneDX SBOMs, run the project tests to validate the upgrade, and fail the run if vulnerabilities persist or the build is not reproducible.', NULL, ARRAY['python']);
-- Compact batch 23/29: rows 1101-1150

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Security & Cryptography', 'Secure Software Development', 'Secure Build & Dependency Management', 'In a mixed Python/Node project, use pip-audit and npm audit to identify one direct and one transitive vulnerable dependency, then remediate with minimal safe upgrades by pinning versions (requirements/constraints and package.json overrides) and regenerating lockfiles with strict hash verification. Enforce deterministic offline installs via pip.conf and .npmrc (require-hashes, npm ci with scripts disabled), generate CycloneDX SBOMs, and output a brief report of fixed CVEs and final versions.', NULL, ARRAY['python']),
('Security & Cryptography', 'Secure Software Development', 'Secure Build & Dependency Management', 'In a polyglot monorepo (Python, Node.js, and Rust), audit dependencies with pip-audit, npm audit, and cargo audit, then apply the minimal safe upgrades and regenerate lockfiles with hashes. Emit a consolidated CycloneDX SBOM and JSON advisory report, wire a CI script to fail on new critical CVEs, and verify builds and tests still pass.', NULL, ARRAY['python', 'rust']),
('Security & Cryptography', 'Secure Software Development', 'Secure Build & Dependency Management', 'Scan a Python + Node monorepo with pip-audit and npm audit, then apply minimal safe upgrades via constraints.txt and npm overrides to remove all High/Critical findings and regenerate lockfiles. Output a unified JSON diff of vulnerabilities before/after and enforce deterministic installs with pip --require-hashes and intact npm integrity fields.', NULL, ARRAY['python']),
('Security & Cryptography', 'Secure Software Development', 'Static Code Analysis & Vulnerability Detection', 'Author a custom Semgrep rule set that models taint flow across a mixed Python/Node.js codebase to detect untrusted input reaching command execution, SQL queries (string interpolation), and unsafe path joins (zip-slip). Run the rules to produce a SARIF report, refactor the code to remediate all high-severity findings, and re-run to confirm zero remaining issues.', NULL, ARRAY['python', 'sql']),
('Security & Cryptography', 'Secure Software Development', 'Static Code Analysis & Vulnerability Detection', 'Create a Semgrep taint-mode rule pack that detects flows from Flask request inputs to sqlite3 query execution (SQL injection) and to filesystem access (path traversal), outputting findings in SARIF. Run the scan on a provided Flask app, then refactor to parameterize queries and normalize/whitelist file paths, and re-scan to verify zero findings.', NULL, ARRAY['sql']),
('Security & Cryptography', 'Secure Software Development', 'Static Code Analysis & Vulnerability Detection', 'Install and run Bandit and Semgrep on a deliberately vulnerable Python microservice, and author custom Semgrep rules to detect ECB mode, static IVs, weak hashes, and subprocess calls with shell=True, then refactor the code to remove all findings. Produce before/after SARIF reports and a CI-failing script that exits non‑zero if any critical issues remain.', NULL, ARRAY['python']),
('Security & Cryptography', 'Secure Software Development', 'Static Code Analysis & Vulnerability Detection', 'Run Semgrep (community security rules) and Bandit on a mixed Python/Node codebase to detect command injection, unsafe YAML loading, path traversal, and insecure temp-file/cookie settings. Fix the findings with minimal behavioral change, add a pre-commit and CI configuration that fails on reintroduction, and emit a before/after SARIF or JSON report to /app/output/findings.json.', NULL, ARRAY['security', 'python']),
('Security & Cryptography', 'Secure Software Development', 'Static Code Analysis & Vulnerability Detection', 'Scan a mixed Java/Python codebase for XML External Entity (XXE) vulnerabilities using Semgrep/Bandit, author at least one custom rule per language to detect unsafe parser instantiation, and refactor the code to harden XML processing (disable DTDs and external entities) without changing observable behavior. Re-run the analyzers to confirm zero findings and ensure all tests still pass.', 'hard', ARRAY['java', 'python']),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Alerting & Incident Response Automation', 'Build a correlation-driven incident responder that tails auth and web access logs to detect account takeover patterns (failed logins from IP A, success from IP B, then privileged action within 5 minutes) and, upon detection, automatically disables the user, revokes sessions, writes a firewall block, and emits both JSON and human-readable alerts with a timeline. Provide an executable CLI to replay logs from files for testing and a stateful deduplication mechanism with auto-expiry of blocks.', NULL, ARRAY['web', 'testing']),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Alerting & Incident Response Automation', 'Build a lightweight fail2ban-style responder that tails a custom service log, detects credential-stuffing/401-flood patterns via a regex filter, and posts JSON alerts to a local webhook endpoint. On threshold breach, automatically quarantine offending IPs by appending to a persistent blocklist enforced by a provided mock-firewall script, with tests that simulate attacks and benign traffic to verify correct alerting and no false positives.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Alerting & Incident Response Automation', 'Configure auditd to log reads of files under /app/secrets and any subsequent outbound network connection by the same PID, then build a responder that correlates events within 10 seconds, quarantines the executable and blocks its egress via nftables. The responder must also POST a JSON alert to a local webhook and write a detailed incident record to /app/incidents/incident-*.json.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Alerting & Incident Response Automation', 'Implement a real-time incident responder that tails auth and HTTP logs, loads YAML rule files defining thresholds over sliding windows, and on matches emits HMAC-signed JSON alerts while executing mapped actions (append IP with TTL to /app/blocked_ips, lock a user, or disable a service) and producing a triage bundle with relevant logs, process tree, and socket info. Validation simulates brute-force SSH and repeated 403s, expecting alert files in /app/alerts, auto-expiring IP entries, and timestamped incident report directories with captured evidence.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Audit Logging & Monitoring Setup', 'Configure Linux auditd to record execve events for privileged commands and writes under /etc, enable persistent journald storage, and set up rsyslog (via imjournal) to forward auth/kern/daemon logs to a local RELP+TLS syslog listener with certificate pinning. Verify by triggering sudo, user management, and config file edits, then assert the events appear in ausearch and in the forwarded stream with correct timestamps, host metadata, and persistence across service restarts.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Audit Logging & Monitoring Setup', 'Configure auditd and systemd-journald for persistent, immutable auditing of privileged execve events and file writes under /app/secret, and set up rsyslog to forward audit and authpriv logs over TLS to a provided local syslog endpoint. Trigger test events and produce /app/output/verification.txt proving capture locally and remotely, and that rules persist after service and system restarts.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Audit Logging & Monitoring Setup', 'Configure persistent journald storage and auditd rules to record execve, sudo usage, writes to /etc/ssh, and kernel module loads with distinct AUDIT_KEY tags. Bridge audit logs into journald and forward them via systemd-journal-upload to a local journal-remote collector, then trigger each event and verify receipt with expected fields and counts.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Audit Logging & Monitoring Setup', 'Configure persistent system-wide logging by enabling systemd-journald persistence and auditd with rules to track /etc changes, kernel module loads, and executions of privileged binaries, then forward events via rsyslog over TLS to a local mock SIEM listener. Validate by triggering sample events and confirming they appear in persistent logs, are rotated with a 7‑day retention policy, and are received by the SIEM with hostname and UTC timestamps.', NULL, ARRAY['logging']),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Audit Logging & Monitoring Setup', 'Enable persistent journald storage and configure auditd rules to log all execve by uid 0, any chmod that adds setuid/setgid bits, and writes under /app/secure. Validate by triggering events and generate /app/audit_report.json summarizing counts and the latest five matching events from both audit logs and the journal.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Security Policy Compliance', 'Build a CLI tool that audits a Linux host’s PAM and OpenSSH configurations against selected CIS Level 1 controls (e.g., password length/history, account lockout, root login, approved ciphers/MACs), performs safe in-place remediations to achieve compliance, and generates a machine-readable before/after compliance report. Tests validate by parsing system configs and the emitted JSON to confirm each control’s state transitioned to compliant.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Security Policy Compliance', 'Build a lightweight compliance tool that gathers Linux system facts to JSON and evaluates them with OPA/Rego policies implementing a subset of CIS Linux controls, emitting both JUnit XML and a human-readable report in /app/report. Provide idempotent remediation scripts for failing checks (e.g., SSH root login, file permissions, password aging), re-run to reach at least 90% passing, and archive before/after evidence.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Security Policy Compliance', 'Build a terminal-based compliance scanner/remediator that evaluates a Debian/Ubuntu image against a small set of CIS Level 1 controls (SSH configuration, password/PAM policy, world-writable files, and root PATH safety), emitting a deterministic JSON report to /app/output/report.json. When invoked with --fix, apply compliant changes idempotently and record a rollback plan at /app/output/rollback.sh.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Security Policy Compliance', 'Create a policy-as-code CLI that loads YAML control definitions aligned to NIST 800-53/CIS (each with check commands, expected outcomes, and remediation steps), executes them to produce a JSON compliance report with evidence artifacts, and optionally applies fixes via --enforce. Validation targets include SSH root login policy, PAM password complexity, permissions on /etc/passwd and /etc/shadow, and ensuring auditd is enabled and persistent.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Security Policy Compliance', 'Implement a Python CLI that validates a minimal CIS Ubuntu Server baseline by auditing password/PAM policy, SSH hardening, key sysctl settings, and world-writable permissions, emitting a structured JSON pass/fail report with remediation guidance. Provide an idempotent --fix mode that backs up affected files and applies compliant configurations where safe.', 'hard', ARRAY['python']),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Vulnerability Scanning & Reporting', 'Build a CLI that generates an SBOM for a local project, scans it with Trivy and Grype in offline mode, de-duplicates CVEs, and emits both SARIF and an HTML dashboard. Enforce a YAML-defined policy to suppress known issues and fail on configurable severity thresholds, including recommended fixed versions and dependency paths in the report.', NULL, NULL),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Vulnerability Scanning & Reporting', 'Create a CLI that scans a provided Docker image and its source tree with both Trivy and Grype, merges and deduplicates CVE findings, and emits SARIF plus a concise markdown summary with severity counts and fix-available mapping before/after remediation. Update the Dockerfile and dependency manifests with minimal pinned upgrades to reduce all Critical findings with fixes to zero, then prove compliance by rescanning and writing pass to /app/gate.status.', NULL, ARRAY['docker']),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Vulnerability Scanning & Reporting', 'Scan a provided Docker image with Trivy, output a CycloneDX SBOM (/app/report/sbom.json) and SARIF findings (/app/report/trivy.sarif), then refactor the Dockerfile and dependency pins to remediate all HIGH/CRITICAL vulnerabilities. Add a .trivyignore entry for a documented vendor-backported CVE, rebuild and rescan to verify zero remaining HIGH/CRITICAL except the ignored CVE, and write a concise remediation summary to /app/report/summary.txt.', NULL, ARRAY['docker']),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Vulnerability Scanning & Reporting', 'Scan a provided container image and its running service with Trivy and nmap to generate a consolidated SARIF report, then refactor the Dockerfile and service configuration to remediate all Critical findings and close an inadvertently exposed admin port. Rebuild and re-scan to verify the report shows zero Critical/High CVEs and the admin port is unreachable, saving before/after reports in /app/reports.', NULL, ARRAY['container']),
('Security & Cryptography', 'Security Monitoring & Compliance', 'Vulnerability Scanning & Reporting', 'Scan the provided container image tarball at /app/images/webapp.tar using Trivy, output a SARIF report and a CSV listing only fixable High/Critical CVEs sorted by CVSS. Then create /app/output/Dockerfile.patched that updates the base image and pinned packages to remediate all fixable findings without altering the container’s entrypoint or command.', NULL, ARRAY['container']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Common Vulnerability Reproduction', 'Exploit JWT algorithm confusion in a Node.js API that mistakenly accepts HS256 for tokens intended to be RS256 by using the server’s public key as the HMAC secret to forge an admin token and retrieve the admin-only secret to /app/secret.txt. Then harden the authentication middleware to strictly enforce RS256 with the correct key material while preserving normal login behavior.', 'hard', ARRAY['api']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Common Vulnerability Reproduction', 'Investigate a C logging daemon with a format-string vulnerability in printf that allows leaking stack memory; craft an input to extract FLAG{...} and write it to /app/flag.txt, then patch the code to use fixed format strings and recompile, confirming normal logging still works.', NULL, ARRAY['logging']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Common Vulnerability Reproduction', 'Reproduce an XML External Entity (XXE) vulnerability in a service that parses uploaded XML files by crafting a payload to read /app/flag.txt and save the contents to /app/leak.txt. Then harden the XML parser to disable external entities and DTDs while preserving normal document handling.', 'hard', NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Common Vulnerability Reproduction', 'Reproduce an unsafe deserialization flaw in a Python CLI that loads untrusted YAML using yaml.load by crafting a payload that triggers benign code execution (e.g., creating /app/pwned.txt). Then harden the program to use a safe loader while preserving valid configuration behavior and verify the exploit is no longer effective.', 'hard', ARRAY['python']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Common Vulnerability Reproduction', 'Reproduce and mitigate a ZIP Slip directory traversal in a Python CLI that naively extracts user-submitted archives. Craft a malicious ZIP to write a marker file outside the intended directory, then patch the extractor to sanitize paths so extraction is safe and idempotent.', NULL, ARRAY['python']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Exploit Development & Analysis', 'Analyze a custom C-based CLI notes service with a heap use-after-free bug and develop a minimal exploit that achieves arbitrary read to extract a secret from /app/secret.bin without crashing the process. Then implement and rebuild a hardening patch (e.g., pointer invalidation and allocator checks) that eliminates the bug while preserving normal functionality.', 'hard', NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Exploit Development & Analysis', 'Analyze a hardened 64-bit ELF service to locate a format-string vulnerability, then craft a multi-stage exploit that leaks stack/libc addresses and overwrites a GOT entry to redirect execution to a hidden print_secret() routine. Write the recovered secret to /app/flag.txt after successful exploitation.', 'hard', NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Exploit Development & Analysis', 'Develop a PoC exploit against a sandboxed 64-bit Linux ELF service with a format-string bug in its logging path: leak the stack canary and libc base, then craft a ret2libc/ROP payload that creates three marker files to prove control. Provide an automated script that launches the target, derives offsets under ASLR with NX enabled, delivers the exploit reliably, and verifies the markers.', NULL, ARRAY['logging']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Exploit Development & Analysis', 'Develop a Python exploit that performs a CBC padding oracle attack against a provided local HTTP service to recover the plaintext of an encrypted session token and forge a valid admin token to access a protected endpoint. Include a concise analysis of the oracle behavior and implement a mitigation that removes the side-channel while preserving normal application behavior.', NULL, ARRAY['python']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Exploit Development & Analysis', 'Develop a proof-of-concept that exploits JWT algorithm confusion in a local API by re-signing an RS256 token as HS256 using the service’s RSA public key to forge an admin token and trigger a privileged action that writes /app/pwned.txt. Add brief inline analysis of the root cause and mitigations (algorithm whitelisting and key-type separation).', NULL, ARRAY['api']),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Patch Verification & Mitigation Testing', 'Build a deliberately vulnerable C binary with a stack buffer overflow, exploit it to achieve code execution on an unprotected build, then rebuild with hardening (ASLR, NX, stack canaries, PIE, full RELRO) enabled. Programmatically verify mitigations via ELF inspection and /proc/sys checks, rerun the exploit to confirm it fails, and output a concise pass/fail report.', 'hard', NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Patch Verification & Mitigation Testing', 'Build a deliberately vulnerable C service and an automated harness that compiles it twice (unhardened vs hardened with stack canaries, NX, PIE, full RELRO, and FORTIFY), verifies each mitigation via readelf/sysctl, and runs a supplied overflow input. The report must show the exploit succeeds only on the unhardened build while the hardened build safely aborts, failing the task if any mitigation is missing or ineffective.', 'hard', NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Patch Verification & Mitigation Testing', 'Compile and run a vulnerable C program that attempts to execute stack-injected shellcode and a companion probe that records memory addresses across multiple executions. Verify NX and ASLR are effective by confirming the shellcode attempt crashes without code execution and that address layouts vary significantly across ≥20 runs, then output a machine-readable mitigation_report.json in /app.', NULL, NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Patch Verification & Mitigation Testing', 'Create an automated harness that compiles a deliberately vulnerable C program under four builds (no mitigations, stack canary, NX/DEP, PIE+ASLR), runs a provided overflow exploit, and verifies exploitation only succeeds when the relevant mitigations are absent. The script must also assert mitigation presence via binary introspection (e.g., readelf/objdump symbols, GNU_STACK, PIE) and system checks (/proc/sys/kernel/randomize_va_space), outputting a machine-readable report of flags detected and exploit outcomes.', NULL, NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Patch Verification & Mitigation Testing', 'Rebuild a provided overflow-vulnerable C service with hardening (stack canaries, PIE, full RELRO, NX) and enable ASLR, then verify each mitigation via ELF/Procfs inspection. Demonstrate that the supplied working exploit succeeds pre-harden and reliably fails post-harden, recording evidence to a verification file.', 'hard', NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Privilege Escalation Demonstrations', 'Exploit a misconfigured sudoers rule granting NOPASSWD access to a root-run backup helper that invokes tar over a user-writable directory with unquoted wildcards. Discover the entry and escalate via tar option/wildcard injection to execute code as root, proving success by creating /root/pe_proof.txt.', NULL, NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Privilege Escalation Demonstrations', 'Exploit a root-run backup script that calls utilities without absolute paths and executes in a user-writable directory by performing a PATH hijack to gain root and prove it by creating /root/owned.txt. Then remediate by hardcoding absolute paths and fixing permissions so the backup still works but the escalation is no longer possible.', 'hard', NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Privilege Escalation Demonstrations', 'Exploit a root-run cron backup script that invokes commands via an insecure PATH and a world-writable /usr/local/bin by dropping a malicious shim to hijack execution and gain root. Read /root/flag.txt and write a byte-identical copy to /app/flag.txt.', NULL, NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Privilege Escalation Demonstrations', 'Exploit a sudoers misconfiguration that permits running tar as root to achieve privilege escalation by abusing tar’s extended features to execute a root-owned payload, then prove success by creating /root/pe_proof.txt and exporting a protected file to /app/shadow_copy. The task includes a decoy backup workflow and must be completed without editing sudoers or using external tools.', NULL, NULL),
('Security & Cryptography', 'Vulnerability Demonstration & Exploitation', 'Privilege Escalation Demonstrations', 'Starting as an unprivileged user, exploit a sudoers misconfiguration that permits passwordless tar to escalate via --checkpoint-action=exec (or crafted wildcard injection), then prove root by creating /root/owned.txt. Mitigate by tightening the sudoers rule to a safe, argument-pinned form so the exploit is no longer possible.', NULL, NULL),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Architecture & Modularization', 'Refactor a monolithic Python CLI in /app/tool.py into a modular package with a plugin architecture, splitting core parsing, IO, and business logic into reusable modules and dynamically discovering subcommands from a plugins/ namespace (or entry points) while preserving the exact CLI and outputs. Add a thin library API, dependency inversion for side effects, and an environment variable to control plugin loading for deterministic runs.', NULL, ARRAY['python', 'api']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Architecture & Modularization', 'Refactor a monolithic Python log-ingestion CLI into a modular package with layered components (parsers, processors, sinks) and a plugin system that discovers extensions via entry points. Preserve the original CLI flags and output behavior while enabling backend swapping through configuration and dependency injection.', NULL, ARRAY['python', 'backend']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Architecture & Modularization', 'Refactor a single-file Python data-cleaning CLI at /app/monolith.py into a modular package with a core library, a thin CLI, and a plugin system using setuptools entry points for pluggable transforms while preserving identical CLI behavior and outputs. Include an in-tree example plugin and ensure both the CLI and importable API use the same core logic verified by tests.', NULL, ARRAY['python', 'api']);
-- Compact batch 24/29: rows 1151-1200

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Code Cleanup & Refactoring', 'Refactor a 500-line legacy Bash deployment script with duplicated logic, unquoted expansions, and ad-hoc temp files into a maintainable POSIX-compliant script: extract reusable functions, enable strict mode (set -euo pipefail), add trap-based cleanup, and make it shellcheck-clean while preserving exact outputs and exit codes. Tests verify byte-for-byte identical stdout/stderr across scenarios, idempotent reruns, and no subshell-induced environment leaks.', NULL, NULL),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Code Cleanup & Refactoring', 'Refactor a legacy Node.js CLI tool that uses nested callbacks and scattered file I/O into a modular ES module codebase with async/await, shared utilities, and centralized error handling while preserving the exact CLI interface, exit codes, and stdout/stderr formatting. Remove dead code and duplicate logic, clarifying separation of concerns without altering observable behavior.', NULL, NULL),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Code Cleanup & Refactoring', 'Refactor a legacy Node.js Express API that mixes routing, business logic, and DB calls into a layered modules structure (routes/controllers/services/dao), converting callback-style code to async/await without changing endpoints, status codes, or JSON schemas. Enforce ESLint (Airbnb) and Prettier formatting, add JSDoc typedefs for public functions, and ensure existing black-box tests pass unchanged.', NULL, ARRAY['api']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Code Cleanup & Refactoring', 'Refactor a small C CLI utility currently implemented with extensive macros and global state into a modular architecture using opaque structs, function pointers, and .c/.h separation, preserving identical CLI behavior and output. Replace unsafe string handling with bounded APIs, remove dead code, add const-correctness, and keep -O2 performance within 5% of the original.', NULL, ARRAY['performance']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Code Cleanup & Refactoring', 'Refactor a sprawling monolithic Makefile in /app/Makefile into a maintainable build system using pattern rules, variables, and included fragments, preserving all target names, dependencies, and artifacts. Serial and parallel builds (-j) must remain deterministic and identical to the original, with the default target and CLI UX unchanged.', NULL, ARRAY['parallel']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Dependency Modernization', 'Migrate a FastAPI service from FastAPI 0.9x and pydantic v1.x to FastAPI 0.110+ and pydantic v2.x, updating models, validators, settings, and serialization (parse_obj -> model_validate, .dict()/.json() -> model_dump()/model_dump_json()). Preserve all route signatures, OpenAPI schema, and response payloads so the existing tests (including schema snapshots) pass unchanged.', NULL, NULL),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Dependency Modernization', 'Upgrade a FastAPI microservice from Pydantic v1.x and FastAPI <0.100 to Pydantic v2.x and the corresponding FastAPI release, refactoring models, validators, and response models to the new API (or applying the pydantic.v1 shim) while preserving the external HTTP API and OpenAPI schema. Update dependency pins and build config so the existing tests pass unchanged and runtime behavior remains stable.', NULL, ARRAY['api']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Dependency Modernization', 'Upgrade a Python data pipeline from NumPy 1.x/Pandas 1.x to NumPy 2.x/Pandas 2.x, replacing deprecated APIs (e.g., dtype aliases, numpy.matrix, pandas time zone handling, integer NA) and migrating packaging to PEP 517/621. Preserve identical CLI behavior and produce byte-for-byte identical CSV outputs across provided fixtures and hidden tests.', NULL, ARRAY['python', 'pandas']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Dependency Modernization', 'Upgrade a legacy Django 1.11 project to Django 4.2 LTS by replacing deprecated APIs (url() → path/re_path, MIDDLEWARE_CLASSES → MIDDLEWARE, ugettext → gettext), updating settings, and adjusting project/app structure as needed. The application must start successfully, apply migrations cleanly, and pass the existing tests without behavior changes.', NULL, NULL),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Dependency Modernization', 'Upgrade a legacy Spring Boot 1.5.x Maven service to Spring Boot 3.3.x, migrating javax.* to jakarta.*, updating Spring Security to 6.x and JUnit 4→5. Preserve public REST endpoints and config semantics; build with Java 17+, pass mvn verify, and keep /health responding on localhost:8080.', NULL, ARRAY['security', 'rest', 'java']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Performance Profiling & Optimization', 'Profile a Go CLI that aggregates per-key metrics from a large NDJSON stream but suffers from excessive allocations, JSON decoding hotspots, and goroutine leaks. Use pprof and benchmarks to identify bottlenecks, then refactor to a streaming, buffered design (pre-sized maps, sync.Pool, proper backpressure) to cut memory churn and achieve at least 2× throughput without changing outputs.', NULL, NULL),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Performance Profiling & Optimization', 'Profile a Python CLI that computes PageRank on a large graph using pure Python lists and nested loops, identifying CPU and memory hot spots. Refactor it to use efficient sparse matrix operations and streaming I/O to achieve at least a 5× speedup while preserving identical CLI behavior and output.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Performance Profiling & Optimization', 'Profile a Python CSV-to-JSON ETL script that processes 1M rows and exhibits long runtimes and high peak memory. Using cProfile and memory_profiler, optimize hot paths (vectorize operations, stream rows, precompile regex/date parsers, eliminate temporary lists) to produce identical output with ≥3× speedup and ≥50% lower peak RSS.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Performance Profiling & Optimization', 'Profile a Python HTTP access-log analyzer that is CPU-bound and memory-heavy, then refactor it to stream input, remove regex hot spots, and parallelize safe stages to achieve at least 5× throughput with identical CLI and outputs. Add a --profile flag that emits cProfile and memory reports to /app/profile/ while preserving deterministic ordering.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Code Maintenance, Refactoring & Optimization', 'Performance Profiling & Optimization', 'Profile a Rust log aggregation tool that is slow due to repeated string allocations and O(n^2) lookups, then refactor it to use zero-copy slice parsing and hash-based indexing to reduce runtime and peak memory by at least 3x without changing output. Provide before/after flamegraphs and memory measurements to demonstrate the optimization.', NULL, ARRAY['rust', 'optimization']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Change Log & Release Notes', 'Consolidate scattered CHANGES files and an inconsistent legacy changelog into a single Keep a Changelog–compliant CHANGELOG.md, auto-inferring the next SemVer (including pre-release and breaking-change bumps) from Conventional Commits since the last stable tag. Produce release_notes_vX.Y.Z.md with compare links, grouped sections (Added/Changed/Fixed/etc.), deduplicated PR/issue references, and create an annotated git tag for the release.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Change Log & Release Notes', 'Implement a CLI for a polyglot monorepo (Node, Python, Rust) that parses Conventional Commits since each package’s last tag to determine semver bumps, update manifests, and generate per-package Keep a Changelog entries plus a consolidated top-level release note with PR links and contributor summaries. The tool must create annotated tags per package and a meta release tag, exclude chore/release commits, deduplicate co-authors, and be idempotent across repeated runs.', NULL, ARRAY['python', 'rust']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Change Log & Release Notes', 'Implement a CLI that scans a multi-package Git monorepo for Conventional Commits since the last semver tag and generates Keep a Changelog-compliant CHANGELOG.md for each package plus a root RELEASE_NOTES.md, including breaking-change footers and issue/PR links. The tool must be idempotent, correctly handle merge/revert commits, and support promoting Unreleased to a new version with --next-version/--dry-run options and creating the annotated tag.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Change Log & Release Notes', 'Implement a CLI tool that scans git commits since the last tag in a monorepo using Conventional Commits to generate per-package CHANGELOG.md files and a root RELEASE_NOTES.md, grouping entries by type (Added/Changed/Fixed) and flagging BREAKING CHANGE items with compare links. Support dry-run and release modes that update files, compute the next semantic version, and create a signed annotated tag vX.Y.Z.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Change Log & Release Notes', 'Write a POSIX-compliant script at /app/bin/release-notes that analyzes git history since the last tag using Conventional Commits to calculate the next SemVer, update CHANGELOG.md in Keep a Changelog format (rolling Unreleased → version), and output a Markdown release body with compare links. It must support monorepos (packages/*) by generating per-package sections, highlight BREAKING CHANGE footers and deprecations, and return non-zero if it encounters malformed commit messages or missing tag anchors.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Documentation & README Creation', 'Author a production-grade README.md and CONTRIBUTING.md for an existing CLI tool in /app, covering Quickstart, Configuration via environment variables, copy-pastable Usage examples, and Troubleshooting. All README code blocks must execute verbatim in the container to produce the documented outputs, and CONTRIBUTING must define branching, Conventional Commits, and local test/lint workflow.', NULL, ARRAY['container']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Documentation & README Creation', 'Author a reproducible README.md and CONTRIBUTING.md for a small CLI in /app that includes Quickstart, installation matrix, a Mermaid architecture diagram, and troubleshooting, plus a scripts/verify_snippets.py that executes every shell code fence from the README and asserts expected outputs. Provide a Makefile doc target that runs the verifier and fails on drift to keep documentation and behavior in sync.', NULL, ARRAY['installation']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Documentation & README Creation', 'Create an executable README.md and CONTRIBUTING.md for the provided multi-command Python CLI in /app/tool, including installation, feature overview, per-subcommand usage, examples, and contribution steps. The harness will run markdownlint, execute all shell code blocks in the README to verify outputs, and fail if the README’s documented flags or a generated help2man man page diverge from the program’s --help.', NULL, ARRAY['python', 'installation']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Documentation & README Creation', 'Given a small CLI project in /app, write a production-grade README.md and CONTRIBUTING.md that include installation, usage (auto-synced from the tool’s --help), examples, troubleshooting, and licensing. Add an offline `make docs` workflow that verifies all README code blocks execute, checks links, and builds a static docs/ site artifact, failing the build if examples or links are stale.', NULL, ARRAY['installation']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Documentation & README Creation', 'Produce a contributor-focused documentation suite for a small monorepo (Python FastAPI service + Node/React client): a top-level README with Mermaid architecture, quickstart via Docker Compose, a troubleshooting matrix, and performance benchmarking guidelines, plus CONTRIBUTING.md with branch strategy, Conventional Commits, code style, test commands, and a PR checklist. Generate a docs site (MkDocs Material) that auto-builds API references from Python docstrings (pdoc) and JSDoc (TypeDoc) and add a CI check that fails on broken links and incomplete sections.', NULL, ARRAY['python', 'docker', 'performance', 'api']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Project Management & Issue Tracking', 'Build a repo-local CLI that scans the codebase for TODO/FIXME annotations and migrates them into GitHub Issues via gh against a local dummy remote, assigning owners from CODEOWNERS, applying labels/milestones from inline tags, and attaching file-line permalinks. The tool must be idempotent (update instead of duplicate), aggregate related comments into checklist issues, and auto-close issues when the corresponding comments are removed.', NULL, NULL),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Project Management & Issue Tracking', 'Build an offline Git-based issue workflow: given a repo with commits/branches and a backlog.csv, implement a CLI that imports the backlog into issues/*.md with YAML metadata, auto-labels via rules.yaml, links duplicates, and auto-closes issues when commits reference ''fixes #ID''. The tool must generate a Kanban board HTML and a burndown.csv for a specified milestone, matching expected counts and state transitions.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Project Management & Issue Tracking', 'Create a local file-based issue tracker CLI that imports backlog.csv into /app/issues as YAML files, applies label/assignee rules from label-rules.yml, organizes milestones from milestones.yml, and de-duplicates similar issues. Implement an auto-close command that scans git commit messages for ''Fixes #ID'', updates issue states, and generates release_notes.md grouped by milestone and label.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Project Management & Issue Tracking', 'Implement a CLI tool that ingests a YAML/CSV backlog and uses the GitHub CLI to batch-create, label, milestone, and assign issues across multiple repositories. Configure and verify automation that links PRs, auto-closes issues on merge when commits include ''Fixes #ID'', and moves their GitHub Projects (v2) cards to Done.', NULL, NULL),
('Software Engineering & Development', 'Collaboration, Review & Documentation', 'Project Management & Issue Tracking', 'Using the distributed issue tracker git-bug, migrate a backlog exported as /app/export/issues.csv into the Git repo at /app/repo, applying label and milestone mappings from /app/policy.yml, deduplicating and linking related issues, and closing items automatically based on a test results JSON. Generate a release-notes.md grouped by milestone from the closed issues, with commit cross-references resolved to short SHAs.', NULL, ARRAY['distributed', 'git']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Environment & Configuration Fixes', 'A Node.js workspace fails to install because a native add-on (bcrypt) ships a prebuilt binary for the wrong Node ABI and node-gyp cannot find Python/build tools. Diagnose the ABI and toolchain mismatch, pin the correct Node version, configure node-gyp to use Python 3, force a from-source rebuild of the affected modules, and verify yarn install and yarn test complete successfully.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Environment & Configuration Fixes', 'Fix a C/C++ application that fails at runtime with dlopen/ld.so errors because its plugins and dependent .so files are installed under a nonstandard /app/lib path not on the dynamic loader search path. Resolve by configuring the runtime search path (e.g., rpath or LD_LIBRARY_PATH) so the correct shared library versions are found without modifying source code.', NULL, NULL),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Environment & Configuration Fixes', 'Fix a FastAPI app that crashes in the sandbox because its dependencies assume glibc wheels (e.g., uvloop/orjson) while the Docker base image is musl-based. Adjust optional deps and env flags (e.g., disable uvloop), pin compatible versions, regenerate the lockfile, and verify the server boots and passes a health check.', NULL, ARRAY['docker']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logging & Observability Enhancements', 'Enhance a Python FastAPI gateway and a background worker to emit OpenTelemetry traces, structured JSON logs, and Prometheus metrics. Propagate W3C trace-context across HTTP and Redis queue boundaries, correlate logs with trace_id/span_id and a request_id, redact PII (emails/tokens), export to a local OTLP collector, and satisfy tests that verify span topology, error statuses, and metric cardinalities.', NULL, ARRAY['python', 'redis']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logging & Observability Enhancements', 'Given a small Node.js Express + background worker app with intermittent failures, replace console logs with structured JSON logging, propagate a request/job correlation ID end-to-end, add OpenTelemetry tracing for HTTP and job processing, and expose Prometheus metrics (latency histograms, error counters) on /metrics. Implement redaction of sensitive fields and dynamic log level control via an environment variable, with a test harness that asserts logs, traces, and metrics are emitted for specified failure and success scenarios.', NULL, ARRAY['logging']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logging & Observability Enhancements', 'Instrument an existing Python FastAPI API plus a separate background worker (spawned via subprocess) with OpenTelemetry tracing and Prometheus metrics, using structured JSON logs with sensitive-field redaction and request-scoped correlation IDs. Ensure W3C trace context propagates across HTTP handlers, asyncio tasks, and the subprocess, expose /metrics, and make the harness pass by asserting expected logs, spans, error tags, and latency histograms for a failing endpoint.', NULL, ARRAY['python', 'api']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logging & Observability Enhancements', 'Instrument an existing Python Flask service and its subprocess worker with structured JSON logs and OpenTelemetry tracing that writes spans to /app/traces.json and logs to /app/logs.json without external backends. Propagate context across HTTP handlers and child processes so worker logs include trace_id, span_id, and a run_id, and provide a script that issues a request and verifies end-to-end correlation.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logic & Behavior Bugs', 'A CLI tool computes a build plan from a YAML manifest but produces an invalid, unstable order because it misinterprets before/after constraints and mishandles self-dependencies. Fix the graph construction and Kahn topological sort to respect all constraints deterministically and detect/report cycles with a clear error message.', NULL, NULL),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logic & Behavior Bugs', 'A Node.js library that claims to implement the JSON Canonicalization Scheme (RFC 8785) produces incorrect canonical strings for numbers, Unicode escapes, and object key ordering, causing digest mismatches. Identify and fix the serialization logic so SHA-256 hashes of provided fixture JSONs match the expected values and the CLI yields stable byte-for-byte canonical output.', NULL, NULL),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logic & Behavior Bugs', 'A Rust CLI reads a JSON adjacency list from stdin and prints a topological order, but the existing DFS logic produces non-deterministic results and fails to detect cycles and self-loops. Debug and replace the logic with a stable Kahn-style algorithm that deterministically orders siblings by lexical node ID and prints ''CYCLE'' with exit code 2 when a cycle is present so the supplied tests pass.', NULL, ARRAY['rust']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logic & Behavior Bugs', 'Fix a cron-like scheduler’s next_run function that produces incorrect times around DST transitions and in time zones with non-integer offsets. Correct wall-clock semantics so 02:30 jobs are correctly skipped or duplicated across transitions and match a provided reference timeline.', NULL, NULL),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Logic & Behavior Bugs', 'Fix a web framework’s URL router that incorrectly prioritizes parameterized routes over static ones, causing paths like /users/new to match /users/:id instead of /users/new. Identify the flawed matching logic and update precedence (static > param > wildcard) and trailing-slash handling so all routing tests pass.', NULL, ARRAY['web']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Runtime Error Debugging', 'A C-based CLI backup tool sporadically segfaults when the user presses Ctrl-C during a deep directory scan due to unsafe access to freed memory in a SIGINT handler. Use gdb and valgrind to isolate the use-after-free, make the signal handling async-signal-safe and fix the buffer lifetime, then add a deterministic regression test that reproduces the original crash.', NULL, NULL),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Runtime Error Debugging', 'A Python service uses a CPython C extension for 2D convolution that intermittently segfaults on specific input shapes. Reproduce the crash, use gdb or AddressSanitizer to pinpoint the out-of-bounds write from incorrect NumPy buffer stride/shape handling, and patch the C code to validate inputs and index safely so all tests pass.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Runtime Error Debugging', 'Debug a C++ network daemon that consistently segfaults on the second configuration reload (SIGHUP) due to a use-after-free of a shared configuration object. Reproduce the crash, inspect the core with gdb and run under AddressSanitizer to pinpoint the invalid access, then fix ownership/lifetime and verify that multiple consecutive reloads complete without crashes or leaks.', NULL, NULL),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Runtime Error Debugging', 'Diagnose and fix a heap-buffer-overflow that triggers a segmentation fault in a C-based PNG decoder CLI when processing a specific image. Reproduce the crash, use gdb and AddressSanitizer to pinpoint the faulty scanline index arithmetic, patch the logic, and verify decoding succeeds without memory errors.', NULL, NULL),
('Software Engineering & Development', 'Debugging & Issue Resolution', 'Runtime Error Debugging', 'Diagnose and fix an intermittent SIGSEGV in a multithreaded C++ log collector caused by a use-after-free in an async callback. Reproduce under load, pinpoint with AddressSanitizer/gdb, and patch object ownership/lifetimes to eliminate the crash and pass a stress test.', NULL, NULL),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Build Tool Configuration', 'Configure Bazel (with Bzlmod) for a polyglot monorepo (Python, TypeScript, and Go) that builds, tests, and packages each component, including a custom Starlark rule to stamp git version metadata into binaries. Ensure hermetic, reproducible builds with remote-cache fallback and a //:release target that emits signed tarballs and a CycloneDX SBOM.', NULL, ARRAY['python', 'typescript', 'git']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Build Tool Configuration', 'Configure a Bazel monorepo to build a Go CLI and a TypeScript library, vendoring dependencies (Go modules and npm) for offline, hermetic builds and running unit tests for both. Provide WORKSPACE/BUILD files so that bazel build //... and bazel test //... produce reproducible, versioned artifacts in a clean container.', NULL, ARRAY['typescript', 'container']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Build Tool Configuration', 'Configure a Bazel workspace for a polyglot monorepo (Go, Python, and TypeScript) that generates language bindings from a shared .proto, defines reproducible build/test targets, and produces versioned artifacts. Enable hermetic toolchains, a local HTTP remote cache, and build stamping from git via a small Starlark macro so incremental builds and cache hits are verifiable.', NULL, ARRAY['python', 'typescript', 'git']);
-- Compact batch 25/29: rows 1201-1250

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Build Tool Configuration', 'Configure a Meson+Ninja build for a small C library and CLI that builds shared and static variants, generates a pkg-config file, and embeds a git-derived version string. Add debug/release options toggling sanitizers, wire up unit tests, and ensure `meson dist` produces a bit-for-bit reproducible source tarball with fixed timestamps and sorted file order.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Build Tool Configuration', 'Create a CMake build for a C++ library that generates a version header from git metadata, fetches fmt via FetchContent, and installs exportable targets with both a CMake package config and a pkg-config .pc file for downstream consumption. The harness should build out-of-tree, run ctest, install to /app/dist, then compile a separate consumer using find_package or pkg-config, and verify incremental builds are no-ops and the reported version matches the latest git tag.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Continuous Integration (CI) Pipelines', 'Create a GitHub Actions workflow for a Python+Node monorepo that uses path filters to generate a dynamic job matrix, caches pip/npm, provisions a Postgres service with health checks, shards pytest across 3 parallel jobs, runs jest with coverage, and publishes a combined coverage summary. On semantic-version tags, build a multi-arch Docker image with buildx, sign it via OIDC/cosign with an attached SBOM, and upload release artifacts while skipping release steps on other pushes.', 'hard', ARRAY['python', 'parallel', 'docker']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Continuous Integration (CI) Pipelines', 'Create a GitHub Actions workflow for a monorepo that uses path filters and a matrix to run Python and Node.js tests (with a Postgres service), caching dependencies, and on failure triggers an automatic git-bisect job that posts the first bad commit as a PR comment. On tagged releases, build and push a signed multi-arch Docker image to GHCR with Buildx, generate CycloneDX SBOMs for both components, and fail if the vulnerability scan reports high-severity issues.', NULL, ARRAY['python', 'git', 'docker']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Continuous Integration (CI) Pipelines', 'Create a GitHub Actions workflow for a polyglot monorepo (server/ Python API, web/ Node frontend) that conditionally runs by path, uses a matrix, caches dependencies, and hands off built web artifacts to backend integration tests. The pipeline should start Postgres and Redis services, merge coverage to Cobertura, and on v* tags build and push a Docker image to a local registry.', NULL, ARRAY['python', 'api', 'web', 'frontend', 'backend', 'redis', 'docker']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Continuous Integration (CI) Pipelines', 'Create a reusable GitHub Actions workflow (invoked via workflow_call) that builds a dynamic matrix from changed subdirectories to lint/test polyglot components (Go, Node, Python) with dependency caching, provisions PostgreSQL and MinIO services for integration tests, and builds multi-arch Docker images via buildx. On version tags, sign the image using keyless cosign via OIDC, generate an SBOM and SLSA provenance, and publish artifacts to GitHub Releases and the container registry.', NULL, ARRAY['python', 'postgresql', 'docker', 'container']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Continuous Integration (CI) Pipelines', 'Implement a GitHub Actions workflow for a polyglot monorepo (Python package, Node library, and Go CLI) that uses path-based change detection to run only impacted jobs, caches per-language dependencies, runs a version/OS matrix, aggregates JUnit and coverage artifacts, and produces a versioned release bundle only on annotated tags. The pipeline must use reusable workflows, pinned action SHAs with actionlint validation, concurrency to cancel superseded runs, and artifact handoff between jobs without re-installing dependencies.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Developer Environment Setup', 'Create a hermetic polyglot developer environment using Nix Flakes that exposes a dev shell with pinned Python (Poetry), Rust (cargo), and Node (pnpm), plus pre-commit and reproducible caches. Provide flake.nix and a Justfile so `nix develop` drops into a non-root shell where `just lint` and `just test` execute the configured tooling.', NULL, ARRAY['python', 'rust']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Developer Environment Setup', 'Create a reproducible VS Code devcontainer for a polyglot repo (FastAPI backend + Next.js frontend) with hot-reload debugging, pre-commit hooks, and Postgres/Redis services started via docker-compose, exposing ports 8000 and 3000. Include a non-VS Code bootstrap script and health checks so the task passes only when both apps, databases, and lint hooks are active.', NULL, ARRAY['backend', 'frontend', 'debugging', 'redis', 'docker']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Developer Environment Setup', 'Create a reproducible polyglot developer environment using Nix flakes and direnv for a repo combining Python 3.11, Rust 1.75, and Node 20, with a matching Docker image generated from the same flake. Pin all toolchains, wire pre-commit hooks, and ensure `make test` runs identically inside `nix develop` and `docker run`.', NULL, ARRAY['python', 'rust', 'docker']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Developer Environment Setup', 'Set up a Nix flake-powered VS Code devcontainer that provides a hermetic polyglot toolchain (Rust, Python, Node.js, Go) with pinned versions, LSPs, and pre-commit hooks via direnv integration. Enable Docker Buildx/QEMU multi-arch and ccache/sccache so the same make bootstrap and make test targets work identically on x86_64 and arm64.', NULL, ARRAY['rust', 'python', 'docker']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Developer Environment Setup', 'Set up a VS Code Dev Container backed by Docker Compose that provisions Python 3.11 (via uv), Node.js 20, and Rust stable, with Postgres 15 and Redis 7 as sidecar services. Include bootstrap scripts and pre-commit so that entering the container pins tool versions and a single make verify command builds a sample Rust crate, runs npm install, and executes pytest integration tests that talk to the databases.', NULL, ARRAY['container', 'docker', 'python', 'rust', 'redis']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Toolchain Customization & Extensions', 'Build a custom clang-tidy module that implements a check (e.g., modernize-avoid-raw-new) to flag and auto-fix uses of new/delete by suggesting std::make_unique/make_shared, and integrate it into a CMake project with a .clang-tidy config and a script to run it on the codebase. Include unit tests for the check and ensure the plugin builds and runs entirely within the sandbox.', NULL, NULL),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Toolchain Customization & Extensions', 'Build an ESLint plugin that adds a rule ''no-unbounded-fetch'' to detect fetch/axios calls without an AbortController or timeout, including an autofix to wrap calls with a cancellable pattern. Integrate it into a sample TypeScript project via .eslintrc, npm scripts, and a pre-commit hook, with RuleTester tests covering both reports and fixes.', NULL, ARRAY['typescript']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Toolchain Customization & Extensions', 'Implement a Prettier plugin for a simple INI-like key=value DSL with sections and comments, adding a custom parser and printer that preserves comments, normalizes indentation, and alphabetically sorts keys within each section while remaining idempotent. Provide a CLI that formats all .conf files in a directory and verifies byte-for-byte equality with expected outputs.', NULL, NULL),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Toolchain Customization & Extensions', 'Implement an ESLint plugin that adds a custom rule with autofix to enforce import hygiene: group imports as [node builtins, external, internal ''@app/*'', relative], keep one blank line between groups, alphabetize within groups, and place side-effect-only imports last. Provide fixtures and a test CLI so running npm test verifies exact diagnostics pre-fix and that the autofixed output matches a golden file.', NULL, NULL),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Toolchain Customization & Extensions', 'Implement an ESLint plugin with a rule that bans direct fetch/axios usage and enforces routing HTTP calls through a project-specific client API, including a robust AST-based autofixer. Provide CLI tests that lint and auto-fix sample files to verify correct detection, messages, and transformations.', NULL, ARRAY['api']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Version Control & Branching', 'Implement and register a Git custom merge driver that performs a deterministic, key-aware three-way merge for JSON files, with .gitattributes routing *.json through it. Create two feature branches that introduce overlapping edits to the same JSON documents, merge them back to main without manual conflicts, and leave a verifiable merged state and commit history.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Version Control & Branching', 'Locate a regression by automating git bisect with a script that runs the project’s tests to identify the first bad commit, then create a hotfix branch with the minimal fix. Cherry-pick the fix onto the latest release branch and create a GPG-signed annotated patch-release tag, ensuring linear history and passing tests on both branches.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Version Control & Branching', 'Migrate a repository from using a submodule at libs/foo to using a git subtree, preserving the submodule’s commit history within libs/foo/ across main and release branches. Then remove all blobs >5MB via history rewrite, rebase feature/* branches onto the cleaned main resolving conflicts (enable rerere), and push to a bare origin with a specified branch/tag layout and verifiable commit graph with no large blobs.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Version Control & Branching', 'Migrate a repository that uses two submodules into a monorepo via git subtree, preserving complete history and retagging releases with component-prefixed tags. Then merge three diverging feature branches with an octopus merge, resolving JSON conflicts via a custom merge driver configured through .gitattributes.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Development Tooling & Workflow Automation', 'Version Control & Branching', 'Split a provided monorepo’s frontend/ and backend/ histories into two standalone repositories using git subtree split while preserving commit history and tags, then re-integrate them into the original monorepo as subtrees wired to remotes. Demonstrate a subsequent upstream sync by pulling new commits from both remotes and merging into main as fast-forwards with no content drift.', NULL, ARRAY['frontend', 'backend', 'git']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'API Design & Integration', 'Create a WebSocket-to-MQTT bridge that authenticates clients via JWT and implements a JSON protocol for subscribe, unsubscribe, and publish operations, translating them to MQTT with QoS1 and retained message support. Expose an HTTP endpoint to list active client sessions and subscriptions, and ensure clean mapping of error states between WebSocket frames and MQTT acknowledgments.', NULL, NULL),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'API Design & Integration', 'Implement a Go-based gRPC↔HTTP JSON transcoding gateway that exposes REST endpoints (including a Server-Sent Events stream) mapped to methods in a provided .proto, translating errors/metadata and supporting bidirectional streaming via WebSockets. The gateway must validate incoming JWTs using a remote JWKS (with caching/rotation) and inject verified claims into gRPC metadata for the upstream service.', NULL, ARRAY['grpc', 'rest']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'API Design & Integration', 'Implement a minimal S3-compatible object storage server that supports AWS Signature v4 auth and interoperates with aws-cli for CreateBucket/ListBuckets, PutObject/GetObject/DeleteObject, and presigned GET URLs; persist objects to /data and return AWS-style XML responses. Include ETag/If-None-Match handling and ensure requests with invalid signatures return the correct S3 error codes.', NULL, ARRAY['aws']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'API Design & Integration', 'Implement an HTTP gateway that exposes a REST+SSE API on localhost:8080 and transparently bridges to a local gRPC service at 127.0.0.1:50051, translating unary and streaming RPCs into JSON and server-sent events. Ensure consistent gRPC→HTTP error mapping, request deadlines via headers, idempotency keys for POSTs, and auto-generate an OpenAPI spec for the REST facade.', NULL, ARRAY['rest', 'api', 'grpc']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'API Design & Integration', 'Implement an HTTP-to-gRPC gateway that reads a provided OpenAPI spec and matching .proto, translating REST requests into gRPC calls (including server-streaming exposed as Server-Sent Events) and mapping errors to correct HTTP codes. The gateway must validate requests/responses against OpenAPI, support path/query/header mapping, and preserve backward compatibility for both v1 and v1beta routes.', NULL, ARRAY['grpc', 'rest']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Algorithm Implementation', 'Implement a KLL streaming quantile sketch with configurable error (epsilon) that supports insert, merge, serialize/deserialize, and percentile queries, exposing both a small library and a CLI that reads floats from stdin and returns requested quantiles. Provide deterministic tests verifying accuracy bounds against exact quantiles on synthetic and adversarial streams.', NULL, NULL),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Algorithm Implementation', 'Implement a single-pass streaming quantile estimator using the Greenwald–Khanna algorithm with guaranteed epsilon-approximate quantiles. Expose a CLI that ingests an arbitrary-length numeric stream and outputs requested quantiles deterministically for a given epsilon.', NULL, NULL),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Algorithm Implementation', 'Implement a succinct trie using the LOUDS representation with bit-level rank/select to support exact lookup, prefix enumeration, and lexicographic k-th word queries. Provide a CLI to build from a newline-delimited dictionary and answer mixed queries from stdin efficiently.', NULL, NULL),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Algorithm Implementation', 'Implement an exact 64-bit integer convolution using Number Theoretic Transforms: compute convolutions modulo multiple NTT-friendly primes and recombine via the Chinese Remainder Theorem (or Garner’s algorithm) to match Python big-integer results. Provide a CLI that reads two integer sequences from stdin and outputs their full convolution, running in O(n log n) for n up to 200,000.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Code Generation & Automation Utilities', 'Build a CLI that reads a JSON Schema (Draft 7) from /app/schema.json and generates a Rust crate at /app/gen with idiomatic serde-annotated structs/enums, correctly handling oneOf/anyOf/allOf, defaults, optional fields, and safe field/enum naming. Emit and compile a validator binary that uses the generated types to validate all /app/samples/*.json inputs and writes a summary JSON reporting per-file validity and human-readable errors.', NULL, ARRAY['rust']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Code Generation & Automation Utilities', 'Build a CLI that scans a polyglot monorepo (Python, Node.js, Go) to detect service roots and generate optimized multi-stage Dockerfiles and matching .dockerignore files using lockfiles for deterministic caching and stable layer ordering. Provide a validate subcommand that builds each image and outputs a summary of image sizes, cache efficiency, and hadolint violations.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Code Generation & Automation Utilities', 'Create a CLI tool that scans a repository for Dockerfiles and generates a GitHub Actions workflow YAML that builds and pushes a multi-architecture image matrix for each Dockerfile (respecting ARG defaults and build contexts). The tool must validate the YAML, support dry-run and overwrite modes, and output both the workflow at .github/workflows/build.yml and a machine-readable summary report.', NULL, NULL),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Code Generation & Automation Utilities', 'Implement a CLI that converts a docker-compose.yml into a Helm chart (Chart.yaml, values.yaml, and templates/*) and auto-generates a GitHub Actions workflow for chart linting and release. The tool (/app/compose2helm.py) supports multiple compose files and environment interpolation, and tests verify the rendered manifests on a sample project.', NULL, ARRAY['docker']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Code Generation & Automation Utilities', 'Implement a Python AST-based codemod (using LibCST) that migrates a codebase in /app/src from requests to httpx, updating imports, call sites, Sessions/Clients, timeouts, redirect flags, and exception types. Apply changes in-place while also writing a unified patch to /app/patch.diff and a machine-readable /app/report.json summarizing per-file edits.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Feature Extension & Enhancement', 'Extend a minimal HTTP JSON TODO service by adding optimistic concurrency via ETag/If-Match on PUT/PATCH, stable cursor-based pagination for GET /todos, and a /metrics endpoint exporting request counts and p95 latency. Ensure thread-safe handling under concurrent clients and persistence across restarts using a write-ahead log.', NULL, NULL),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Feature Extension & Enhancement', 'Extend an existing Node.js Express REST API backed by SQLite by adding optimistic concurrency (ETags/If-Match), soft deletes with a restore endpoint, and full-text search over titles/descriptions using SQLite FTS5. Introduce /search and /items/:id/restore routes, apply schema migrations, and update the OpenAPI spec to reflect the new behavior and error codes.', NULL, ARRAY['rest', 'api']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Feature Extension & Enhancement', 'Extend an existing Python CLI notes/todo app by adding a `search` subcommand backed by SQLite FTS5 that queries titles and bodies with ranked results and optional --limit/--offset/--json output. Implement a migration to create and keep the FTS index in sync, update the data layer, and add tests covering ranking, pagination, no-match behavior, and Unicode tokenization.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Feature Extension & Enhancement', 'Extend an existing Python CLI that reads NDJSON logs to support a SQL-like query engine (SELECT fields, WHERE, ORDER BY, LIMIT/OFFSET, GROUP BY with count/sum/avg) and a --follow mode that processes streaming input in constant memory. Add a plugin system for custom aggregate functions discovered at runtime and a plugins subcommand to list/validate them with robust error reporting.', NULL, ARRAY['python', 'sql']),
('Software Engineering & Development', 'Feature Implementation & Algorithm Development', 'Feature Extension & Enhancement', 'Extend the provided Python static site generator by adding an incremental build cache and a `watch` subcommand. Track per-page dependencies via content hashes (templates, partials, assets) so only affected outputs are rebuilt on change, and write a manifest proving cache hits/misses for each build.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Code Signing & Verification', 'Build a CLI tool that verifies a release artifact by validating a signed checksums manifest (detached OpenPGP signature against a pinned keyring) and, if present, a Minisign/Signify signature, confirming digests across multiple algorithms (SHA-256/512, BLAKE2b), and emitting a fail-closed JSON report with signer identities, algorithms, and timestamps. Handle revoked/expired keys, ambiguous filenames, and newline/whitespace normalization, and return non-zero on any verification failure.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Code Signing & Verification', 'Build a minimal TUF-secured release flow: generate root, targets, snapshot, and timestamp metadata with a 2-of-3 threshold for targets, sign a target artifact, and implement a client that downloads and verifies the artifact enforcing expiry and key rotation. The verifier must fail on expired metadata, insufficient signature threshold, or tampered targets, and exit 0 only when all checks pass.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Code Signing & Verification', 'Build an offline CLI that signs deployment tarballs with Ed25519 and emits a DSSE JSON attestation (including SHA-256, builder ID, and timestamp), with a keygen and public keyring export. Implement a verifier that enforces a JSON trust policy (allowed keys per release channel, expiry window, and revocation list) and fails on any tampering or policy violation.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Code Signing & Verification', 'Implement a Git pre-receive hook and verifier script that require any refs/tags/release/* update to be an annotated, GPG-signed tag by a key in .ci/trusted.gpg and that all commits reachable from the tag since the previous release are signed by trusted, non-expired, non-revoked keys. The verifier must also build a git archive of the tagged tree, compare its SHA256 to a checksums.json tracked in the tag, and output a JSON report listing verification results and any offending objects, exiting non-zero on failure.', NULL, ARRAY['git']),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Code Signing & Verification', 'Implement a pair of CLIs that produce and verify a signed release manifest: the signer walks /app/build, generates a reproducible SHA-256 manifest, wraps it in a DSSE JSON envelope, and signs it with an Ed25519 key derived from a passphrase. The verifier must validate the signature with the public key, detect tampering or missing files via the manifest, and only copy artifacts to /app/deploy upon successful verification.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Dependency & License Compliance', 'Build a CLI that ingests a polyglot repo (npm, pip, and Cargo), parses their lockfiles to map dependencies to SPDX license IDs, and emits a unified SPDX 2.3 SBOM at /app/sbom.spdx.json. Enforce a policy.yaml with allow/deny lists and per-package exceptions plus OSV vulnerability thresholds, verify each dependency’s LICENSE presence or resolvable URL, produce /app/compliance_report.json with remediation suggestions, and exit non-zero on any violation.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Dependency & License Compliance', 'Build a CLI that scans a polyglot monorepo (Python/Poetry, Node/pnpm, and Go modules), resolves all transitive dependencies, normalizes their SPDX license expressions, and enforces an allowlist/denylist with per-package exceptions. The tool must output an SPDX 2.3 SBOM and a deterministic THIRD_PARTY_NOTICES.md with license texts and source URLs, run offline from lockfiles, and ship a CI workflow that fails the build on violations.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Dependency & License Compliance', 'Create a CLI that compares two dependency lockfiles (npm, Poetry/pip-tools, and Go modules), resolves each dependency’s license (including transitives), and writes a policy-checked diff report to /app/license_diff.json. It must detect license changes, dual-licensing nuances, and missing metadata, consult OSV for disallowed packages, and exit non-zero on violations.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Dependency & License Compliance', 'Implement a CLI that audits a polyglot repo (Python/Node/Rust) by parsing poetry.lock, package-lock.json, and Cargo.lock to resolve all transitive dependencies offline, extracting SPDX license identifiers from local metadata. It must generate a CycloneDX SBOM and a consolidated THIRD_PARTY_NOTICES.txt, enforce a policy forbidding AGPL/SSPL/GPL-3.0-only and unknown licenses unless explicitly allowlisted, and exit non-zero on violations.', NULL, ARRAY['python', 'rust']);
-- Compact batch 26/29: rows 1251-1300

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Dependency & License Compliance', 'Set up a polyglot repo (Python, Node.js, Rust) with a CI-like script that generates an SPDX SBOM, scans for CVEs and license terms, enforces a policy banning copyleft licenses and high-severity vulnerabilities, and fails if violations are found. Remediate by pinning or replacing flagged dependencies, regenerate lockfiles, and produce a machine-readable compliance report under /app/compliance/report.json.', NULL, ARRAY['python', 'rust']),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Reliability & Fault Tolerance', 'Create a resilient HTTP proxy that fronts multiple flaky upstreams, implementing bounded retries with exponential backoff and jitter, a circuit breaker with half-open recovery, and request hedging; when all upstreams fail, serve stale cache entries (cache-aside) and surface structured errors. Add idempotency-key handling for POST requests and a /metrics endpoint exposing success/failure counts so tests can inject chaos and verify graceful degradation and self-healing.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Reliability & Fault Tolerance', 'Implement a Redis-backed job worker that guarantees at-least-once delivery with idempotent processing, exponential backoff with jitter, a dead-letter queue, and crash-safe recovery via a transactional outbox. Provide a chaos test harness that kills the worker mid-task and injects duplicate deliveries to verify no duplicate side effects, eventual completion, and correct quarantining of permanently failing jobs.', NULL, ARRAY['redis']),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Reliability & Fault Tolerance', 'Implement a crash-safe transactional outbox for a toy Orders service using SQLite and a background dispatcher: persist orders and outbox records atomically, then reliably deliver them to an append-only mock broker with idempotency keys, exponential backoff with jitter, and a retry budget. After simulated crashes and restarts, the system must guarantee at-least-once delivery without duplicate observable publishes and reconcile any in-flight work.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Reliability & Fault Tolerance', 'Implement a local SQS-like durable task queue with visibility timeouts, exponential backoff with jitter, idempotency-key deduplication, and a dead-letter queue; provide a worker that can be killed mid-run and must resume without double-processing. The harness injects crashes, network-like delays, and duplicates to verify eventual processing and no duplicate side effects via a persisted checksum ledger.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Reliability & Fault Tolerance', 'Implement a resilient webhook relay daemon that reads events from /app/queue.jsonl and delivers them to a flaky HTTP endpoint using capped exponential backoff with jitter, a circuit breaker (with half-open probing), and a token-bucket rate limiter. On persistent failure it must dead-letter events, serve stale-but-valid cached results, and guarantee idempotent, exactly-once visible delivery across restarts via checkpointed offsets and idempotency keys.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Secure Coding Practices', 'Audit and harden a Python backup/restore utility that creates and extracts tar archives, currently vulnerable to Zip Slip path traversal and a --post-hook command injection via shell=True. Constrain extraction to a specified base directory, sanitize tar members, replace shell execution with safe subprocess invocation and an allowlist, and add tests that confirm malicious archives cannot escape the sandbox while preserving normal behavior.', 'hard', ARRAY['python']),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Secure Coding Practices', 'Given an intentionally vulnerable Node.js microservice/CLI in /app, remediate prototype pollution in JSON merging, command injection in child_process usage, and ReDoS in a user-supplied regex. Replace unsafe patterns with secure alternatives, add regression tests, and ensure npm audit reports no high/critical issues.', NULL, NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Secure Coding Practices', 'Harden a Python FastAPI file-processing service that currently uses shell=True and trusts filenames by replacing unsafe subprocess calls with argument lists, adding strict input validation and path whitelisting to prevent command injection and directory traversal, and moving credentials out of source into an .env file with parameterized loading. Add automated tests that demonstrate exploit payloads are blocked (command injection, path traversal) while valid operations still pass.', 'hard', ARRAY['python']),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Secure Coding Practices', 'Harden a deliberately vulnerable FastAPI microservice that fetches URLs and converts images by eliminating SSRF, path traversal, and command injection—replace shell calls with safe libraries, enforce allowlists/timeouts/size limits, canonicalize paths, and validate inputs with Pydantic. Rotate any leaked secrets into environment variables, add a pre-commit secret scanner and CI SAST step, and provide tests proving malicious payloads are blocked while legitimate requests still succeed.', 'hard', NULL),
('Software Engineering & Development', 'Security, Compliance & Reliability Engineering', 'Secure Coding Practices', 'Harden a vulnerable Python Flask file-processing API by replacing unsafe YAML loading, removing shell=True subprocess usage to prevent command injection, and blocking path traversal/symlink attacks on uploads. Add tests proving malicious payloads are neutralized while preserving intended functionality.', 'hard', ARRAY['python', 'api']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Codebase Architecture Documentation', 'Build a CLI that analyzes a polyglot monorepo (Python, Go, Node) to generate a cross-language dependency graph and a C4 Container diagram, writing docs/architecture.md and SVG diagrams with components annotated by primary git authorship and service boundaries. Include a CI mode that fails on new dependency cycles or components missing ADR links, and provide a deterministic make target to regenerate all artifacts.', NULL, ARRAY['python', 'container', 'git']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Codebase Architecture Documentation', 'Build a CLI that scans a polyglot monorepo (Python, Go, Node.js) and its docker-compose.yml to produce C4 Container/Component diagrams, a module import matrix, and a cross-service topology, exporting PlantUML/SVG and Markdown docs. The tool must enforce layer rules from a config by failing on forbidden imports and append an ADR entry whenever dependency topology changes are detected.', NULL, ARRAY['python', 'docker', 'container']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Codebase Architecture Documentation', 'Create a script that scans a polyglot repo (Python and Node) to generate /app/docs containing a C4-style container diagram inferred from docker-compose.yml, per-service module dependency graphs, and a consolidated architecture README. The script must be idempotent, detect and list circular dependencies, and output diagrams in both Mermaid and PlantUML formats.', NULL, ARRAY['python', 'container', 'docker']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Codebase Architecture Documentation', 'Given a polyglot microservices monorepo with docker-compose, build a tool that parses source imports and compose files to generate a cross-service architecture diagram and per-language package dependency graph, emitting Graphviz artifacts and a Markdown overview. The tool must detect cycles and boundary violations and fail CI when found, with tests verifying required nodes/edges and report contents.', NULL, ARRAY['docker']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Design Pattern Implementation', 'Build a Unix-style streaming log processor CLI where pipeline stages share a common Stream interface and are composed using Chain of Responsibility, with cross-cutting concerns (timing, retries, metrics) applied via Decorator. Support dynamic stage discovery and instantiation through an Abstract Factory that loads plugins from a plugins/ directory and a config file describing the pipeline.', NULL, NULL),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Design Pattern Implementation', 'Implement a Python CLI task runner that uses the Command pattern for actions, Memento for undo/redo, and an Abstract Factory to load pluggable commands from /app/commands at runtime. The tool must support run, undo, redo, and macro commands (Composite) without modifying the core engine when new commands are added.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Design Pattern Implementation', 'Implement a mini rule-engine that parses and evaluates a boolean filtering DSL over JSON records using the Interpreter and Composite patterns. Provide a CLI that reads NDJSON from stdin, prints matching lines, and supports adding new operators via pluggable terminals without modifying the core parser.', NULL, NULL),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Design Pattern Implementation', 'Refactor a monolithic log-processing tool into a pluggable pipeline that uses Chain of Responsibility for filter stages, Strategy for parse/format variants, and an Abstract Factory to construct pipelines from a YAML config. Discover processors from /app/plugins without modifying core code and verify correctness by producing byte-identical outputs on provided fixtures before and after the refactor.', NULL, NULL),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Design Pattern Implementation', 'Refactor a provided Python CLI to-do app into a modular design that applies Command (with undo/redo), Strategy (pluggable storage backends: in-memory, JSON file, SQLite), and Observer (event hooks for sync/logging). Implement dynamic plugin discovery for new Strategy backends and a test suite validating persistence, undo/redo semantics across restarts, and event notifications.', NULL, ARRAY['python', 'logging']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Refactoring for Maintainability', 'Refactor a Node.js Express REST API that uses module-scoped singletons and inline SQL into a layered design (controllers/services/repositories) with dependency injection and repository interfaces so side effects are isolated and components are easily testable. Preserve all route paths and JSON responses exactly, and ensure the existing test suite passes unchanged.', NULL, ARRAY['rest', 'api', 'sql']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Refactoring for Maintainability', 'Refactor a Python Flask service in /app/app.py that currently intertwines routing, SQL queries, and business rules into a hexagonal architecture: extract domain logic into /app/core with ports, implement adapters for HTTP and SQLite, and wire dependencies at a composition root. Preserve all HTTP routes and JSON schemas exactly; tests will assert identical responses while enabling unit tests of the core via an in-memory adapter.', NULL, ARRAY['python', 'sql']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Refactoring for Maintainability', 'Refactor a monolithic Go HTTP service that directly accesses PostgreSQL and global singletons into a hexagonal (ports/adapters) architecture with domain, application, and infrastructure layers, explicit interfaces, and constructor-based dependency injection. Preserve all HTTP routes, response schemas, environment variables, and logging formats, and add golden tests to verify identical behavior before and after.', NULL, ARRAY['postgresql', 'logging']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'Refactoring for Maintainability', 'Refactor a small Flask microservice in /app to a ports-and-adapters (hexagonal) architecture by extracting domain logic into pure modules, introducing repository/service interfaces, and isolating framework/infrastructure behind adapters. Preserve the exact REST API and CLI behavior and response formats while removing global state and adding dependency injection seams for testability.', NULL, ARRAY['rest', 'api']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'System & Module Design', 'Design a hexagonal architecture (ports and adapters) for a feature-flag evaluation service. Define the domain module and interfaces (FlagStore, TargetingRuleEngine, Evaluator), implement separate adapters for JSON-file persistence and an HTTP query endpoint, and a composition root that wires dependencies while keeping the domain free of I/O or framework dependencies.', NULL, NULL),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'System & Module Design', 'Design a minimal CQRS + event-sourcing Todo service with strict module boundaries: define interfaces for CommandBus, EventStore, AggregateRoot, Projector, and ReadModelRepository, and implement swappable in-memory and SQLite adapters via dependency injection. Expose a thin write-only REST for commands and a separate read-model API, with contract tests that validate the command→event→projection flow and enforce no cross-dependencies between write and read modules.', NULL, ARRAY['rest', 'api']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'System & Module Design', 'Design a pluggable feature-flag evaluation engine with clean module boundaries: Strategy (targeting/rollout), Store (flag/state persistence), and AuditSink (decision logging) wired via dependency inversion. Define interfaces and data flow, load implementations from a TOML config at runtime, and provide a CLI that evaluates flags for user contexts without the core depending on concrete classes.', NULL, ARRAY['logging']),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'System & Module Design', 'Design a plugin-based data pipeline with strict layering (domain: Stage/DAG interfaces, application: orchestrator, infrastructure: adapters) where stages are discovered dynamically and wired from a YAML-defined DAG. Implement CSVReader, Filter, Aggregate, and JSONWriter stages and provide a static dependency check that fails if any stage module imports the orchestrator or adapters.', NULL, NULL),
('Software Engineering & Development', 'Software Architecture & Design Patterns', 'System & Module Design', 'Design and implement a hexagonal architecture for an inventory reservations system: define domain entities and ports for commands, queries, time, and event publishing, keeping the core framework-agnostic. Provide interchangeable adapters for persistence (SQLite and in-memory) and I/O (HTTP and CLI) wired via a lightweight DI container so tests can swap adapters without modifying the domain.', NULL, ARRAY['container']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Continuous Testing Integration', 'Configure a GitHub Actions workflow runnable locally via act that builds a Python project, starts a Postgres service with docker-compose, and runs tox-based unit, integration, type (mypy), and security (bandit) tests across a Python version matrix with pip caching, sharded pytest execution, and flaky-test retries while enforcing a 90% coverage gate. The pipeline must upload JUnit XML and coverage reports as artifacts to /app/ci_artifacts and pass end-to-end locally.', 'hard', ARRAY['python', 'docker', 'security']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Continuous Testing Integration', 'Create a GitHub Actions pipeline for a polyglot monorepo (Python, Node.js, and Go) that runs unit and Dockerized integration tests in a matrix, spins up Postgres/Redis services, caches dependencies, splits and retries tests, enforces per-language coverage thresholds, and uploads JUnit/Cobertura artifacts. Add path-based change filters to skip unaffected jobs, lock down secrets, require status checks, and schedule a nightly run that aggregates flaky-test statistics.', NULL, ARRAY['python', 'redis']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Continuous Testing Integration', 'Create a GitHub Actions workflow for a polyglot monorepo (Python, Node.js, and Go) that caches dependencies, runs unit and integration tests (integration via docker-compose Postgres), aggregates JUnit and coverage across languages into a single report, and fails if combined coverage is below 85% while uploading artifacts. Implement a sharding-and-retry runner that splits slow tests across two matrix shards and retries failures once, marking the job as unstable via an output file when a retry passes.', 'hard', ARRAY['python', 'docker']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Continuous Testing Integration', 'Implement a local CI harness that reads a ci.yml, spins up required services via Docker Compose (e.g., PostgreSQL), shards tests across N parallel workers for Python and Node subprojects, and retries failures once to classify flakiness. The pipeline must produce merged JUnit XML and coverage reports, enforce coverage thresholds during the build stage, and output a summary.json with pass/fail, flake counts, and artifacts paths.', 'hard', ARRAY['docker', 'postgresql', 'parallel', 'python']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Continuous Testing Integration', 'Integrate mutation testing and coverage gating into a tox-driven pipeline that runs unit and integration tests across Python 3.10–3.12, emitting JUnit and coverage XML to /app/artifacts and merging per-environment results. Implement path-aware test selection and dependency caching so only affected tests run on changes, and fail the pipeline if mutation score < 85% or coverage < 90%.', NULL, ARRAY['testing', 'python']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'End-to-End (E2E) & Regression Testing', 'Build a Dockerized E2E and regression test harness that launches a three-service sample app (frontend, API, Postgres) via docker-compose, seeds test data, and runs headless Playwright tests against localhost. Include golden JSON and visual snapshot assertions, collect screenshots/HAR/logs on failure, and provide a single CI-friendly script that exits nonzero on any diff.', NULL, ARRAY['frontend', 'api', 'docker']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'End-to-End (E2E) & Regression Testing', 'Create a migration regression harness that spins up a temporary PostgreSQL instance, applies upgrade/downgrade paths across a set of SQL migrations, seeds data, and verifies invariants by comparing normalized query results and schema digests to golden snapshots. Include a CLI to run the full matrix (all from→to versions), detect drift and irreversibility, and optionally update snapshots.', NULL, ARRAY['postgresql', 'sql']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'End-to-End (E2E) & Regression Testing', 'Create a regression harness that runs the same Playwright E2E suite against two app versions (baseline vs candidate) via docker-compose, capturing API response snapshots and page screenshots, then producing semantic diffs and visual diffs in /app/artifacts. Implement retry-based flake detection with automatic quarantine list generation and emit a JUnit XML summary suitable for CI.', NULL, ARRAY['docker', 'api']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'End-to-End (E2E) & Regression Testing', 'Design an automated cross-version E2E regression harness that spins up vN and vN-1 of a sample service (HTTP + gRPC) with docker-compose, replays recorded interactions, and validates API responses, protobuf/JSON schemas, DB state, and log invariants. Integrate OpenAPI/Buf breaking-change checks and emit JUnit and HTML reports to fail the pipeline on incompatibilities.', NULL, ARRAY['grpc', 'docker', 'api']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'End-to-End (E2E) & Regression Testing', 'Implement an E2E regression harness that launches two Dockerized versions (v1 and v2) of a FastAPI service against a fresh Postgres, applies Alembic migrations, seeds fixtures, replays a recorded HTTP trace, and diffs normalized JSON responses to detect behavior drift. The runner must produce JUnit XML and a readable diff, ignoring nondeterministic fields (ids/timestamps/order), and exit nonzero on any semantic mismatch.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Mocking & Test Data Simulation', 'Build a cassette-driven GraphQL mock server that captures live queries/responses, redacts secrets, and replays them offline with deterministic seeding. Include a CLI to inject schema drift, latency/jitter, and controlled error rates so client tests can verify resilience and schema-compatibility via golden snapshots.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Mocking & Test Data Simulation', 'Build a minimal mock OpenID Connect provider that serves a .well-known discovery document and a rotating RS256 JWKS, and generates signed ID tokens for a set of fixture users. Include a verifier script that fetches the JWKS over HTTP and validates tokens before and after a key rotation, ensuring expired/old-key tokens fail and newly issued tokens pass.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Mocking & Test Data Simulation', 'Create a spec-driven mock HTTP service that reads an OpenAPI 3.0 spec from /app/spec.yaml and deterministically generates synthetic responses from schema examples and constraints, with toggles for error injection, pagination, and rate limits. Include fixtures and a test harness that validate a client’s handling of boundary values, idempotency keys, and retry/backoff behavior using only the mock service.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Mocking & Test Data Simulation', 'Implement a local S3-compatible mock server that supports CreateBucket, PutObject, GetObject, and ListObjects while simulating eventual consistency and fault injection (configurable latency, transient 500s, and delayed listings). Provide fixtures that generate synthetic objects with edge-case keys and varied payload sizes, and emit structured logs to validate client retry/backoff and read-after-write behavior.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Mocking & Test Data Simulation', 'Implement a local mock payment gateway and webhook relay that mimics Stripe-style semantics: create/confirm payment intents, idempotency-key enforcement, HMAC-signed webhooks with rotating secrets, and deterministic fake card numbers that trigger specific error paths. Provide a seedable synthetic data generator and runtime toggles to simulate latency, network flakiness, 3DS challenge flows, rate limits, and partial outages to validate client robustness.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Static Analysis & Linting', 'Configure a Python+TypeScript monorepo to enforce strict static analysis: mypy --strict (with custom stub packages for a vendored library), flake8 with a custom plugin forbidding naive datetime usage, and ESLint/TypeScript rules requiring explicit return types. Refactor the codebase to eliminate all violations, wire the tools into pre-commit and a CI script that produces /app/lint_report.json and fails on any new issues.', NULL, ARRAY['python', 'typescript']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Static Analysis & Linting', 'Create and integrate a custom Flake8 plugin that flags timezone-naive datetime usage, mutable default arguments, and broad exception handlers in a small Python repo. Configure pyproject.toml so flake8 (with the plugin) and mypy --strict both run clean after refactoring the code to satisfy all checks.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Static Analysis & Linting', 'Implement a custom Flake8 plugin that flags calls where the return value of any function decorated with @must_use is ignored, emitting code MUU100 at the call site. Register the plugin and provide a script that runs flake8 on /app/src and writes a SARIF report to /app/lint.sarif.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Static Analysis & Linting', 'Implement a custom flake8 plugin that flags functions and methods using mutable default arguments (lists, dicts, sets, and comprehensions), supporting per-line noqa and exemptions for dataclasses field(default_factory=...). Integrate it via setup.cfg, run it over a small Python package with seeded violations, and make the codebase pass with a combination of fixes and justified ignores.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Static Analysis & Linting', 'Set up a cross-language lint/type pipeline for a Python+TypeScript monorepo using mypy (strict), flake8, and ESLint with typescript-eslint, plus custom analyzers that compare an OpenAPI spec to implemented HTTP routes to flag missing or undocumented endpoints. Integrate via pre-commit and a make lint target that emits a single consolidated JSON report and exits nonzero on any violations.', NULL, ARRAY['python', 'typescript']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Unit & Integration Test Implementation', 'Create a comprehensive pytest test suite for a cron expression parser and scheduler, including unit tests for ranges/steps/lists, W/L/# modifiers, invalid inputs, and timezone/DST edge cases, plus property-based tests that verify monotonic and gap-free next_run sequences. Add integration tests that exercise the CLI with stdin/files to validate output formatting, error messages, and exit codes, targeting at least 90% line and branch coverage.', NULL, NULL);
-- Compact batch 27/29: rows 1301-1350

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Unit & Integration Test Implementation', 'Implement unit and integration tests with pytest, pytest-asyncio, and Hypothesis for an asyncio WebSocket chat server skeleton, verifying message routing, rate limiting, heartbeat timeouts, and graceful disconnects. Tests must spin up the server on localhost, connect multiple clients concurrently, assert broadcast ordering and backpressure handling, and run deterministically without external network access.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Unit & Integration Test Implementation', 'Implement unit tests for a timezone-aware recurring event scheduler to verify next_occurrence and occurrences_between across DST forward/back transitions, month-end rollovers, and timezone changes using IANA zoneinfo and frozen time. Add integration tests that run the provided CLI against a fixtures.yaml of recurrence rules to assert deterministic outputs and round-trip serialization across multiple timezones.', NULL, NULL),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Unit & Integration Test Implementation', 'Write a comprehensive pytest + Hypothesis suite for an existing Python CRDT library (e.g., LWW-Element-Set and OR-Map) that verifies merge algebra (commutativity, associativity, idempotence), serialization round-trips, and tombstone semantics. Tests should generate randomized concurrent operation sequences across multiple replicas, simulate partitions and merges, and assert eventual consistency and order-independence in end-to-end scenarios.', NULL, ARRAY['python']),
('Software Engineering & Development', 'Testing, Validation & Quality Assurance', 'Unit & Integration Test Implementation', 'Write unit and integration tests for an existing Python resumable chunked downloader and its `fetch` CLI using pytest (and pytest-asyncio), mocking filesystem and network layers and spinning up a local HTTP server for end-to-end flows. Verify checksum validation, HTTP range-request handling, resume-after-interrupt via SIGINT/SIGTERM, and atomic file moves, enforcing at least 90% coverage.', NULL, ARRAY['python']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Cloud CLI Setup', 'Configure the AWS CLI to use a named profile that obtains credentials via credential_process from a local Python helper and auto-assumes a role, with region/output defaults set. Point S3 and STS to LocalStack endpoints so offline aws sts get-caller-identity and aws s3api list-buckets work using that profile.', NULL, ARRAY['aws', 'python']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Cloud CLI Setup', 'Install Google Cloud SDK and create two named gcloud configurations (dev and prod) using provided service account key files in /app/keys, setting distinct default project IDs and compute regions/zones for each. Activate each config in turn, initialize Application Default Credentials from its key, and verify the active account, project, and ADC source switch correctly between dev and prod.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Cloud CLI Setup', 'Install and configure AWS CLI, gcloud SDK, and Azure CLI to target local emulators (LocalStack for AWS, fake-gcs-server for GCP, and Azurite for Azure), creating named profiles and endpoint overrides so storage commands work offline. Verify by creating a bucket/container in each emulator and listing them via the respective CLIs using the configured profiles.', NULL, ARRAY['aws', 'azure', 'gcp', 'container']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Cloud CLI Setup', 'Install and configure AWS CLI, gcloud, and Azure CLI to target local emulators (LocalStack for AWS, Pub/Sub emulator for GCP, and Azurite for Azure) with offline credentials, named profiles/projects/subscriptions, and endpoint overrides so basic list/describe commands succeed without internet. Ensure defaults (region/output) are set and all configs reside in the standard dot-directories for each CLI.', NULL, ARRAY['aws', 'azure', 'gcp']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Cloud CLI Setup', 'Install and configure AWS CLI, gcloud, and Azure CLI to use local emulators (LocalStack, gcloud Pub/Sub emulator, and Azurite) with isolated named profiles and custom endpoints, persisting settings in the standard credential/config files. Validate by switching profiles and creating a test S3 bucket, Pub/Sub topic/subscription, and Azure Blob container, then listing them to confirm connectivity.', NULL, ARRAY['aws', 'azure', 'container']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Remote Session & SSH Configuration', 'Provision a bastion and an internal SSH service, create OpenSSH user and host CAs, and configure both servers to accept only CA-signed keys (password auth off), installing host certificates and @cert-authority entries in known_hosts. Add a client ~/.ssh/config with ProxyJump so ''ssh internal-via-bastion'' works, verify scp through the jump, and demonstrate a local port forward (15432 -> internal:5432) established via the bastion.', NULL, NULL),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Remote Session & SSH Configuration', 'Provision two OpenSSH servers (bastion and private) on an isolated network and configure the client to reach the private host only via the bastion using ProxyJump with per-host identities in ~/.ssh/config. Disable password authentication, hash and pin host keys in known_hosts, block direct access to the private host, and verify by copying a file and executing a remote command through the jump while confirming direct SSH to the private host fails.', NULL, NULL),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Remote Session & SSH Configuration', 'Provision two local OpenSSH daemons as a bastion (port 2222) and an internal host (port 2223), create an SSH CA, sign both host keys and a user certificate, and configure the servers to accept only CA-signed certs (no passwords or raw public keys). Configure the client with @cert-authority and hashed known_hosts plus a ~/.ssh/config using ProxyJump and ControlMaster to reach the internal host via the bastion, verifying SSH and SCP work with StrictHostKeyChecking=yes.', NULL, NULL),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Remote Session & SSH Configuration', 'Provision two local OpenSSH servers representing a bastion (port 2222) and an internal host (port 2223). Configure certificate-based SSH using a user CA and a host CA, disable passwords, require ProxyJump through the bastion, pre-populate known_hosts with @cert-authority entries, and verify that a single ssh command using the signed client key reaches the internal host via the bastion and creates /tmp/ok.', NULL, NULL),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Remote Session & SSH Configuration', 'Stand up two local sshd instances (bastion on one port, app on another), configure ProxyJump through the bastion, and implement OpenSSH certificate-based auth by creating a user CA to sign the client key and a host CA to sign the app host key; configure sshd and the client with hashed known_hosts using @cert-authority for non-interactive trust. Verify that connecting to the app via the bastion works passwordlessly without host-key prompts and that access breaks when certs expire or principals don’t match.', NULL, NULL),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Resource Provisioning & Management', 'Launch a LocalStack instance and use Terraform to provision an S3 bucket, an SQS queue, and a least-privilege IAM user/policy, wiring S3 ObjectCreated notifications to the queue. Configure awscli with the issued credentials, upload a test object, and consume the resulting queue message to validate the end-to-end flow.', NULL, NULL),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Resource Provisioning & Management', 'Provision an AWS-like event ingestion pipeline on LocalStack via CLI/IaC: create an S3 bucket (versioning + SSE), an SNS topic, an SQS queue with DLQ, and an IAM user restricted to a specific S3 prefix. Wire S3 ObjectCreated notifications S3→SNS→SQS and verify by uploading with the new credentials and receiving the resulting queue message.', NULL, ARRAY['aws']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Resource Provisioning & Management', 'Provision an AWS-like stack on LocalStack using Terraform: an S3 bucket with versioning and SSE, an SNS topic, and an SQS queue subscribed to the topic via least-privilege IAM policies and resource tags. Configure AWS CLI to target LocalStack and demonstrate end-to-end by uploading to S3, publishing to SNS, verifying the message in SQS, and exporting a JSON summary of created ARNs.', NULL, ARRAY['aws']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Resource Provisioning & Management', 'Start a LocalStack container and use Terraform to provision an SNS topic, an SQS queue with a dead-letter queue, and a subscription connecting the topic to the queue. Publish a test message and verify delivery by consuming it from the queue via the AWS CLI.', NULL, ARRAY['container', 'aws']),
('System Setup & Configuration', 'Cloud & Remote Environment Configuration', 'Resource Provisioning & Management', 'Use Terraform against LocalStack to provision an S3→SNS→SQS event pipeline: a versioned bucket that publishes object-created events to an SNS topic with an SQS subscription and a dead-letter queue. Validate by uploading a test object via awscli, reading the event from SQS, and demonstrating DLQ redrive behavior.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Backup & Snapshot Management', 'Create an LVM volume group on a loopback disk with an ext4 logical volume mounted at /mnt/data, and implement consistent backups by taking read-only LVM snapshots before each run. Configure a systemd service/timer that uses rsync --link-dest to maintain 7 rotating daily backups in /srv/backups, then demonstrate recovery by restoring a file from a previous snapshot.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Backup & Snapshot Management', 'Deploy a local MinIO S3 service and configure restic with repository encryption to back up /app/data using exclude patterns, tags, and a scheduled cron job enforcing a retention policy (keep 7 daily, 4 weekly, 12 monthly). Perform initial and follow-up backups with file changes, run forget/prune, then restore a specific snapshot to /app/restore verifying checksums and that no plaintext filenames exist in the repository.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Backup & Snapshot Management', 'Install and configure BorgBackup to create encrypted, deduplicated snapshots of /data into a local repository at /backups/borg with an hourly systemd timer and a retention policy of 7 daily, 4 weekly, and 6 monthly snapshots, followed by a repository integrity check. Demonstrate correctness by modifying and deleting sample files, restoring a chosen snapshot to /restore, and verifying contents and that prune retained the requested generations.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Backup & Snapshot Management', 'Install and configure rsnapshot to perform rotating, hard-link-based backups of /etc and /srv/data to /backup/rsnapshot with retentions hourly:6, daily:7, weekly:4 and excludes for logs/cache, scheduled via cron. Verify by modifying files, running backups, confirming deduplication through hard-link counts across snapshots, and restoring a deleted file to its original location.', 'hard', NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Backup & Snapshot Management', 'Set up two loopback-backed btrfs filesystems mounted at /srv/src and /srv/backup, create a /srv/src/data subvolume, and automate hourly read-only snapshots named snap-YYYYMMDDHH with pruning to the latest 6 plus incremental btrfs send/receive mirroring to /srv/backup, logging to /var/log/btrfs-backup.log. Verify recovery by deleting a file in /srv/src/data and restoring it from the newest snapshot on the backup target.', NULL, ARRAY['logging']),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Disk Partitioning & Mounting', 'Create a 3 GiB sparse file as a loop device, partition it with GPT into two volumes (512 MiB ext4 labeled DATA and the remainder as a LUKS-encrypted XFS labeled SECRET) and set proper labels/UUIDs. Configure /etc/crypttab and /etc/fstab to unlock and mount them at /mnt/data and /mnt/secret by UUID, then verify persistence by detaching/reattaching the loop device and remounting without using device paths.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Disk Partitioning & Mounting', 'Create a 3GB sparse disk image, attach it as a loop device, and partition it with GPT into a small unencrypted FAT32 partition and a LUKS2-encrypted data partition containing an LVM PV/VG/LV formatted as ext4. Mount the LV at /mnt/secure with restrictive options and persist the configuration via /etc/crypttab and /etc/fstab using UUIDs/labels, verifying it remounts correctly after detaching and reattaching the loop device.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Disk Partitioning & Mounting', 'Create a GPT-partitioned loopback disk, build an LVM-on-LUKS stack on one partition, and mount the decrypted logical volume at /srv/secure with persistent /etc/crypttab and /etc/fstab entries using UUIDs and a root-only keyfile. Demonstrate that after detaching and reattaching the loop device the volume auto-opens and mounts non-interactively, preserving data.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Disk Partitioning & Mounting', 'Provision a 3 GiB loopback disk image and partition it with GPT into: 512 MiB EFI System (vfat, label EFI), 2 GiB Linux filesystem (ext4, label DATA), and the remainder as Linux swap. Format and label each partition, mount EFI at /mnt/efi and DATA at /mnt/data, enable swap, and persist the setup using PARTUUID-based entries in /etc/fstab so all mounts reattach correctly after unmounting and remounting.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Disk Partitioning & Mounting', 'Provision an encrypted LVM-on-LUKS volume backed by a sparse loop device: create a GPT partition on the loop device, initialize it with LUKS using a keyfile, build a VG with two LVs (data and logs), format, and mount at /srv/secure/{data,logs}. Persist the setup via /etc/crypttab and /etc/fstab using UUIDs so it can be unlocked and mounted non-interactively and verified with mount -a after reattaching the loop device.', NULL, NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Filesystem Permissions & Quotas', 'Create a loopback-backed XFS filesystem mounted at /mnt/projects with pquota enabled and configure project quotas for two directories (/mnt/projects/build-cache and /mnt/projects/datasets) via /etc/projid and /etc/projects, setting soft/hard limits of 500MB and 1GB with a 7-day grace period. Apply setgid and default ACLs so group ''research'' has rwx on both trees, then verify quota enforcement with xfs_quota reports and writes that exceed the limits.', 'hard', NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Filesystem Permissions & Quotas', 'Create a loopback-backed XFS filesystem mounted at /srv/projects with project quotas (prjquota) enabled, defining two directory-scoped projects via /etc/projects and /etc/projid with distinct soft/hard limits, and configuring setgid plus default ACLs for collaborative inheritance. Verify enforcement by filling each project until soft/hard limits and grace periods trigger, confirming with xfs_quota reports and observed EDQUOT failures.', 'hard', NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Filesystem Permissions & Quotas', 'Create a loopback-backed XFS volume mounted at /mnt/projects with project quotas enabled, defining project IDs for team-a (200 MiB) and scratch (50 MiB) and enforcing hard limits. Configure a shared dir with setgid and default POSIX ACLs for group inheritance, a scratch dropbox with the sticky bit, and a logs dir marked append-only via chattr, then verify quota and permission behavior with automated writes.', 'hard', NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Filesystem Permissions & Quotas', 'Provision a loopback-mounted XFS filesystem at /mnt/projects with prjquota enabled, bind project ''projA'' to /mnt/projects/projA via /etc/projid and /etc/projects, and enforce an 80MB soft/100MB hard project quota (7-minute grace) using xfs_quota with persistence in /etc/fstab. Validate enforcement by writing data as multiple users and confirming over-limit writes are denied.', 'hard', NULL),
('System Setup & Configuration', 'Filesystem & Storage Management', 'Filesystem Permissions & Quotas', 'Provision an XFS filesystem on a loopback device mounted at /mnt/studio with prjquota enabled, configure a project quota that caps /mnt/studio/assets at 1 GiB via /etc/projects and xfs_quota, and set per-user soft/hard quotas for alice and bob. Enforce collaborative permissions by applying setgid, sticky bit, and default ACLs for the ''artists'' group, demonstrate chattr +i on a file, and verify quota/permission enforcement with xfs_quota and failed writes.', 'hard', NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Firewall & Security Rules', 'Configure a host-wide nftables firewall with default-drop policy using inet and ip6 tables: allow established/related, loopback, necessary ICMP/ICMPv6 (ND, RA, RS, PTB), and open SSH (22) and HTTP (8080) only on IPv4, with rate-limited new connections. Make the rules persistent across reboots and enable logging of dropped packets to /var/log/nftables-drop.log; verify IPv4 HTTP works while IPv6 HTTP is blocked and ICMPv6 neighbor discovery still functions.', NULL, ARRAY['logging']),
('System Setup & Configuration', 'Networking & Connectivity', 'Firewall & Security Rules', 'Configure iptables to implement a three-step port knocking sequence (4001 → 4002 → 4003 within 10s) that temporarily opens SSH (port 22) only for the knocking source, while default-denying inbound and allowing established/related traffic. Persist the rules across reboots and verify SSH is blocked before knocking, opens after the sequence, and relocks after a timeout.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Firewall & Security Rules', 'Configure iptables-based port knocking that keeps SSH (port 22) closed by default and opens it for 60 seconds only to the knocking client after a correct sequence on three high ports. Enforce a default-deny inbound policy, add logging for knock attempts, and verify both denial without knocking and successful SSH after the sequence.', NULL, ARRAY['logging']),
('System Setup & Configuration', 'Networking & Connectivity', 'Firewall & Security Rules', 'Configure nftables to enforce a default-deny host firewall with stateful rules. Allow loopback/established traffic, port-forward 80→8080 to a local service, restrict SSH on 2222 to 10.0.0.0/24 with rate limiting, block outbound SMTP except to smtp.example.net, log drops with prefix TB-FW, ensure persistence across reboots, and verify behavior over IPv4/IPv6 with curl/nc tests.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Firewall & Security Rules', 'Configure nftables to enforce a host firewall with default drop on inbound traffic, allowing established/related and loopback, permitting HTTP/HTTPS to a local nginx, and opening SSH only after a correct 3-step UDP port-knock (1111→2222→3333 within 10s) with rate-limited access. Make the rules persistent across reboots and include a verification script that demonstrates SSH is closed before knocking, opens for 30 seconds after the sequence, then automatically closes again.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Interface & IP Configuration', 'Configure a dual-homed Linux host where eth0 uses DHCP and eth1 has a static 192.168.50.2/24; add policy-based routing so traffic sourced from 192.168.50.0/24 uses gateway 192.168.50.1 while all other traffic uses the DHCP default route. Set per-domain DNS with systemd-resolved so queries to corp.local use 192.168.50.53 via eth1, ensuring settings persist across reboots with netplan.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Interface & IP Configuration', 'Using iproute2 and network namespaces, build a routed lab with three namespaces (hostA, router, hostB): create veth links, assign static IPv4/IPv6, configure per-namespace DNS via a dnsmasq bound to the router, add policy-based routing and default routes, enable IPv4 forwarding/NAT on the router, and verify end-to-end connectivity and name resolution.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Network Diagnostics & Debugging', 'Build a three-namespace topology (client–router–server) with mismatched MTUs and ICMP “Fragmentation Needed” filtered, then diagnose a stalled TCP connection using tracepath/ping and tcpdump to pinpoint a PMTU blackhole. Fix the issue by permitting ICMP or enabling TCP MSS clamping and verify successful HTTP transfer end-to-end.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Network Diagnostics & Debugging', 'Create a three-namespace topology (client-router-server) where the router drops ICMP Fragmentation Needed, producing a PMTU blackhole that causes HTTPS requests to hang. Use ping with DF, traceroute, and tcpdump to confirm the issue, then fix it by enabling TCP MSS clamping (or lowering MTU) on the router and verify curl completes successfully.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Network Diagnostics & Debugging', 'Create three network namespaces (client, router, server) linked by veth; drop ICMP ''Fragmentation Needed'' on the router to simulate a PMTU black hole that causes HTTP transfers to hang. Diagnose with ping -M do, traceroute, ss, and tcpdump, then fix by enabling TCP MSS clamping on the router and verify a large curl from client to server succeeds.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Network Diagnostics & Debugging', 'Diagnose and fix stalled large file downloads caused by PMTU blackholing: use tcpdump, tracepath, and curl to confirm ICMP “Fragmentation Needed” is being dropped and that the interface MTU is misconfigured. Restore connectivity by allowing the required ICMP through iptables and/or clamping TCP MSS, then verify the transfer completes.', NULL, NULL),
('System Setup & Configuration', 'Networking & Connectivity', 'Network Diagnostics & Debugging', 'Simulate and diagnose a path MTU black hole using Linux network namespaces by creating a multi-hop topology with mismatched MTUs and blocking ICMP “Fragmentation Needed,” causing large TCP transfers to stall. Use ping with DF, traceroute, tcpdump, and curl to pinpoint the issue, then fix it by allowing ICMP or applying TCP MSS clamping and verify sustained transfers succeed.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'Environment Variables & Profiles', 'Create a layered cross-shell environment configuration that defines APP_ENV=staging and prepends /opt/tools/bin to PATH without duplicates via /etc/profile.d/bench.sh (interactive shells) and ~/.config/environment.d/bench.conf (session-wide). Ensure these variables appear in login and non-login shells, ssh commands, a cron job for user ''bench'', and a systemd --user service, while interactive-only prompt changes do not affect scripts.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'Environment Variables & Profiles', 'Create a unified environment manager that reads variables and PATH entries from ~/.config/envvars.yaml and generates shell-agnostic snippets sourced by both Bash and Zsh for login, non-login, interactive, and non-interactive sessions. Provide an envsync command to rebuild the snippets and verify PATH precedence, LANG/LC_ALL, and HTTP(S)_PROXY/NO_PROXY persist across new shells and subprocesses.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'Environment Variables & Profiles', 'Install and configure direnv for both bash and zsh by adding the required shell hooks to the correct profile files so it runs in interactive sessions. Create a project directory with an .envrc that exports custom variables (e.g., APP_ENV, LOG_LEVEL), prepends ./bin to PATH, optionally activates a Python virtualenv if present, and verify the environment loads only inside the directory after direnv allow and unloads on exit.', NULL, ARRAY['python']),
('System Setup & Configuration', 'Operating System Configuration', 'Environment Variables & Profiles', 'Install and configure direnv globally for bash and zsh, then create /app/project with an .envrc that adds /app/bin to PATH, sets APP_ENV=dev and API_URL, and activates a Python virtualenv via layout python. Verify that entering and leaving /app/project automatically loads/unloads the environment in both shells after direnv allow, while non-project shells remain unaffected.', NULL, ARRAY['python']);
-- Compact batch 28/29: rows 1351-1400

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('System Setup & Configuration', 'Operating System Configuration', 'Environment Variables & Profiles', 'Install direnv and integrate it system-wide for both bash and zsh via /etc/profile.d so it loads automatically for all users. Create /srv/project with a .envrc that exports APP_ENV=staging, prepends /opt/tools/bin to PATH, sets PYTHONPATH, and verify as user dev that entering/leaving the directory auto-loads/unloads these variables.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'Process & Resource Management', 'Create a cgroups v2-backed systemd slice (tb-limited.slice) enforcing CPUQuota and MemoryMax, run a resource-hungry service (e.g., stress-ng via tb-hog.service) within it, and schedule it with a systemd timer that adds a randomized delay. Verify enforcement by observing throttling/OOM-kill behavior, automatic restarts, and corroborating logs via journald and systemd-cgtop.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'Process & Resource Management', 'Create a dedicated cgroup v2 slice (batch.slice) and a templated systemd service (batch@.service) that runs a CPU/memory-intensive script under strict limits (CPUAffinity to specific cores, CPUQuota=25%, MemoryMax=150M, TasksMax=32) with automatic restart on OOM. Trigger it via a systemd timer every 2 minutes and verify via cpu.stat/memory.events that throttling and OOM handling occurred, with logs persisted to /var/log/batch/.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'Process & Resource Management', 'Create a sandboxed systemd service and timer that runs a worker script under cgroups v2 with CPUQuota=40%, MemoryMax=200M, CPUAffinity pinned to one core, TasksMax, and hardening options (NoNewPrivileges, PrivateTmp, ProtectSystem=strict), logging to both the journal and /var/log/worker.log. Enable and verify the limits by starting the unit, inspecting cgroup metrics, and demonstrating the timer’s persistence and randomized delay.', 'hard', ARRAY['logging']),
('System Setup & Configuration', 'Operating System Configuration', 'Process & Resource Management', 'Create a systemd slice and templated service constrained by cgroup v2 (e.g., CPUQuota and MemoryMax) and trigger it via a Persistent=true systemd timer to run periodic batch jobs. Verify scheduled execution, enforced resource limits, automatic restart-on-failure behavior, and logs accessible with journalctl.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'Process & Resource Management', 'Define and enable a custom systemd slice (bench.slice) with cgroup v2 limits (CPUQuota=40%, MemoryMax=512M) and a service (bench-workload.service) bound to it that runs a CPU/memory stress script with Restart=always and hardened sandboxing. Add a systemd timer to schedule the workload every 5 minutes, log output to /var/log/bench-workload.log, and verify enforcement by inspecting cgroup stats for CPU throttling and memory limits.', 'hard', NULL),
('System Setup & Configuration', 'Operating System Configuration', 'System Parameters & Kernel Settings', 'Configure persistent kernel core dump handling by setting kernel.core_pattern to pipe to a custom collector script that stores and compresses cores under /var/lib/cores with metadata. Create and enable a systemd service that runs a small crashing test binary with LimitCORE=infinity to verify capture and persistence across reboots.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'System Parameters & Kernel Settings', 'Configure system-wide core dump handling by setting kernel.core_pattern to pipe crashes into a custom /usr/local/bin/core_collector that compresses dumps under /var/cores with metadata, and disable systemd-coredump while enforcing fs.suid_dumpable=0. Persist ulimit core=0 for all users except a dedicated systemd service that overrides to unlimited, then verify by crashing a test binary from both contexts and confirming only the service produces a compressed core in /var/cores.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'System Parameters & Kernel Settings', 'Enable compressed swap-on-RAM (zram) with a persistent swap device sized to 50% of system memory, tune vm.swappiness=80 and vm.vfs_cache_pressure=200 via /etc/sysctl.d, and disable Transparent Huge Pages at boot. Verify the settings persist and that zram is active in /proc/swaps and THP shows "never" in sysfs.', NULL, NULL),
('System Setup & Configuration', 'Operating System Configuration', 'System Parameters & Kernel Settings', 'Persistently harden and tune the kernel by setting sysctl values via /etc/sysctl.d (disable unprivileged user namespaces and BPF, restrict dmesg, enable TCP syncookies, increase somaxconn and inotify limits) and raise per-user nofile/nproc ulimits via /etc/security/limits.d. Verify live and persistent effect with sysctl and ulimit checks after reloading settings and starting a fresh shell.', 'hard', ARRAY['security']),
('System Setup & Configuration', 'Service & Daemon Management', 'Log Monitoring & Service Debugging', 'Create a socket-activated systemd service (myapp.socket/myapp.service) for a simple HTTP server that initially fails to start, enable persistent journald storage, and use journalctl to pinpoint the root causes (bad WorkingDirectory and missing RuntimeDirectory). Correct the unit (User/Group, WorkingDirectory, RuntimeDirectory, LimitNOFILE), set journald rate limiting, and verify the socket serves 127.0.0.1:8080 with clean, non-repeating logs.', NULL, NULL),
('System Setup & Configuration', 'Service & Daemon Management', 'Log Monitoring & Service Debugging', 'Create a systemd service and timer that runs a backup script every minute, where the initial run fails due to a missing WorkingDirectory and permission errors. Use journalctl to diagnose the failures, fix the unit (User/Group, WorkingDirectory/RuntimeDirectory, ExecStart), and verify via logs that subsequent timer invocations complete successfully with stdout/stderr captured in journald.', NULL, NULL),
('System Setup & Configuration', 'Service & Daemon Management', 'Log Monitoring & Service Debugging', 'Debug a crash-looping systemd service ''img-resizer.service'' by inspecting journald and its app logs to pinpoint failures caused by PrivateTmp and a missing RuntimeDirectory (socket at /tmp/img.sock and unwritable log path). Fix the unit to use /run/img-resizer via RuntimeDirectory=, correct User/Group and dependencies, reload systemd, and confirm the service stays active and the UNIX socket handles a test request.', NULL, NULL),
('System Setup & Configuration', 'Service & Daemon Management', 'Log Monitoring & Service Debugging', 'Debug a systemd unit ''imgsvc.service'' that crash-loops on startup: journald reveals ''listen tcp :80: bind: permission denied'' and write failures due to DynamicUser=yes. Update the unit to grant AmbientCapabilities=CAP_NET_BIND_SERVICE and define a RuntimeDirectory with correct permissions, then verify it binds to 0.0.0.0:80 and runs cleanly with logs visible in journald.', NULL, NULL),
('System Setup & Configuration', 'Service & Daemon Management', 'Log Monitoring & Service Debugging', 'Investigate a failing systemd unit (myapp.service) for a Gunicorn-backed Python web app that repeatedly restarts; use journalctl and the app’s error logs to identify an invalid WorkingDirectory and a missing EnvironmentFile. Fix the unit via a systemd drop-in (correct ExecStart/WorkingDirectory, EnvironmentFile, User, Restart policy), enable persistent journald with size limits, then verify with curl that the service stays active and logs show a 200 response without rate-limit warnings.', NULL, ARRAY['python', 'web']),
('System Setup & Configuration', 'Service & Daemon Management', 'Service Installation & Setup', 'Install Prometheus and node_exporter as system services, configure Prometheus to scrape node_exporter and itself on localhost, and ensure both start on boot. Verify successful scraping by querying Prometheus’s HTTP API for the up metric and confirm it returns 1 for both targets.', NULL, ARRAY['api']),
('System Setup & Configuration', 'Service & Daemon Management', 'Service Installation & Setup', 'Install Prometheus and node_exporter as systemd services, configure Prometheus (/etc/prometheus/prometheus.yml) to scrape node_exporter on localhost:9100 every 5s with a custom external_label and 1h retention. Enable and start both services, then verify the target is UP via the Prometheus HTTP API and that metrics are served at http://localhost:9100/metrics.', NULL, ARRAY['api']),
('System Setup & Configuration', 'Service & Daemon Management', 'Service Installation & Setup', 'Install and configure MinIO as an S3-compatible object storage service with a self-signed TLS certificate, listening on 0.0.0.0:9443 and managed via systemd. Create an admin user and a test bucket with a read-only policy, configure AWS CLI to use the endpoint, and verify by uploading and retrieving a file.', NULL, ARRAY['aws']),
('System Setup & Configuration', 'Service & Daemon Management', 'Service Installation & Setup', 'Install and configure OpenLDAP (slapd) with a base DN (dc=tb,dc=local), enabling StartTLS on 389 and LDAPS on 636 with a self-signed certificate, plus the memberOf/refint overlays and proper indexing. Create an OU and a test user with a salted hashed password, run slapd in the background, and verify authenticated ldapsearch/ldapmodify operations over TLS return expected entries.', NULL, NULL),
('System Setup & Configuration', 'Service & Daemon Management', 'Service Installation & Setup', 'Install and configure the Mosquitto MQTT broker with plaintext (1883) and TLS (8883) listeners, password-based authentication, and persistence managed via systemd. Generate a self-signed certificate, enable the service on boot, and verify pub/sub functionality using mosquitto_pub and mosquitto_sub over both transports.', NULL, NULL),
('System Setup & Configuration', 'Service & Daemon Management', 'Systemd & Init Configuration', 'Create a socket-activated, templated systemd service (echo@.service) that spawns per-connection Python echo handlers from echo.socket on 127.0.0.1:9999 (Accept=yes), running under DynamicUser with hardening and network-online dependencies. Add a non-root user-level timer (with lingering enabled) that health-checks the socket every minute and configure persistent journald logs to verify on-demand startup and successful checks.', 'hard', ARRAY['python']),
('System Setup & Configuration', 'Service & Daemon Management', 'Systemd & Init Configuration', 'Create a socket-activated, templated systemd service (echo@.socket/echo@.service) for a tiny Python HTTP echo server that runs as an unprivileged user and starts on-demand when connections arrive, with instances bound to ports via the instance name. Enable sockets for ports 8081 and 9091, ensure After=network-online.target with basic hardening, and verify curl requests trigger the service and return a response while logs appear in journald.', 'hard', ARRAY['python']),
('System Setup & Configuration', 'Service & Daemon Management', 'Systemd & Init Configuration', 'Create a socket-activated, templated systemd service that spawns per-connection instances of a tiny Python HTTP responder via Accept=yes on port 8080 using the inherited socket (fd 3). Verify on-demand activation with curl, that each instance exits after one request, and enable the socket to start at boot.', NULL, ARRAY['python']),
('System Setup & Configuration', 'Service & Daemon Management', 'Systemd & Init Configuration', 'Implement a socket-activated, per-connection systemd service (using a .socket and a templated .service) that runs a minimal HTTP echo server bound to 127.0.0.1:8085 with DynamicUser and strict sandboxing. Validate that a curl request triggers on-demand startup, serves a response, then the service stops after an idle timeout while the socket remains listening.', NULL, NULL),
('System Setup & Configuration', 'Service & Daemon Management', 'Systemd & Init Configuration', 'Implement a systemd socket-activated, per-connection echo service: echo.socket listens on 127.0.0.1:12345 with Accept=yes, and echo@.service (Type=simple, StandardInput/Output=socket) runs /bin/cat with DynamicUser and strict sandboxing. Enable the socket, verify with nc 127.0.0.1 12345 that input is echoed, and confirm each connection spawns and exits a separate instance recorded in the journal.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Dependency Verification & Repair', 'Diagnose and repair a system with corrupted shared libraries and an interrupted apt/dpkg transaction by verifying package integrity with debsums and fixing the broken package state. Reinstall only the affected packages, refresh the dynamic linker cache, and verify the repair with a successful HTTPS curl and a Python ssl import.', NULL, ARRAY['python']),
('System Setup & Configuration', 'Software & Package Management', 'Dependency Verification & Repair', 'Diagnose and repair missing 32-bit runtime libraries for a provided 32-bit ELF binary (/app/bin/hello32) by enabling i386 multiarch on Debian/Ubuntu, installing required :i386 packages (e.g., libc6, libstdc++6, libgcc-s1), and refreshing the linker cache. Prove the fix by ensuring ldd shows no "not found" entries, the binary executes successfully, and saving the resolved ldd map to /app/output/ldd-hello32.txt.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Dependency Verification & Repair', 'In a Debian-based container with mixed-release APT sources and a corrupted dpkg status (half-installed/half-configured packages), restore the package manager to a healthy, consistent ''stable'' state by fixing the dpkg database, correcting sources, resolving pinned/held packages, and repairing dependency conflicts. Validate by achieving a clean apt-get check/apt-get -f install run and successfully using curl to fetch an HTTPS URL with system OpenSSL.', NULL, ARRAY['container', 'database']),
('System Setup & Configuration', 'Software & Package Management', 'Dependency Verification & Repair', 'Recover a Debian/Ubuntu system from an interrupted apt upgrade that left dpkg in a broken state by repairing the package database, clearing partial installs, and resolving held/unmet dependencies with correct version pinning. Validate the fix by installing and running a provided CLI that initially fails due to missing libssl/libffi SONAMEs, ensuring compatibility libraries are installed properly without manual symlinks.', NULL, ARRAY['database']),
('System Setup & Configuration', 'Software & Package Management', 'Dependency Verification & Repair', 'Resolve a C++ runtime symbol version mismatch by installing a compatible libstdc++ (e.g., GLIBCXX_3.4.26+) on a Debian-based system where a precompiled binary fails with ''version GLIBCXX_3.4.26 not found''. Configure apt pinning/backports to upgrade only libstdc++6 and required dependencies without a full dist-upgrade, then validate the binary runs and ldd shows no unresolved symbols.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Package Installation & Removal', 'Configure APT pinning to add Debian testing with low priority, then install only neovim (>=0.9) from testing while keeping the rest of the system on stable, removing any vim* packages and setting nvim as the default vi/editor via update-alternatives. Place neovim on hold to prevent unintended upgrades and verify pinning/hold status with apt-cache policy and apt-mark.', NULL, ARRAY['testing', 'rest']),
('System Setup & Configuration', 'Software & Package Management', 'Package Installation & Removal', 'Create a GPG-signed local APT repository served over HTTP, build and publish a hello-terminal-bench .deb to it, add the repository to sources.list with its key, then install the package using apt. Finally, remove and purge the package and verify its maintainer scripts executed by logging to a known file.', NULL, ARRAY['logging']),
('System Setup & Configuration', 'Software & Package Management', 'Package Installation & Removal', 'Create a signed local APT repository under /srv/apt (using apt-ftparchive or reprepro), add it to sources.list.d with its GPG key, and install a provided custom .deb from it. Then remove the package, revoke the repo by removing the key and source, run apt clean/update, and verify the package is no longer available.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Package Installation & Removal', 'Enable multiarch on a Debian/Ubuntu system to install side-by-side i386 and amd64 variants of selected libraries (e.g., libssl and zlib), pin the exact versions, and validate via dpkg-query that both architectures are present. Then purge the i386 variants, remove the version holds, disable multiarch, and confirm apt autoremove leaves no residual dependencies.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Package Installation & Removal', 'Set up a signed local APT repository hosting a dummy hello-bench .deb and configure apt to trust and prefer it via pinning. Install hello-bench from the local repo, then remove and purge it, verifying with apt-cache policy and dpkg status that the correct source and state transitions occurred.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Repository Configuration', 'Build and serve a local APT repository containing a trivial hello-world .deb; generate a GPG key, sign the Release, and add a deb822 .sources entry using signed-by in /usr/share/keyrings. Pin priorities so only hello-world resolves from the local repo while all other packages come from the default sources, and verify via apt-cache policy and installation.', NULL, ARRAY['installation']),
('System Setup & Configuration', 'Software & Package Management', 'Repository Configuration', 'Create a custom DNF/YUM repository from a local directory, generate a new GPG key, sign both RPMs and repodata, and publish it via nginx. Add the repo with repo_gpgcheck=1, gpgcheck=1, and a dedicated keyring, set repo priority to prefer only one named package from it, then verify installs succeed only when signatures are valid and the unsigned variant is rejected.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Repository Configuration', 'Create a local APT repository at /repo, serve it over HTTPS via nginx, and sign its Release/InRelease files with a newly generated GPG key; publish and index a minimal hello-world .deb. Add the repo to apt sources using signed-by, confirm apt update and package installation succeed, then demonstrate that altering the signed metadata causes apt to refuse the repo until the correct signature/key is restored.', NULL, ARRAY['installation']),
('System Setup & Configuration', 'Software & Package Management', 'Repository Configuration', 'Create a locally hosted, GPG-signed APT repository (served on localhost) containing a provided .deb, add it to sources using the signed-by option with a keyring in /usr/share/keyrings, and install the package from it. Demonstrate that apt rejects the repo before the key is configured and succeeds after the correct keyring is installed and referenced.', NULL, NULL),
('System Setup & Configuration', 'Software & Package Management', 'Repository Configuration', 'Create a private, HTTPS APT repository on localhost using reprepro and nginx, sign it with a dedicated GPG key exported to /etc/apt/keyrings/bench-archive.gpg, and protect it with HTTP Basic Auth. Add a sources.list.d entry that uses signed-by and credentials from /etc/apt/auth.conf.d plus APT pinning to prefer this origin, then verify apt update and installing a test package come only from the signed repo.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Log Analysis & Troubleshooting', 'Create a CLI that parses /var/log (syslog, auth.log, kern.log, and rotated files) for the last 24h, normalizes PIDs/IPs/hex to cluster messages into error signatures, and ranks them by frequency. Produce /app/log_report.json and /app/log_report.txt summarizing each cluster with count, first/last timestamps, and inferred service to aid rapid fault isolation.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Log Analysis & Troubleshooting', 'Create a log-analysis utility that scans /var/log (including rotated *.1 and *.gz files) and journalctl for the last 24h to detect recurring SSH auth failures per IP, systemd crash/restart loops, kernel OOM/I/O/EXT4 warnings, and SMART/disk errors, outputting counts and first/last timestamps to /var/log/health/report.json. Schedule it via cron to run every 15 minutes, keep a 7-day rolling archive under /var/log/health/history/, and exit non-zero when thresholds are exceeded to flag incidents.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Log Analysis & Troubleshooting', 'Enable persistent journald storage and implement a CLI tool that scans the last 24 hours of logs (journalctl and /var/log/*) to produce /app/output/log_health.json summarizing recurring errors by service, kernel disk I/O/EXT4/SMART warnings, and top IPs for SSH auth failures. Add a systemd service+timer that streams logs and appends an alert to /app/output/alerts.log whenever more than 10 ''Failed password'' messages from the same IP occur within 5 minutes.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Log Analysis & Troubleshooting', 'Enable persistent journald storage and implement a tool that parses the last 24h of system logs to detect flapping systemd services (restarts), recurring kernel I/O errors, and repeated SSH auth failures, aggregating them into /app/incident_report.json with counts and first/last timestamps. The tool must also emit a remediation checklist to /app/remediation.txt with per-issue hints (e.g., identify the failing unit, suggest RestartSec, highlight suspect devices/IPs).', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Log Analysis & Troubleshooting', 'Enable persistent systemd-journal storage and implement a CLI tool that scans /var/log and journalctl for the last 24 hours, normalizes messages into signatures by stripping volatile fields, and ranks recurring errors while flagging previously unseen error types. Produce both a machine-readable JSON report and a concise human summary to guide troubleshooting of the top offenders.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Performance Tuning', 'Deploy Nginx to serve a large static file and record a baseline using a load generator (e.g., wrk). Tune worker and I/O settings (epoll, worker_processes auto, worker_connections, sendfile/tcp_nopush, open_file_cache, reuseport, directio for large files) to achieve at least a 50% throughput increase while keeping 99th percentile latency under a set target.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Performance Tuning', 'Implement PSI-aware memory tuning by enabling zram swap and adjusting vm.swappiness, min_free_kbytes, and dirty writeback thresholds to reduce contention under load. Drive concurrent memory pressure with stress-ng alongside a lightweight HTTP service and demonstrate improved p95/p99 latency and zero OOMs versus a baseline configuration.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Performance Tuning', 'Optimize a localhost Nginx static-file server for high concurrency and low latency by tuning Linux TCP/sysctl (e.g., somaxconn, tcp_max_syn_backlog, ip_local_port_range), file-descriptor limits, and Nginx worker settings (worker_processes, worker_connections, reuseport, sendfile). Provide a repeatable wrk/ab benchmark that records before/after throughput and p99 latency, demonstrating measurable improvement and saving results to /app/perf/results.json.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Performance Tuning', 'Use cgroup v2 (via systemd) to isolate a foreground web service from background CPU and disk contention by tuning cpu.max/cpu.weight, io.weight, and memory.high/memory.min. Induce load with stress-ng/fio and demonstrate improved p99 latency under wrk after tuning compared to a baseline.', NULL, ARRAY['web']),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Performance Tuning', 'Use cgroups v2 (via systemd slices) to constrain a CPU/memory‑intensive background workload while prioritizing a latency‑sensitive HTTP server, and tune kernel networking sysctls (somaxconn, tcp_fastopen) to improve accept throughput. Verify under sustained stress that the server maintains sub‑50 ms p95 latency over 1000 requests and that the batch workload cannot exceed its CPU/memory quotas.', NULL, ARRAY['networking']);
-- Compact batch 29/29: rows 1401-1434

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Resource Monitoring', 'Build a CLI monitoring tool that periodically samples CPU, memory, and disk I/O using top/ps, vmstat, and iostat over a fixed window and writes a single summary report highlighting bottlenecks (top CPU/RSS processes, avg iowait, swap activity, busiest block device). If thresholds are exceeded, emit actionable tuning suggestions (e.g., processes to renice/ionice and kernel parameter hints like vm.swappiness) and exit non-zero to signal an alert.', NULL, ARRAY['monitoring']),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Resource Monitoring', 'Create a shell-based monitor that induces CPU, memory, and disk load, samples vmstat and iostat at 1s intervals for 60s, captures top/ps snapshots, and writes /app/metrics.jsonl plus /app/resource_report.txt. The script must auto-classify the host as CPU-, memory-, or I/O-bound (verifying an injected I/O-bound scenario) and include at least one actionable tuning recommendation.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Resource Monitoring', 'Create an automated profiler that runs a short, reproducible workload and samples top, ps, iostat -x, and vmstat at 1-second intervals for 60 seconds, writing CSV logs and a human-readable summary. Based on thresholds (CPU idle, iowait, swap-in/out, disk util/await), classify the bottleneck as CPU-, memory-, or disk-bound and emit one concrete tuning recommendation (e.g., adjust vm.swappiness or revisit the I/O scheduler).', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Resource Monitoring', 'Create triage.sh that concurrently samples vmstat 1, iostat -xz 1, pidstat/top in batch mode for ~30s, computes averages, classifies the dominant bottleneck (CPU, memory, or I/O) using clear thresholds, and writes both a human-readable report and summary.csv of key metrics. Validate by reproducing three synthetic loads (stress-ng for CPU and memory; fio for disk) and ensure the script labels each correctly and prints tuning hints (e.g., swappiness/read-ahead adjustments) for the detected bottleneck.', NULL, NULL),
('System Setup & Configuration', 'System Monitoring & Diagnostics', 'Resource Monitoring', 'Install and enable sysstat, then create a monitor.sh that samples CPU (user/system/iowait), memory (free/swap), and disk I/O (%util, await) every 5s for 3 minutes using vmstat, iostat, and ps, logging to /var/log/bench-monitor/. After completion, emit a summary.txt flagging thresholds (CPU >85%, iowait >10%, disk %util >80%), listing top 5 CPU and memory processes, and providing brief tuning recommendations.', NULL, ARRAY['logging']),
('System Setup & Configuration', 'User & Access Management', 'Authentication & Access Control', 'Configure OpenSSH to accept only user certificates signed by a local CA, using TrustedUserCAKeys and an AuthorizedPrincipalsFile restricted to the principal ''ci-runner''. Generate the CA and client certs, add a RevokedKeys list for a second cert, and verify cert-based login succeeds while password and plain public key auth are denied.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'Authentication & Access Control', 'Configure OpenSSH to require certificate-based user authentication using a locally generated OpenSSH CA, with TrustedUserCAKeys set and PasswordAuthentication/PubkeyAuthentication disabled. Create one valid signed client cert and one revoked cert via RevokedKeys, then verify only the non-revoked certificate can log in.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'Authentication & Access Control', 'Configure OpenSSH to use an internal user CA for certificate-based logins, creating and trusting a CA key, generating and signing a client key for a new devops user, and disabling password and plain public-key authentication for that account via a Match block on port 2222. Add a least-privilege sudoers policy so devops can run only systemctl status and journalctl without a password, and verify both successful cert login and expected failures for password/unsigned keys.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'Authentication & Access Control', 'Set up OpenSSH certificate-based authentication by creating a local SSH CA, signing both the server host key and a user key, and configuring sshd to trust the CA via TrustedUserCAKeys while disabling raw public-key logins. Define two principals (admin and deploy) with per-principal restrictions (e.g., ForceCommand for deploy and full shell for admin) and verify that only a signed certificate with the correct principal can log in while unsigned or wrong-principal attempts are denied.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'Authentication & Access Control', 'Set up OpenSSH to use a user certificate authority for authentication: generate a CA keypair, sign a user’s key with a principal, configure sshd with TrustedUserCAKeys and AuthorizedPrincipalsFile, and disable password/non-cert public key logins. Verify the user can SSH to localhost using the signed certificate while ordinary keys and passwords are rejected.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'File Ownership & ACL Management', 'Create a collaborative workspace at /data/projects owned by group dev with setgid, where POSIX ACLs grant dev rwX on all current and future content and interns r-X only, while a subdirectory /data/projects/secret explicitly denies user bob all access regardless of group membership. Ensure default ACLs and the mask preserve intended permissions and inheritance for newly created files and directories.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'File Ownership & ACL Management', 'Create a multi-purpose project area at /srv/projects/acme with three subdirectories using POSIX ACLs: incoming as a write-only ''blind dropbox'' (root:submitters, 1733, sticky bit) where only file owners and the reviewers group can read their own uploads; shared as a setgid devs workspace where new files inherit devs:rwx and qa:r-x; and secrets owned by svc-ci where only svc-ci has rwx and leads have r-x, others denied. Ensure inheritance with default ACLs and mask tuning, verify effective permissions with getfacl and by creating test files as representative users.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'File Ownership & ACL Management', 'Create a multi-team workspace in /srv/work using POSIX ACLs, default ACLs, setgid directories, and sticky bits so new files inherit the correct group and permissions: alpha/ and beta/ are rwX for their teams, read-only to the other team, ops has rwX everywhere, dropbox/ is write-only for all except owners and ops, and secrets/ is ops-only. Validate effective rights and inheritance by creating test files as alice, bob, carol, and an ops user via su, and confirming expected access with getfacl and targeted read/write attempts.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'File Ownership & ACL Management', 'Create a multi-tenant /data/workspace with subdirectories shared, dropbox, and pii using POSIX ACLs plus setgid/sticky bits to enforce role-specific access (dev/audit/intern): shared RWX for dev+audit with interns read-only, dropbox write-only for interns with sticky, and pii RWX for dev, read-only for audit, no access for interns. Configure default ACLs so new content inherits correctly and adjust the ACL mask to demonstrate effective permission reduction where required.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'File Ownership & ACL Management', 'Provision a collaborative workspace at /srv/projects/alpha with users (alice, bob, charlie) and groups (dev, qa), enforcing via POSIX ACLs and setgid that dev has rwX, qa has r-X, while a secrets subdirectory is accessible only to alice. Configure default ACLs so new files inherit these rights, set the sticky bit on an uploads subdirectory, and validate effective permissions with getfacl and test file operations.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'User & Group Administration', 'Bulk onboard users from /app/new_hires.csv by creating accounts with per-user private groups, specified shells, home directories seeded from /etc/skel, and installed SSH authorized_keys; enforce password expiry on first login and set default umask to 027 for new users. Create dev and ops groups with least-privilege sudo rules in /etc/sudoers.d and emit a JSON report summarizing each user’s UID, group memberships, home permissions, and sudo access.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'User & Group Administration', 'Implement project-based access: create groups alpha and beta and users alice, bob, and carol with specific memberships; configure /srv/{alpha,beta} with SGID and default ACLs so team members have write access and others read-only. Migrate legacy user ''build'' to a new UID/GID and home at /srv/users/build preserving ownership across the filesystem, then offboard ''bob'' by disabling login and archiving his home to /archive with correct permissions.', NULL, NULL),
('System Setup & Configuration', 'User & Access Management', 'User & Group Administration', 'Provision role-based local accounts with granular sudo and login constraints. Create groups dev, ops, and wheel and users alice (dev), bob (ops), and backup (service); configure sudoers so ops can run systemctl and journalctl without a password, only wheel may su to root, dev may only run docker via sudo, set backup’s shell to /usr/sbin/nologin with no home, and ensure new users inherit a custom /etc/skel.', NULL, ARRAY['docker']),
('System Setup & Configuration', 'User & Access Management', 'User & Group Administration', 'Provision users dev-amy and dev-bob with private groups and homes under /srv/home (0750) from a custom /etc/skel, create groups eng and contractors, and configure /srv/eng/share setgid with default ACLs so eng has rwx and contractors r-x (inherit). Require both users to change passwords on first login and set dev-bob’s account to expire in 30 days.', NULL, NULL),
('System Setup & Configuration', 'Virtualization & Containerization', 'Container Setup & Management', 'Build a Docker Compose stack with a non-root Python web app and a Redis service on a custom user-defined bridge network (with service DNS aliases), using a named volume for Redis persistence and a bind-mounted read-only app config. Harden the containers (read-only rootfs, drop all capabilities except NET_BIND_SERVICE, no-new-privileges), add HTTP healthchecks, CPU/memory limits, and restart policies; expose the app on host port 8081 and verify it can round-trip keys through Redis.', 'hard', ARRAY['docker', 'python', 'web', 'redis']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Container Setup & Management', 'Build and run a Docker Compose stack where a custom Flask API image connects to a Redis container over a user-defined bridge network with aliases, and Redis uses a named volume for persistent storage. Expose the API on host port 8080, add healthchecks and a dependency on Redis readiness, then verify via curl that a request counter persists across restart cycles.', NULL, ARRAY['docker', 'api', 'redis', 'container']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Container Setup & Management', 'Deploy a private Docker Registry v2 on localhost with storage in /srv/registry, secured with self-signed TLS and HTTP basic auth, and configure the host (certs and /etc/hosts) so the Docker client can trust and authenticate to it. Build, tag, and push a custom hello-web image to the registry, then run it from the registry and verify it serves the expected page on port 8081.', NULL, ARRAY['docker', 'web']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Container Setup & Management', 'Deploy a private, TLS-enabled Docker Registry with basic auth backed by a persistent volume, reachable at https://registry.local:5000 on a custom bridge network. Generate a self-signed CA and server cert, install the CA for the Docker client, docker login, build and push a labeled test image, then pull it to verify digest and label integrity.', NULL, ARRAY['docker']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Container Setup & Management', 'Provision Docker Buildx with QEMU emulation, run a local registry at 127.0.0.1:5000, and build a multi-architecture image (linux/amd64, linux/arm64) for the app in /app/demoapp. Push the manifest list to the local registry, run the amd64 variant locally exposing port 8080, and verify it responds with ''Hello, multi-arch''.', NULL, ARRAY['docker']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Image Building & Exporting', 'Assemble a minimal Debian-based cloud image from scratch using debootstrap, package it into a bootable qcow2 with ext4 and GRUB, and enable cloud-init. Export the image to /app/images/debian-cloud.qcow2 and verify it by booting with QEMU using a NoCloud seed that creates a known user and hostname.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Image Building & Exporting', 'Build a bootable Debian 12 cloud-init image from scratch by creating a minimal rootfs with debootstrap, adding kernel/initramfs, OpenSSH, and cloud-init, then installing GRUB into a qcow2 disk and compressing it to /app/images/debian12-cloud.qcow2.zst with a sha256 checksum. Generate a NoCloud seed ISO that provisions user tb with an SSH key, and verify by booting the qcow2 in QEMU with host port 2222 forwarded and successful SSH login.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Image Building & Exporting', 'Build a bootable Ubuntu 22.04 cloud image (qcow2) from scratch using debootstrap, installing cloud-init and OpenSSH, and configuring GRUB for virtio on /dev/vda. Generate a NoCloud seed ISO that creates a sudo user ''tb'' on first boot, then export both artifacts to /app/images/ for reuse.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Image Building & Exporting', 'Build a customized Ubuntu 22.04 cloud qcow2 image that preinstalls Nginx and a systemd healthcheck service, and generate a NoCloud cloud-init seed ISO that creates a ''bench'' user with SSH key and enables the service. Export the compressed qcow2 and seed ISO to /app/images with SHA256SUMS, then verify via QEMU that the VM boots and serves a /health endpoint reachable on host port 8081 via user-mode port forwarding.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Image Building & Exporting', 'Create a minimal Debian qcow2 VM image from scratch using debootstrap, configuring fstab, GRUB for serial console, a non-root tb user with SSH access, and cloud-init. Export the qcow2 and a cloud-init seed ISO, then boot with QEMU to verify SSH connectivity via host port forwarding.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Virtual Machine Provisioning', 'Provision an Ubuntu VM with QEMU/KVM using an official cloud image and a NoCloud (cloud-init) seed that creates a user, installs Nginx, and exposes SSH and HTTP via host port forwards. Use a read-only base qcow2 with per-boot ephemeral overlays, verifying SSH access and that the Nginx default page is reachable from the host.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Virtual Machine Provisioning', 'Provision an Ubuntu cloud-image VM with QEMU and cloud-init (NoCloud) to create a tb-admin user with a provided SSH public key, preinstall nginx, and serve a custom index.html reachable via host port 8080. After verifying SSH and HTTP access, create a named external qcow2 snapshot, change the site, then revert to the snapshot to demonstrate full VM state rollback.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Virtual Machine Provisioning', 'Provision an Ubuntu cloud-image VM with QEMU/KVM using a NoCloud cloud-init seed that creates a tb-admin user with SSH key authentication, sets the hostname, and installs/enables nginx on first boot. Forward host 127.0.0.1:8080 to the VM’s port 80, verify the default nginx page is reachable, and leave the VM running headless.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Virtual Machine Provisioning', 'Provision an Ubuntu cloud-image VM with QEMU/KVM using a NoCloud cloud-init seed that creates a tbuser with an SSH key, enables the qemu-guest-agent, and runs a first-boot script to install and start Nginx serving a custom index.html. Forward host ports 2222->22 and 8080->80, verify SSH and HTTP from the host, and store the qcow2 disk and seed ISO under /app/vm.', NULL, ARRAY['cloud']),
('System Setup & Configuration', 'Virtualization & Containerization', 'Virtual Machine Provisioning', 'Provision an Ubuntu cloud-image VM with libvirt/KVM using cloud-init to create user ''tb'' (password ''benchmarkpass''), inject an SSH public key, set hostname tb-vm, and enable qemu-guest-agent. Create an isolated libvirt network with a static VM IP and host port-forwarding 2222->22, autostart the VM, and verify SSH connectivity and hostname via a test script.', NULL, ARRAY['cloud']);
