-- Compact batch 14/29: rows 651-700

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement a minimal ZeRO Stage-2 optimizer in PyTorch that shards Adam states and gradients across ranks, using reduce-scatter for gradient partitioning and on-demand all-gather of parameters for forward passes. Train a small model on synthetic data and verify numerical parity with an unsharded baseline, correct save/load of sharded checkpoints, and peak memory reduction for world_size 1, 2, and 4.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement vocabulary-parallel sharding for a tiny Transformer language model in PyTorch by partitioning the token embedding and tied LM head across ranks, producing only local logits and using an all-reduce to compute the global softmax cross-entropy loss. Support world sizes 1, 2, and 4, include sharded checkpoint save/load with resharding across different world sizes, and verify numerical parity with a non-sharded reference model.', 'hard', ARRAY['parallel', 'pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Build a CPU-only PyTorch FSDP training pipeline launched via torchrun (world_size=2–4) that auto-wraps a small Transformer, uses DistributedSampler and gradient accumulation to maintain a fixed global batch, and logs rank-local plus aggregated metrics. Save a sharded checkpoint (model and optimizer) and implement resume-on-different-world_size, verifying determinism by matching validation loss and a consolidated state_dict against a single-process baseline.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Build an elastic PyTorch DDP training launcher that tolerates worker failures and dynamic membership (2–4 ranks), re-forming the process group via a rendezvous backend and resuming from rank-0 sharded checkpoints even when world size changes. The test will start/kill workers mid-epoch; the job must continue training to a target accuracy and emit a single consolidated checkpoint at the end.', 'hard', ARRAY['pytorch', 'backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Launch a CPU-only PyTorch Elastic DDP job (torchrun) with 4 ranks across two localhost ''nodes'' that trains a tiny transformer on synthetic data using the gloo backend, writes sharded per-rank checkpoints, then restarts from the latest checkpoint after simulating a rank failure. Consolidate the shards into a single checkpoint and verify equivalence to a single-process baseline by hashing model parameters.', 'hard', ARRAY['pytorch', 'backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Launch a PyTorch FSDP training job for a small Transformer on synthetic sequence data across 4 local ranks with torchrun, enabling mixed precision and activation checkpointing, and write sharded checkpoints plus a script to consolidate them into a single state_dict.pt. Provide a baseline DDP run and verify that resuming from the sharded checkpoint reproduces validation loss within tolerance and that peak memory per rank is at least 30% lower than DDP.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Distributed & Parallel Training Infrastructure', 'Launch an elastic Horovod CPU training run (Gloo backend) on synthetic data that tolerates worker restarts and dynamic scaling, preserving optimizer state and the learning-rate schedule across changes. Begin with 2 workers, add a third after 50 steps, then remove one, and produce a metrics.json with per-step throughput and effective global batch size plus a final saved model.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Convert a Hugging Face BERT-base classifier to ONNX, build an INT8 static-quantized variant from calibration data, and serve both in an NVIDIA Triton model repository (ONNX Runtime backend, CPU) with dynamic batching and multiple instances. Implement a concurrent client to benchmark p50/p95 latency and throughput at batch sizes 1/8/32 and assert the INT8 model delivers ≥1.4x throughput with ≤1% accuracy delta, saving a metrics.json.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Deploy a BERT-base encoder in NVIDIA Triton with two TensorRT backends (FP16 and INT8 calibrated from /app/calib), enable dynamic batching and concurrent instance groups, and write a Python client that drives variable-length requests over HTTP to collect p50/p95 latency and throughput. Validate that the INT8 engine achieves at least 1.8x throughput over a PyTorch eager baseline on the same hardware and emit a JSON report to /app/metrics.json.', 'hard', ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Export a BERT-base QA model to ONNX, build both FP16 and INT8 TensorRT engines (with calibration), and deploy them in NVIDIA Triton with dynamic batching and two concurrent instances. Benchmark with perf_analyzer over batch sizes {1, 8, 32} and write a JSON summary of latency/throughput and INT8 vs FP16 speedups to /app/perf_report.json.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Export a HuggingFace BERT-base encoder to ONNX, build FP16 and INT8 TensorRT engines with entropy calibration, and deploy both behind NVIDIA Triton Inference Server with a model repository configured for dynamic batching and multiple concurrent instances. Provide a CLI load generator to send large-batch requests, verify output parity vs. PyTorch within tolerance, and report throughput to demonstrate INT8 speedup over FP16.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Inference Optimization at Scale', 'Stand up a CPU-only Triton Inference Server hosting an INT8-quantized DistilBERT ONNX model via the ONNX Runtime backend, configured with dynamic batching and multiple instances, then load-test batch sizes {1, 8, 32, 128}. Validate outputs against an FP32 baseline within tolerance and write a JSON report summarizing per-batch throughput plus p50/p99 latency scraped from Triton’s /metrics endpoint.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Build a CPU-only QLoRA fine-tuning pipeline for a small Hugging Face causal LM that can switch between LoRA and prompt-tuning via a CLI flag, training on /app/data/train.jsonl with sequence packing and evaluating perplexity on /app/data/valid.jsonl. Save adapter weights to /app/adapter and a merged full model to /app/merged, and write the final validation perplexity to /app/perplexity.txt.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Fine-tune a small Hugging Face causal LM using LoRA to produce two domain-specific adapters from tiny corpora, then implement on-the-fly weighted adapter merging and a CLI to evaluate perplexity across domains. Verify that each adapter beats the base model on its domain and that online merged logits match an offline-merged checkpoint within a small tolerance.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Fine-tune a small causal LM (e.g., GPT-2) with PEFT to produce two separate LoRA adapters on two distinct domains (e.g., prose and code) in a CPU-only setting. Implement inference-time weighted composition of the adapters to blend or route behaviors, then save individual/composed adapters and a merged variant, and report a metrics JSON comparing cross-entropy/perplexity across domains and blends.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Implement a soft prompt-tuning pipeline for a decoder-only LLM (e.g., distilgpt2) that learns K virtual tokens while freezing all base weights. Provide scripts to train on data/, save/load the prompt embeddings, verify perplexity improvement on a held-out split, and generate completions for prompts.txt into outputs.jsonl with deterministic seeds.', NULL, NULL),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Large Model Fine-Tuning & Adaptation', 'Implement soft prompt tuning for a small causal LM (e.g., distilgpt2) to learn k virtual tokens for reversible date format conversion on a tiny synthetic dataset, keeping the base model frozen. Save the learned prompt vectors separately and provide an inference script that attaches them to the base model to generate outputs for test inputs, writing predictions to /app/date_convert.json.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Build a Python CLI that runs an offline Weights & Biases sweep of 8 trials for a scikit-learn imbalanced binary classifier, performing 5-fold CV per trial and logging per-fold metrics, aggregated results, confusion matrices, and the best model as artifacts under a single grouped sweep. Enable run resumption, and export a self-contained wandb-export.tar.gz with all offline run data to /app/wandb_export plus a best_trial.json summarizing chosen hyperparameters and metrics.', NULL, ARRAY['python', 'logging']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Configure and execute a Weights & Biases hyperparameter sweep in offline mode that trains a scikit-learn model with StratifiedKFold, logging hyperparameters, per-fold AUC/accuracy, ROC curves, and saving the fitted estimator as a W&B artifact for each run. After the sweep completes, programmatically identify the best run by mean AUC and write its run ID, config, and artifact file path to /app/best_run.json.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Create a Python CLI that runs 5-fold cross-validation with hyperparameter search, using MLflow to record a parent run and nested child runs per trial and fold with parameters, metrics, and artifacts (ROC/PR plots, confusion matrices, and serialized models). The CLI must support resuming by skipping completed child runs, produce an aggregated leaderboard.csv, and register the best model with signature and input example in the Model Registry.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Deploy an MLflow tracking server with a SQLite backend and a MinIO S3 artifact store behind an Nginx reverse proxy on localhost, then build a training pipeline that uses nested runs and autologging for both scikit-learn and PyTorch, logs SHAP artifacts, a model signature, and an input example, and registers two models with stage transitions. Add a script that queries MLflow for the best run by a validation metric, downloads its artifacts to run batch inference, logs predictions as a new artifact, and prints the chosen run_id.', NULL, ARRAY['backend', 'pytorch']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Experiment Tracking & Logging', 'Implement a resumable training pipeline that uses MLflow to start a run, log hyperparameters and per-epoch metrics, save checkpoints, then intentionally crashes mid-training. On re-execution it must detect state and resume the exact same run_id (start_run with run_id), continue logging without duplicating epochs, record environment snapshots (pip freeze and git commit) and a learning-curve artifact, and write the run_id to /app/run_id.txt.', NULL, ARRAY['logging', 'git']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Build a local, file-backed model registry with a SQLite metadata store and content-addressable artifact storage, including a CLI to register versions, assign stage aliases (staging/production), and promote or rollback based on evaluation metrics. Provide an inference loader that consumes a lockfile to fetch a pinned version and verifies immutability and lineage.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Create a Python CLI that publishes trained models as OCI artifacts to a local Docker registry (registry:5000), storing model weights and a metadata.json layer with code/data hashes, metrics, and dependency manifest. Support semver tags and aliases (staging/production), integrity verification by content digest, version comparison by a chosen metric, and atomic promote/rollback of the production alias.', NULL, ARRAY['python', 'docker']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Create a schema-aware local model registry that assigns semantic versions (MAJOR/MINOR/PATCH) when registering scikit-learn models by diffing input schema, hyperparameters, and data hashes. Provide a CLI to register, list, and promote versions with stage transitions and a ''champion'' alias, then verify by loading the Production model to score a held-out set.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Model Registry & Versioning', 'Start a local MinIO S3 server on localhost:9000 and configure a DVC remote (s3://model-registry) to serve as a centralized model registry for versioned model artifacts and metrics. Train two scikit-learn models, push both versions with semantic tags (e.g., v0.1.0, v0.2.0), implement a promote.py CLI to label Staging/Production in a bucket-hosted registry.json, and demonstrate a rollback to the prior Production version.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Build a Prefect 2.x flow and deployment that orchestrates an end-to-end ML pipeline with ETag-aware data ingestion, feature engineering, model training/evaluation, and conditional promotion; configure retries, result persistence/caching, and a cron schedule, ensuring re-runs skip unchanged tasks. Provide a CLI to register the deployment, start a worker, trigger a run, and emit versioned artifacts plus a run_report.json summarizing cache hits, timings, and the promotion decision.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Build an Apache Airflow DAG that watches /app/data/incoming with a FileSensor, validates new CSVs using Great Expectations, then transforms, trains, and evaluates a scikit-learn model on accepted data, writing artifacts and metrics under /app/outputs/{{ ds }}. The DAG must use the TaskFlow API with XCom to pass artifact paths, support backfilling for the past 7 days, and run on a daily 02:00 schedule.', NULL, ARRAY['api']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Create a Prefect 2 flow and deployment that ingests new CSVs, computes feature drift against a stored baseline, and conditionally runs preprocessing, training, evaluation, and model registration only when drift exceeds a threshold. Enable local caching and retries, add a cron schedule, and persist run artifacts and the chosen model under /app.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Create an Apache Airflow DAG that monitors /app/current.csv, computes data drift against /app/reference.csv with Evidently, and branches to either retrain/evaluate a scikit-learn model or skip retraining when drift is below a threshold. The DAG must be scheduled daily with retries and SLAs, use XCom to pass metrics, persist artifacts to /app/, and implement a file-content hash to cache and skip redundant training.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Pipeline Construction & Scheduling', 'Create an Apache Airflow DAG that uses dynamic task mapping to run k-fold cross-validation (download/validate data, per-fold training, metric logging, aggregation) and conditionally register the model only if the averaged score exceeds a threshold. Package it for a local Dockerized Airflow setup, schedule it to run daily, ensure idempotent runs, and persist all artifacts/metrics under /app/artifacts.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a Dockerized ML pipeline that achieves bit-for-bit reproducibility by using a conda-lock lockfile, fixing locale/timezone, seeding all RNGs, and pinning BLAS thread counts, while prefetching wheels and dataset artifacts for fully offline execution. Train a scikit-learn model and write both metrics and a SHA256 of the serialized model; rerunning the container (including offline) must produce identical outputs.', NULL, ARRAY['container']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a Dockerized, conda-lock–pinned environment that runs a fully deterministic scikit-learn training pipeline on /app/data.csv, fixing seeds and BLAS threads to yield identical artifacts across runs. The CLI must produce model.pkl, metrics.json, and a reproducibility_report.json capturing package versions, BLAS backend, thread config, and SHA256 checksums of code/lockfile/data, with a make target that verifies bit-for-bit reproducibility.', NULL, ARRAY['backend']),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a Dockerized, fully reproducible ML pipeline using DVC stages (data prep → feature extraction → training) with a conda-lock generated environment and pinned pip extras. Verify that running dvc repro in two clean clones yields bit-for-bit identical model and metrics artifacts and emit a manifest with exact package versions and data checksums.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Build a fully offline, deterministic ML pipeline using DVC stages (data -> features -> train -> eval) and a Dockerized micromamba environment created from a conda-lock lockfile, training a scikit-learn LogisticRegression on synthetic data with fixed seeds. Provide scripts/targets to run dvc repro and verify reproducibility by asserting identical SHA256 checksums of model and metrics artifacts across two consecutive runs.', NULL, NULL),
('Machine Learning & AI', 'Machine Learning Pipelines & Automation', 'Reproducibility & Environment Setup', 'Create a Nix flake that defines a hermetic Python 3.11 environment with pinned NumPy, pandas, and scikit-learn, plus a CLI that trains a logistic regression on /app/data.csv with fixed seeds. Add a Makefile target that builds the flake and runs the training twice in fresh pure shells, asserting identical SHA256 hashes for model.pkl and metrics.json to confirm bit-for-bit reproducibility.', NULL, ARRAY['python', 'pandas']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'Build a CPU-only benchmarking CLI that compares FP32, FP16, and INT8 (quantized) variants of a given image classifier on a provided dataset, reporting accuracy, Brier score, and p50/p95 per-sample latency. Output a leaderboard CSV and select the best configuration by maximizing accuracy under a 95th-percentile latency budget, saving the chosen tag to /app/selection.txt.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'Create a reproducible CLI benchmarking harness that trains and evaluates at least three scikit-learn classifiers on every CSV dataset in /app/data using nested stratified k-fold CV, computing accuracy, ROC-AUC, F1, log-loss, Brier score, and calibration error with 95% bootstrap confidence intervals. Aggregate results across datasets with paired Wilcoxon tests and effect sizes to rank models, also measuring CPU inference latency on fixed batch sizes and exporting a single results.json with per-dataset metrics, CIs, significance, and an accuracy–latency Pareto summary.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'Create a reproducible benchmarking harness that trains five scikit-learn classifiers across four built-in datasets with repeated stratified k-fold, logging ROC-AUC, log loss, Brier score, per-sample latency, and model size to a results file. Run a Friedman test with Nemenyi post-hoc to rank methods, emit a critical-difference diagram, and pick a champion model that satisfies latency and size thresholds.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Benchmarking & Comparison', 'On an imbalanced binary dataset in /app/data.csv, train two baselines (Logistic Regression and Gradient Boosting) and calibrate each with temperature scaling and isotonic regression using a validation split. Benchmark pre/post-calibration AUROC, AUPRC, accuracy, Brier score, NLL, and ECE with bootstrap 95% CIs, then output a metrics CSV and a JSON naming the best-calibrated variant that keeps accuracy within 1% of the uncalibrated best.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Build a Python CLI that runs repeated StratifiedGroupKFold cross-validation to produce out-of-fold predictions for two classifiers, with options for class-weighting to handle imbalance. Compute ROC-AUC/PR-AUC means and bootstrap CIs, apply the Nadeau–Bengio corrected resampled t-test and McNemar’s test to assess performance differences, and write a summary report to /app/results.json.', NULL, ARRAY['python', 'performance']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Create a Python CLI that loads /app/adult.csv and runs nested stratified 5x2 cross-validation comparing tuned logistic regression and tuned random forest on F1 and PR-AUC, then applies Dietterich''s 5x2cv paired t-test and bootstrap 95% CIs for metric differences, writing /app/fold_predictions.csv and /app/eval_report.json with p-values, effect sizes, and CIs. Use fixed seeds for reproducibility, handle class imbalance, and hard-fail if variance across folds is zero.', 'hard', ARRAY['python']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Create a Python CLI that performs nested, stratified GroupKFold cross-validation (outer k=5, inner k=3) to tune a logistic regression on an imbalanced dataset with strict group-leakage prevention, aggregating out-of-fold macro-F1 and BCa bootstrap 95% confidence intervals. Implement the Nadeau–Bengio corrected resampled t-test to compare the tuned model against a majority-class baseline and report Holm–Bonferroni-adjusted p-values.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Implement a CLI that compares two classifiers on /app/data.csv using nested stratified 5x2 cross-validation, reporting ROC-AUC and F1 per fold. Assess significance with Dietterich’s 5x2cv paired t-test and DeLong’s AUC test, and output a reproducible JSON containing fold scores, BCa bootstrap 95% CIs, and overall p-values.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Cross-Validation & Statistical Testing', 'Implement a CLI that runs Dietterich’s 5x2 cross-validation paired t-test to compare LogisticRegression and RandomForest on the scikit-learn breast cancer dataset with stratification and fixed seeds. Aggregate out-of-fold predictions to compute ROC AUC and Average Precision with bootstrap 95% CIs, and write per-fold metrics, the t statistic, p-value, and a significance verdict to results.json.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Build a CLI that ingests a CSV of binary classifier outputs (y_true, y_score, group) and generates reliability diagrams with bootstrap CIs, ROC/PR curves, and score histograms, saving plots to /app/eval. It must choose a decision threshold that minimizes expected cost from user-specified FP/FN costs, then emit per-group confusion matrices and a summary JSON reporting ECE, MCE, Brier, AUROC/AUPRC, and the worst-affected groups.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Build a CLI that ingests multiclass predictions (logits or probabilities) and ground-truth labels, computes calibration metrics (ECE/MCE, class-conditional ECE, Brier, NLL) with bootstrap confidence intervals, fits temperature scaling on a validation split, and recomputes metrics post-calibration. Output adaptive-binning reliability diagrams, confidence histograms, and per-class normalized confusion matrices highlighting top confusable pairs, and save a concise markdown report linking all figures.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Build a CLI that ingests multiclass softmax predictions and ground-truth labels from CSVs in /app/preds/{train,val,test} (optionally including a ''group'' column) and generates per-group confusion matrices and reliability diagrams. Fit temperature scaling on the validation split, then save before/after ECE and Brier scores, interactive HTML plots, and a JSON metrics summary to /app/report.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Create a Python CLI that evaluates multivariate time-series forecasts against ground truth, computing per-horizon sMAPE, MASE, and prediction-interval coverage, and generates horizon-wise error heatmaps plus overlay plots of predictions vs. actuals for the worst 10 series by MASE. Write a metrics.json and PNG plots to /app/output while streaming to handle >1e6 points with numerically stable aggregations.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Error Analysis & Visualization', 'Implement a CLI tool that ingests a CSV of predictions (y_true, y_pred, per-class probabilities) plus optional metadata columns and performs comprehensive error analysis: confusion matrix, per-class PR curves, reliability diagram/ECE, and per-slice metrics for each metadata field. Identify the top-3 statistically significant failure slices via bootstrap, then export PNG plots and a JSON report listing highest-confidence false positives/negatives and an optimal threshold per class.', NULL, NULL);
