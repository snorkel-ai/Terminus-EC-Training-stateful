-- Compact batch 15/29: rows 701-750

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Build a CLI evaluator for named entity recognition that parses BIO/BILOU tag sequences, extracts spans, and computes strict and partial-overlap F1, per-entity-type metrics, and micro/macro aggregates. Handle mismatched sequence lengths and invalid tag transitions with robust decoding, output a metrics.json plus a span_errors.csv highlighting boundary and type confusions.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Build a CLI that ingests CSVs of true labels and model logits/probabilities for in-distribution and OOD samples, fits a single temperature on a validation split, and computes accuracy, macro-F1, ROC-AUC, PR-AUC, NLL, Brier score, ECE/MCE (fixed and adaptive bins), and OOD AUROC/AUPR/FPR@95 using MSP and energy scores. Output a metrics.json and per-class.csv plus reliability diagrams and score histograms with strict input validation and numerically stable computations.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Create a CLI that ingests a CSV of multiclass prediction probabilities and ground-truth labels, computes macro/micro F1, top-1/top-5 accuracy, per-class precision/recall, confusion matrix, and calibration metrics (ECE with adaptive binning, MCE, Brier), and writes a JSON summary plus per-class CSV. Include 1,000-sample bootstrap confidence intervals for scalar metrics and save a reliability diagram and normalized confusion matrix as PNGs.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Create a CLI that ingests a Parquet file containing multilabel ground-truth indicators and model score columns, learns per-label thresholds on a validation split to maximize macro-F1, then evaluates the test split with those thresholds. Output a JSON report with per-label precision/recall/F1, micro/macro averages, LRAP, coverage error, Jaccard index, and the chosen thresholds, and write per-label 2x2 confusion matrices to CSV.', NULL, NULL),
('Machine Learning & AI', 'Model Evaluation & Validation', 'Metric Computation & Reporting', 'Ingest /app/logits.csv (Nx5 logits) and /app/labels.csv (N class ids), fit temperature scaling to calibrate probabilities, and compute top-1/top-5 accuracy, NLL, ECE (15 bins), Brier score, and macro-F1 before and after calibration with 95% bootstrap confidence intervals. Write a JSON summary to /app/report.json and save a before/after reliability diagram to /app/reliability.png.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Build a CPU-only FastAPI inference service that loads a scikit-learn model from /app/model.pkl, performs dynamic batching by aggregating requests for up to 50 ms before a single model call, and hot-reloads the model when the file changes without dropping in-flight requests. Provide a batch_infer.py CLI that reads /app/input.csv, runs identical pre/post-processing for batched predictions, and writes results to /app/preds.csv including a model_version column.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Build a dual-mode inference app around an ONNX image classifier using onnxruntime: a CLI that runs batched CPU inference over all images in /app/images and writes predictions.csv, and a lightweight HTTP server exposing /predict for single-image requests with lazy, thread-safe model loading and unified preprocessing. Implement optional dynamic batching on the server (short timeout window) and enable onnxruntime CPU optimizations for consistent, reproducible outputs.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Export a pretrained torchvision MobileNetV2 to ONNX, then implement an ONNX Runtime-powered FastAPI endpoint for online top-5 ImageNet predictions and a batch script that scores all images in a directory. Include optional dynamic quantization and write JSONL predictions plus latency/throughput metrics for the batch run.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Batch & Online Inference', 'Export a pretrained torchvision ResNet-18 to ONNX and implement a CPU-only FastAPI service backed by ONNX Runtime that supports single-image and dynamic micro-batched inference (≤50 ms window). Provide a CLI for offline batch predictions over a folder, saving top-5 classes per image and recording throughput and p95 latency to /app/metrics.json for both online and batch modes.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Build a CPU-only FastAPI inference server for a torchvision ResNet-18 and implement a background queue that performs dynamic micro-batching (coalesce requests for up to 16 images or 10 ms) with a single forward pass using a TorchScript-compiled model and tuned num_threads. Provide a CLI load generator to compare baseline (no batching) versus micro-batched serving and write p50/p95 latency and requests/sec to /app/results.json.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Build a CPU-only FastAPI service that serves a DistilBERT sentiment classifier via ONNX Runtime, then add three optimizations: static INT8 quantization with calibration, adaptive micro-batching (max batch 16, 10ms timeout), and an LRU cache of tokenized inputs keyed by content hash. Provide a benchmark script that drives concurrent requests and outputs a JSON report comparing p50/p95 latency and throughput before vs after, with a required ≥1.5× throughput and ≥25% p95 latency improvement to pass.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Build an async Python inference server for a small Hugging Face Transformer that adds micro-batching (time-windowed), dynamic int8 quantization, and a tokenizer cache. Provide a replay benchmark that outputs baseline vs optimized p50/p95 latency and throughput to a JSON report.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Containerize a FastAPI inference service for a small Transformer text classifier that implements a background request queue with dynamic micro-batching and an LRU tokenizer cache, and provide an ONNX Runtime int8-quantized variant. Use a load generator to measure QPS and p50/p95 latencies for fp32, int8, and ''int8+batching+cache'' modes, writing a comparison summary to /app/bench.json.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Latency & Throughput Optimization', 'Create a CPU-only FastAPI inference server for a pretrained DistilBERT classifier that implements time-windowed dynamic batching, int8 dynamic quantization of Linear layers, and a content-hash response cache; include a short warm-up and pad inputs to multiples of 8 tokens. Supply a load generator that compares baseline vs optimized builds and outputs p50/p95 latency and throughput, with tests requiring at least 1.5x throughput improvement without violating a p95 latency SLA.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Export a PyTorch sequence model to both TorchScript (scripted) and ONNX with dynamic axes and opset 17, saving weights in safetensors with strictly pickle-free serialization. Provide a CLI that loads the exported artifacts on CPU, runs onnxruntime and TorchScript inference on the same inputs, and asserts numerical parity within a tight tolerance.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Export a scikit-learn Pipeline that includes a custom categorical encoder and a RandomForest model to ONNX by implementing a custom converter and shape calculator, then verify output parity against the original pipeline with onnxruntime on a mixed-type dataset. Save the portable ONNX and a small parity report summarizing numerical differences and opset/ir metadata.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Export the ''sentence-transformers/all-MiniLM-L6-v2'' PyTorch encoder to ONNX with dynamic batch and sequence axes and also produce an int8-quantized ONNX using onnxruntime. Provide a CLI that tokenizes sentences from /app/input.txt, runs PyTorch vs ONNX (fp32 and int8), checks cosine-similarity parity (1e-3/1e-2), and writes a JSON report with model sizes and average latency.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Implement a CLI that builds a variable-length batched BiLSTM tagger in PyTorch using PackedSequence, exports it to TorchScript and ONNX (opset 17) with dynamic batch and sequence length, and provides an ONNX-compatible unpacking path. Validate numerical parity across eager PyTorch, TorchScript, and ONNX Runtime on randomized inputs, saving all artifacts and a sample input under /app/export.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Inference & Serving', 'Model Export & Serialization', 'Train a scikit-learn mixed-type Pipeline (ColumnTransformer with numeric StandardScaler and categorical OneHotEncoder feeding LogisticRegression), then export it to ONNX (opset ≥17) with a dynamic batch axis, explicit input dtypes, and embedded class-label metadata. Validate with onnxruntime that class probabilities and predictions match scikit-learn within a tight tolerance across a holdout CSV, and save both the ONNX artifact and a JSON parity/latency report.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Build a FastAPI microservice that serves a scikit-learn model from /app/models/current.pkl with atomic, zero-downtime hot-reloads on file change via a background watcher and readers–writer lock, exposing /predict (batch), /healthz, /readyz, and /promote endpoints. Include a script that swaps in a new model and demonstrates uninterrupted concurrent requests while logging requests and responses to SQLite.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Deploy a FastAPI microservice that serves a small ONNX sentiment classifier via ONNX Runtime with async micro-batching (max batch size and wait time), concurrency-safe tokenization, and pydantic request validation. Provide Dockerfile and startup scripts, expose /predict, /healthz, and /metrics (Prometheus) endpoints, and implement a zero-downtime hot-swap endpoint that atomically loads and switches to a new model file.', NULL, NULL),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Implement a Python gRPC inference server that loads a CPU-only ONNX Runtime model and performs dynamic micro-batching (bounded by batch size and a 50 ms queue window), exposing the gRPC health checking service and a Prometheus /metrics endpoint. Provide a CLI load generator to issue concurrent requests and write a JSON report comparing latency/QPS in batched vs unbatched modes to /app/bench.json.', NULL, ARRAY['python', 'grpc']),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Package a CPU-only ResNet18 into a TorchServe .mar with a custom handler that performs torchvision preprocessing and returns top-5 class probabilities as JSON. Launch TorchServe on 0.0.0.0:8080, register the model, support both single and batched image requests at /predictions/resnet18 with proper 400 errors for invalid inputs, and expose /ping and Prometheus /metrics for health and monitoring.', NULL, ARRAY['monitoring']),
('Machine Learning & AI', 'Model Inference & Serving', 'Serving & Deployment', 'Package a pre-trained PyTorch ResNet18 into a TorchServe .mar with a custom handler that accepts base64-encoded images, applies preprocessing, and returns top-3 labels with probabilities. Launch TorchServe with dynamic batching (e.g., max_batch_size=8), register the model via the management API, issue batched requests, and write a latency/throughput summary to /app/serve_metrics.json.', NULL, ARRAY['pytorch', 'api']),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Add LoRA adapters to a pretrained T5-small and fine-tune on a local summarization dataset in /app/data (train.jsonl, val.jsonl) while freezing all base weights, then export both an adapter-only checkpoint and a merged full model. Verify base weights are bitwise-identical pre-merge, that the merged model matches adapter outputs on a fixed prompt, and that ROUGE-L on val improves over the frozen baseline.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Fine-tune T5-small with parameter-efficient prefix tuning to translate English task descriptions into Bash one-liners on a provided dataset; evaluate exact match and BLEU on a held-out split, and save both the prefix adapter and a merged model plus predictions.json to /app.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Fine-tune a pretrained sentence-transformer (e.g., sentence-transformers/all-MiniLM-L6-v2) on provided positive/negative sentence pairs with a contrastive cosine-similarity loss to adapt it for semantic search. Provide a CPU-only CLI that trains, saves the fine-tuned encoder, and reports MRR@10 and Recall@10 on a held-out query set using exact cosine similarity over corpus embeddings.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Fine-Tuning Pretrained Models', 'Implement parameter-efficient fine-tuning by inserting lightweight adapter modules into a pretrained DistilBERT for domain text classification, training only adapters with discriminative layer-wise learning rates and a slanted triangular schedule. Provide a CLI to train/evaluate, export adapter-only weights, verify the frozen base model hash is unchanged, and support hot-swapping different adapter checkpoints at inference.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Build a CLI that performs multi-objective hyperparameter optimization with Optuna for a PyTorch CNN on Fashion-MNIST, maximizing validation accuracy while minimizing wall-clock time via ASHA pruning and a SQLite-backed study that can be resumed. After the search, pick the Pareto-optimal trial under a 30s time budget, deterministically retrain on the full training set, and export best_config.json, study.db, metrics.json, and best_model.pt.', NULL, ARRAY['optimization', 'pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Build a reproducible Optuna-powered multi-fidelity search (ASHA pruner) that tunes learning rate, weight decay, batch size, and transformer depth for a small text classifier on a provided CSV, persisting the study to SQLite. After the search, retrain the best trial and export the final model along with a Pareto front that balances validation accuracy against measured inference latency.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Build an Optuna multi-objective HPO pipeline that trains a tabular classifier (XGBoost if installed, otherwise scikit-learn GradientBoosting) with stratified K-fold CV to jointly minimize validation log loss and 95th-percentile per-sample prediction latency. Implement pruning, class-imbalance handling, and a dynamic search space conditioned on the chosen estimator, then export the selected Pareto-optimal trial’s hyperparameters, a serialized model, and a small report of the Pareto front.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Create a reproducible, multi-objective Optuna study that tunes a CPU-only PyTorch tabular classifier (layers, width, dropout, learning rate, weight decay, batch size) to simultaneously maximize ROC-AUC and minimize measured inference latency using ASHA pruning within a fixed time budget. Persist the study to a local SQLite DB, export the Pareto-optimal trials to /app/pareto.json, and save the fastest model meeting a target AUC threshold to /app/best_model.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Hyperparameter Optimization', 'Run a multi-objective Optuna study to tune a CPU-only LightGBM classifier on the UCI Adult income dataset, maximizing ROC-AUC while minimizing p95 inference latency over 10k predictions. Persist the study, write the Pareto front with hyperparameters to JSON, and export the fastest Pareto-optimal model artifact to /app.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Build a PyTorch DDP training harness that runs a small CNN on synthetic data in both single-process and 4-rank torchrun modes, auto-selecting CPU/GPU and gloo/NCCL while pinning one device per rank and using gradient accumulation with optional AMP to keep the effective batch size constant. Log images/sec per rank, global throughput, speedup vs single-process, and (if GPUs exist) per-GPU memory/utilization via nvidia-smi, saving a final JSON/CSV report.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Build a PyTorch training launcher that runs a small CNN under DistributedDataParallel across all available GPUs (falling back to multi-process CPU via gloo), automatically selecting the largest per-device microbatch via OOM-aware binary search and using gradient accumulation to hit a target global batch size. Record per-rank and averaged throughput, communication overlap vs non-overlap timings, and memory usage to a CSV, and support resuming checkpoints when world_size changes.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Implement a CPU-only pipeline-parallel trainer that splits a small Transformer into 2–4 stages across torch.distributed processes with a GPipe-style microbatch schedule, auto-falling back to single-process when world_size=1. Run via torchrun and output a JSON report comparing baseline vs pipeline throughput, per-stage latency, and memory usage.', NULL, ARRAY['parallel', 'distributed']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Implement a PyTorch DistributedDataParallel training launcher that runs on CPU (gloo) but uses CUDA if available, employs ZeroRedundancyOptimizer to shard optimizer state, and supports gradient accumulation with optional mixed precision. It must log per-rank and aggregate throughput and memory usage, save a single rank-0 checkpoint, and verify synchronized weights across ranks after each epoch.', 'hard', ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Resource Management & Parallel Training', 'Implement a PyTorch training launcher that uses torchrun to start DistributedDataParallel across all detected GPUs (falling back to multi-process CPU with Gloo), pins each rank to a device, and enables AMP on CUDA. Log per-rank throughput (images/sec) and memory stats to a shared JSONL and aggregate a final /app/throughput.json confirming that multi-GPU total throughput scales over a single process baseline.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Build a semi-supervised pipeline that uses Label Spreading to pseudo-label unlabeled samples, then trains a calibrated LogisticRegression on the union of labeled and high-confidence pseudo-labeled data and benchmarks against a labeled-only baseline. Provide a CLI for labeled fraction, confidence threshold, and seed, and write accuracy/F1/ROC-AUC plus class balance summaries to /app/metrics.json.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Implement a from-scratch Gaussian Mixture Model with EM that supports full/diagonal covariances, K-means++ initialization, and automatic component selection via BIC, then train on a large CSV to output cluster assignments and model parameters. Ensure strong numerical stability (log-sum-exp, covariance regularization) and a streaming-compatible E-step for memory-constrained environments.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Implement an EM-based Gaussian Mixture Model trainer (NumPy only) with diagonal covariances that selects the optimal number of components via BIC and enforces monotonic log-likelihood across iterations. Provide a CLI that reads a CSV, runs multiple random restarts per K, and saves the chosen K, parameters, responsibilities, and cluster labels to /app/out.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Learn a supervised metric using scikit-learn’s Neighborhood Components Analysis on labeled data, then perform k-means clustering in the learned embedding on the full dataset. Save cluster assignments and evaluate clustering quality with adjusted mutual information and silhouette scores to a metrics file.', NULL, NULL),
('Machine Learning & AI', 'Model Training & Optimization', 'Supervised & Unsupervised Learning', 'Train a sparse autoencoder in PyTorch on a numeric tabular dataset with early stopping and L1 activation regularization, saving the encoder to /app/encoder.pt. Freeze the encoder to generate embeddings and train a scikit-learn logistic regression on labels, reporting stratified 5-fold ROC-AUC and writing metrics and artifact paths to /app/results.json.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a PyTorch training loop for a stateful RNN that performs truncated backpropagation through time on a streaming text dataset, correctly carrying/detaching hidden states across chunks with global-norm gradient clipping and LR warmup+cosine decay. Add fully resumable mid-epoch checkpoints that capture optimizer/scheduler/RNG/file offsets/per-stream hidden states, plus early stopping on validation perplexity with top-k best checkpoints.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a PyTorch training loop for a tiny character-level language model on a given corpus with truncated BPTT, gradient accumulation, global-norm gradient clipping, and early stopping on validation loss. Support rotating checkpoints (keep N), exact resume of optimizer/scheduler and RNG state after interruption, and deterministic results when a seed is provided.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a PyTorch training loop that supports mixed-precision (AMP) with GradScaler, gradient accumulation, global-norm clipping, cosine LR scheduling, early stopping on validation loss, and checkpointing that rotates top-2 best models plus the latest. The loop must be fully resumable (optimizer/scheduler/AMP scaler/RNG states) after an external SIGTERM or KeyboardInterrupt, skip NaN/Inf batches safely, and produce deterministic metrics given a seed on a tiny synthetic dataset.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a custom PyTorch training loop for SimCLR-style contrastive learning on a small image dataset with mixed precision, gradient accumulation, per-step gradient clipping, cosine warmup+restarts, temperature annealing, EMA weights, and robust checkpointing (best/last with resume). Validate via k-NN on frozen embeddings each epoch and support early stopping on top-1 accuracy.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Model Training & Optimization', 'Training Loop Implementation', 'Implement a pure-PyTorch training loop for a CNN on a synthetic image dataset with gradient accumulation, automatic mixed precision (CPU/GPU fallback), global-norm gradient clipping, cosine-annealing learning-rate scheduling, and early stopping on a validation metric. Add robust checkpointing that saves and restores model/optimizer/scheduler states, best metric, and dataloader progress to resume mid-epoch or after SIGINT/SIGTERM, producing both latest and best artifacts.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Build a CLI that reads a JSON-specified stochastic GridWorld (walls, terminal states, slip probability) and trains a tabular Q-learning agent with epsilon-greedy exploration and learning-rate decay to learn an optimal policy. Support variable map sizes and multiple maps in a folder, ensure deterministic seeding, evaluate the greedy policy to meet a success-rate threshold, and save both the Q-table and a human-readable policy artifact.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Build a PettingZoo AEC multi-agent predator–prey gridworld with optional message-passing actions and invalid-move masking, register it with Gymnasium, and configure RLlib to train decentralized PPO policies with GAE, vectorized rollout workers, and resume-from-checkpoint support. Evaluate on held-out procedurally generated maps (fixed seed) and require a ≥0.70 predator win rate over 100 episodes while logging training metrics to TensorBoard.', NULL, ARRAY['logging']);
