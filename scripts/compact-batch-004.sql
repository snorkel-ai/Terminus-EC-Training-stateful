-- Compact batch 4/29: rows 151-200

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that performs a temporal as-of join across three datasets—ad impressions (CSV), clicks (JSONL), and conversions (Parquet)—to attribute each conversion to the most recent prior click (≤7 days) and impression (≤24 hours before the click) for the same user_id with deterministic tie-breaking. Write a unified Parquet with attribution fields and a JSON report listing conversions with ambiguous or missing joins.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Join network event logs with IP ownership datasets by performing longest-prefix-match containment over IPv4/IPv6 CIDR ranges, resolving overlaps via most-specific prefix and source priority. Produce an enriched events output annotated with org/asn and a summary report of per-org counts and unmatched coverage.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Create a command-line script that streams a gzip-compressed NDJSON file at /app/events.ndjson.gz, selecting only records within a given UTC time window whose URL host appears in /app/allowlist.txt while enforcing a per-user cap K (discarding excess events per user in-window). Output the retained events as a stable-order, 2-space-indented JSON array to /app/filtered.json, preserving original key order.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Create a streaming CLI that scans /app/events for newline-delimited JSON files (.jsonl, .jsonl.gz, .jsonl.zst), filters records by a UTC time window, a regex on event_type, and an email-domain whitelist from /app/allowed_domains.txt, and outputs filtered.jsonl with only selected fields in input order. Also write rejected.jsonl logging each discarded record’s id and machine-readable reason, ensuring constant-memory processing without loading entire files.', NULL, ARRAY['logging']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Implement a command-line tool that reads one or more COCO-format annotation JSONs in /app/dataset and filters annotations to categories listed in /app/categories.txt while enforcing bbox area and score thresholds from /app/filter.yaml, dropping any now-orphaned images. Output a pruned COCO JSON with deterministically ordered, remapped contiguous ids and a newline manifest of kept image file paths.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Filtering & Selection', 'Write a script that scans /app/logs for Apache/Nginx access logs (plain text and .gz), filters entries whose client IP falls within any CIDR in /app/eu_cidrs.txt, whose request path matches any pattern in /app/paths.txt, and whose status is 2xx. Output /app/filtered.csv with ISO8601 timestamp, ip, method, and path, deduplicated per ip+path within rolling 10‑minute windows.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Build a CLI that scans /app/datasets for CSV/TSV and JSONL files with mixed encodings, delimiters, header spellings, and timestamp/number formats; auto-detects encoding and delimiter, canonicalizes Unicode (NFKC), maps header synonyms to a target schema (user_id, event, ts_utc, amount), and standardizes timestamps to ISO 8601 UTC and numbers to dot-decimal, writing RFC4180 UTF-8 CSVs to /app/normalized. Emit /app/audit.json summarizing per-file detection results, header mappings, and counts of fixed/dropped rows with a small before/after sample of transformed values.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that ingests a mixed-format coordinate dataset (DMS strings, decimal degrees with hemisphere letters, and UTM zones) and normalizes all positions to WGS84 latitude/longitude in signed decimal degrees with fixed precision. Validate ranges, correct common notations (e.g., stray hemisphere suffixes), and emit a clean, standardized output with a report of any unresolvable rows.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that ingests mixed CSV/JSON sensor datasets from /app/data where numeric values include heterogeneous units and localized formats (e.g., 72F, 21.1 °C, 55 mph, 24,6 m/s, 1 234.5). Normalize all measurements to SI units with canonical float types and ISO 8601 UTC timestamps, then emit a unified standardized Parquet and a normalization report summarizing per-field conversions, unit assumptions, and any imputations or drops.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that scans /app/data for CSV/TSV/JSON product records and outputs /app/normalized.csv with sku, weight_g, length_mm, width_mm, height_mm, price_usd, and updated_at (UTC ISO-8601). It must normalize unit‑labeled values (g/kg/lb/oz; mm/cm/in/ft), localized numbers (commas, spaces), and currencies (using /app/rates.json), converting all timestamps to UTC and writing irrecoverable rows with reasons to /app/errors.csv.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Normalization & Standardization', 'Create a script that scans /app/logs for mixed-format logs (Apache, syslog, JSON), normalizes all timestamps (including timezone and epoch forms) to ISO 8601 UTC (Z) and standardizes severity to a fixed set [DEBUG, INFO, WARN, ERROR, FATAL] while lowercasing hostnames. Emit a single JSON Lines file at /app/normalized_logs.jsonl with a consistent schema and entries sorted strictly by timestamp.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a command-line script that ingests time-series CSV files in /app/sensors, normalizes ISO-8601 timestamps, and detects per-sensor outliers using median absolute deviation plus sanity checks (negative values where forbidden, duplicated timestamps, and sudden step changes). Output cleaned CSVs to /app/cleaned, append all flagged rows with reason codes to /app/anomalies.csv, and write a per-file summary to /app/report.txt.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a script that ingests /app/readings.csv of IoT telemetry and writes /app/clean.csv plus /app/anomalies.csv, flagging schema/plausibility errors (invalid UTF-8, column count, status not in {OK,WARN,FAIL}, out-of-range temp/humidity/battery, duplicate device_id+timestamp, and non-monotonic timestamps per device). Additionally, mark statistical outliers in temperature per device using a 1-hour rolling median with MAD-based thresholds and annotate anomalies with semicolon-separated reason codes.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a script that ingests GPX/CSV GPS tracks, normalizes timestamps and coordinate formats, and flags/removes points that imply impossible speeds, large teleports, or non-monotonic time. Output cleaned tracks and an anomalies report summarizing each file’s discarded points and reasons.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Create a script that ingests heterogeneous service logs from /app/logs (Apache/Nginx, JSONL app logs, and syslog), normalizes timestamps to UTC, correlates entries by request_id when present, and flags anomalies including unparsable lines, clock-skew/causal-order violations, negative durations, and latency outliers via median absolute deviation. Output a CSV of anomalies with standardized fields and a cleaned, time-ordered normalized JSONL stream for downstream use.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Outlier & Error Detection', 'Write a script that scans /app/trips/*.csv (columns: trip_id, device_id, timestamp, lat, lon, odometer_km) to detect geospatial and temporal anomalies such as impossible speeds via haversine distance/time, out-of-bounds coordinates, non-monotonic timestamps per device, duplicate trip_ids, and odometer regressions. Output anomalies.jsonl with one record per issue (including a machine-readable reason and fields) and cleaned_trips.parquet with all offending rows removed.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Build a CLI tool that scans /app/data for CSV, JSONL, or Parquet files and writes profile.json containing per-column summaries (type, non-null count, distinct≈HLL, min/max, mean, median, std, p5/p25/p50/p75/p95) and fixed-bin histograms for numeric fields; if a timestamp column exists, also output hourly counts. Emit a stratified 1% sample that preserves rare categories to /app/sample.ndjson.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Create a CLI that streams over all .jsonl and .jsonl.gz files in /app/events, filters by a UTC date range, and computes overall and per-endpoint latency summaries: count, distinct_user_ids, mean, median, std, p90, p99, and error_rate. Output a 2-space-indented JSON report to /app/latency_summary.json, sorted by descending count and excluding non-numeric or missing latencies from calculations.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Create a streaming profiler that scans all CSV and JSONL (including .gz) files under /app/data and produces per-column summaries: non-null count, distinct count (case-insensitive), min, max, mean, median, and p95 for numeric fields, and top 5 categories with frequencies for non-numeric fields. Write a consolidated report to /app/profile.csv and a machine-readable /app/profile.json, handling mixed schemas across files and locale-formatted numbers (commas and decimals).', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Implement a streaming profiler for /app/data.csv (optionally gzipped) that infers column types from a sample and then computes per-column summaries without loading the entire file: numeric -> count, nulls, mean, std, min, max, median, p95 (approx via P^2), categorical -> count, nulls, distinct count, top-3 modes with counts. Write a single JSON report to /app/profile.json containing total rows, file size, and a per-column section with the computed statistics.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Descriptive Statistics & Summaries', 'Implement a streaming profiler that scans /app/data for CSV/TSV/JSONL files (optionally .gz) and writes profile.json with per-column stats: total rows, non-null count, min/max, mean, std, quartiles/median, top-3 frequent values, and distinct counts (exact below a threshold, HyperLogLog otherwise). For timestamp columns also emit rolls.csv with per-hour counts and, if a user_id column exists, approximate hourly unique users.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Build a CLI that reads arbitrarily large, multi-file clickstream data (CSV and JSONL) and emits a fixed-size, user-level stratified reservoir sample that preserves the hourly traffic curve and the joint distribution of country×device within ±5% of the original. The tool must stream in one pass, be seed-reproducible, and write the selected records and a manifest to /app/sample/.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a CLI script that, given customers.csv, orders.csv, and order_items.csv, produces a reproducible 1% per-month stratified sample of orders via deterministic hashing, then emits subsetted CSVs for all three tables that include only rows linked to sampled orders while preserving referential integrity and original column order. Additionally, write a summary manifest reporting per-month sample rates and total rows kept per table.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a reproducible, referentially consistent subset of a two-table dataset by selecting a 1% hash-based sample of users (seeded) from /app/data/users.parquet and including all their related events from /app/data/events/*.parquet within an optional --since time window. Output sampled users and events Parquet files plus a summary verifying that the users’ country distribution deviates by no more than ±1% from the full dataset, and a manifest listing sampled user_ids.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a script that builds a deterministic, stratified sample of a normalized CSV dataset in /app/data (e.g., users, orders, order_items, events) by selecting users proportionally across country and signup cohort using a seed from /app/seed.txt and including only their related rows. Output referentially complete subset CSVs to /app/sample and a manifest with per-stratum counts and basic distribution checks.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Sampling & Subsetting', 'Create a script that reads a large time-series CSV at /app/timeseries.csv (columns: series_id,timestamp,value) and produces a representative subset at /app/timeseries_sample.csv by applying Largest-Triangle-Three-Buckets (LTTB) downsampling to keep at most 2,000 points per series while preserving chronological order and the original header. Also emit /app/summary.json with the algorithm parameters, per-series retained counts, and a SHA256 of the sample for reproducibility.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Create a CLI that profiles missing-data patterns in a large CSV/JSONL via streaming with reservoir sampling, then renders an ASCII heatmap (rows=sampled records, columns=fields) using dense/light characters for present vs missing alongside per-field summaries. Write a human-readable report to /app/report.txt including missingness rates, longest present/missing streaks, and the top co-missing field pairs.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Create a command-line script that reads /app/events.ndjson of user event logs, computes a five-step conversion funnel (visit → view → add_to_cart → checkout → purchase) with counts and stage-to-stage rates per day and overall. Output a human-readable ASCII report to /app/funnel_report.txt including a monospaced funnel chart, a histogram of daily purchases, and annotations flagging anomalous days using a 3-sigma rule.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Implement a script that reads /app/events.csv with timestamps and categories, performs stratified reservoir sampling to bound memory, and generates an ASCII calendar heatmap of daily activity for the last month plus a per-category horizontal bar chart of counts. Write a single human-readable report to /app/report.txt.', NULL, NULL),
('Data Processing & Scripting', 'Data Sampling & Exploration', 'Visualization & Reporting (CLI-based)', 'Write a script that profiles a mixed dataset directory (CSV, JSONL, Parquet), performs deterministic sampling, infers column types, and generates a single CLI report with ASCII histograms for numeric columns, a character-based missingness heatmap by column, and top-k value tables for categoricals. Output the human-readable report to /app/profile.txt with sections per file and columns sorted by variability.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Build a tool that compares two release artifacts (e.g., .tar.gz vs .zip) for content-equivalence by extracting, normalizing metadata (timestamps, owners, entry order), and normalizing text EOL for .txt/.md before computing per-file hashes and a directory Merkle root. Emit a machine-readable JSON report of added/removed/changed/renamed files and permission-only changes, optionally validating against a provided manifest.json and exiting nonzero on drift.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Create a CLI that compares two mixed-format dataset snapshots at /app/v1 and /app/v2 using rules from /app/rules.yaml: JSON compared semantically (ignoring key order/whitespace), CSV diffed by declared primary keys, text normalized for EOL/encoding, and binaries by SHA256. Emit a single /app/diff_report.json summarizing added/removed/modified records/files and exit non‑zero if any change violates the allowlist rules.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Create a script that compares dataset snapshots at /app/stage_a and /app/stage_b by canonicalizing contents (CSV normalized by a configured primary key from /app/diff_config.toml; JSON/JSONL with sorted keys) and computing content checksums per file. Write /app/diff_report.json listing added/removed files, checksum mismatches, and for changed CSVs counts of added/removed/modified rows keyed by the primary key.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Create a script that compares two tar.gz filesystem snapshots at /app/snapshots/v1.tar.gz and v2.tar.gz entirely in-stream (no full extraction), computing SHA256s to report added/removed/changed files and detecting renames by matching content hashes while distinguishing content vs metadata-only changes. It must also validate v2 against /app/manifest.csv of expected paths, sizes, and hashes and emit a concise diff report.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Integrity Checks & Diffs', 'Implement a CLI that compares two dataset snapshots in /app/snap_a and /app/snap_b, verifies SHA-256 integrity, and classifies per-table differences as identical, row-reordered-only, or content-changed across CSV and Parquet files. Detect renamed files by matching content hashes and write /app/changes.json (added/removed/modified/moved with reasons) and /app/violations.txt (checksum failures).', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Create a script that merges CSV enrollment files in /app/enrollments and validates against /app/students.csv, deduplicating repeated registrations (same student_id, course_id, term) and flagging registrations with missing or unknown student_ids. Output a cleaned /app/enrollments_clean.csv plus /app/duplicates.csv (grouped duplicate sets with source filenames) and /app/missing.csv (rows with null or unmatched keys).', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Create a script that scans all NDJSON user records under /app/incoming and /app/archive, flags rows with missing required keys (id, email, last_name) or malformed emails/phones, and deduplicates users across files by case-insensitive email or normalized phone or Levenshtein-1 full-name match with identical birth_date. Write /app/clean.ndjson containing one canonical record per user and /app/anomalies.csv listing file:line, record_id (if present), and the reason for each missing/duplicate/inconsistent record.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Implement a script that scans mixed-format contact datasets in /app/data (CSV and JSONL), normalizes them to a common schema, detects duplicates via canonicalized emails plus approximate name matching, and flags rows with missing required fields. Output a deduplicated clean_contacts.csv, a duplicates_report.json with cluster details and chosen canonical records, and a missing_summary.csv of per-column missing counts.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Missing & Duplicate Detection', 'Write a script that ingests all headered CSVs in /app/logs/daily with fields sensor_id,timestamp,value, deduplicates rows by the (sensor_id,timestamp) key across files, and emits a consolidated /app/clean.csv sorted by sensor_id then timestamp. For each sensor, detect missing 1-minute intervals between its min and max timestamp, insert placeholder rows with empty value for gaps, and produce /app/quality_report.json summarizing per-sensor duplicates removed and gaps inserted.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a CLI regression harness that compares a baseline and candidate directory of mixed CSV/JSON/Parquet outputs by canonicalizing record/order, normalizing floats (1e-6), and keying rows by a provided primary key to detect schema drift, added/removed records, and numeric deltas beyond a tolerance. Write a structured diff to /app/report.json and PASS/FAIL to /app/result.txt using zero-drift for schema and 0.1% relative tolerance for numeric fields.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a CLI regression harness that runs data transformation scripts declared in /app/tasks.yaml, writes outputs (CSV, JSONL, Parquet) to /app/out, and compares them to goldens in /app/golden using canonicalization (deterministic row ordering, normalized JSON keys) and configurable numeric tolerances. Support per-file ignore/remap rules (e.g., timestamps) via config and emit both a JUnit XML report and a concise text diff summary of mismatches.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a regression harness that compares two pipeline outputs in /app/run_prev/ and /app/run_new/ (CSV or Parquet): verify identical table set, column order and types against /app/schema.json, per-partition row counts and key-level checksums match, and numeric aggregates stay within tolerances from /app/thresholds.toml. Emit a JUnit-style XML report to /app/results.xml and exit non-zero on any failure.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Create a script that compares /app/baseline and /app/candidate outputs across matching CSV or NDJSON files by joining records on id (order-insensitive), enforcing schema compatibility (no removed columns) and comparing numeric fields within a configurable tolerance while ignoring volatile fields like updated_at. Write a summary to /app/regression_report.txt and exit non-zero on missing/extra rows, type changes, or values outside tolerance.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Regression Testing for Data Outputs', 'Implement a CLI that compares new aggregation outputs in /app/current/*.parquet to golden snapshots in /app/baseline/*.parquet using /app/spec.yaml to define composite key columns and per-column rules (exact for categoricals, relative/absolute tolerances for numerics, allowed new columns). Align by keys, flag missing/extra keys, schema narrowing, null-rate increases, and tolerance violations; write /app/report.txt and /app/report.json and exit non-zero on any failure.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Build a CLI script that validates all NDJSON event files in /app/events against /app/schema.json (JSON Schema draft-2020-12), enforcing required fields, types, enum values, and custom formats (uuid, email, RFC 3339 timestamps). Coerce only lossless type fixes (e.g., numeric strings to numbers), write all valid records to /app/validated/merged.ndjson, emit a line-level error report to /app/invalid_report.json for rejects, and exit non-zero if any record fails.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Build a validator that reads a JSON Schema from /app/schema.json and validates user records aggregated from three sources (CSV, JSONL, and a SQLite table), performing strict type/format checks (email, uuid, URI, timezone-aware date-time) and conditional rules (e.g., if country == ''US'' then state is a 2-letter code; if status == ''ACTIVE'' then deactivated_at is null). Produce /app/validation_report.json summarizing per-rule violations and global uniqueness errors (id and case-insensitive email), and write a normalized /app/valid_merged.jsonl containing only valid, de-duplicated records with extra fields stripped.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Create a CLI validator that reads a versioned schema.yaml and validates mixed-format ''orders'' datasets (CSV, NDJSON, Parquet) for strict type adherence, including decimal precision/scale, UUID/email formats, UTC datetimes, nested objects/arrays, required fields, enums, and ranges. Produce a machine-readable JSON violations report per file and emit a Parquet containing only the records that pass validation.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Create a script that validates all NDJSON event records in /app/events against /app/schema.json, enforcing cross-record constraints (globally unique event_id, per-session non-decreasing ISO‑8601 timestamps) and conditional typing based on event_type (e.g., purchase requires a positive numeric amount, view must not include amount). Write valid, normalized records (timestamps coerced to RFC3339 Z, safe numeric coercions) to /app/valid.ndjson and emit a machine-readable /app/errors.csv with file, line, and codes for every rejected record.', NULL, NULL),
('Data Processing & Scripting', 'Data Validation & Quality Assurance', 'Schema & Type Validation', 'Create a script that validates all records in /app/events/*.ndjson against /app/schema.json plus extra rules (user_id must be UUIDv4, timestamp ISO 8601 and not in the future, amount nonnegative with currency-specific decimal precision). Output /app/validation_report.csv with line_number,error_code,field,message for all failures and /app/valid.ndjson containing only canonicalized, schema-conforming records.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create a CLI that recursively scans /app/input for text files, auto-detects encoding (e.g., UTF-8/UTF-16/Windows-1252/Shift-JIS), converts them to UTF-8 with LF newlines in /app/normalized, then builds a reproducible tar.zst archive with sorted entries and fixed metadata. Output a manifest.csv listing original path, detected encoding, original/normalized sizes and SHA256 hashes, and support a verify mode that extracts the archive and validates checksums against the manifest.', NULL, NULL);
