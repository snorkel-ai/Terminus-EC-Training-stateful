-- Compact batch 3/29: rows 101-150

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'Create a portable release script that parses Conventional Commits since the last git tag to compute the next semver (major/minor/patch), updates a VERSION file and CHANGELOG.md, then creates an annotated tag (vX.Y.Z). On success, build the project to dist/, archive it as project-vX.Y.Z.tar.gz with SHA256SUMS, and refuse to run on a dirty tree while verifying the changelog covers exactly the tagged commit range.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'Implement a release pipeline for a two-package monorepo (Python lib at libs/pycalc and Go CLI at apps/gocalc) that reads Conventional Commits since each package’s last namespaced tag to compute the next semantic version per package. Update pyproject.toml/go.mod, generate a combined CHANGELOG.md with per-package sections, create annotated tags pycalc-vX.Y.Z and gocalc-vA.B.C only for changed packages, and place built artifacts plus SHA256 checksums in dist/.', NULL, ARRAY['python']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'Implement a repo-local release automation that, using Conventional Commits, calculates the next semantic version since the last git tag, updates version fields in both package.json and pyproject.toml, regenerates CHANGELOG.md, creates an annotated tag, and produces tarball artifacts with a SHA256SUMS file. Validate by seeding example commits, invoking the release script twice, and asserting that tags, changelog sections, and checksums match expected outputs.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Release Engineering & Version Control Integration', 'Version Tagging & Release Automation', 'In a Git monorepo with a Rust workspace of multiple crates, implement a release script that parses Conventional Commits since the last per‑crate tag, bumps versions in Cargo.toml/Cargo.lock with dependency propagation, writes per‑crate and aggregate changelogs, and creates annotated tags (v{crate}-{version}) plus a workspace tag. The run must support dry‑run vs apply, handle prerelease channels (e.g., -beta) and BREAKING CHANGE semantics, be idempotent on a second run, and leave the workspace building successfully at the new versions.', NULL, ARRAY['git', 'rust']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Build the provided C++ library into both a versioned shared object (with correct SONAME symlinks) and a static archive, and generate pkg-config and CMake package config files while staging headers and licenses under a prefix. Package the staged tree into a relocatable tar.gz release and verify by compiling a tiny consumer that locates the library via pkg-config and find_package.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Configure a CMake project and use CPack to produce both .deb and .rpm packages for a small C/C++ CLI, including /usr/bin binary, a man(1) page, bash completion, and a separate -dbg package with split debug symbols. Validate package metadata (version from git tag), ownership/permissions, dependencies, and contents via dpkg-deb and rpm queries, and by extracting to a temporary root to confirm correct install paths.', NULL, ARRAY['git']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Convert the provided Java library into an OSGi-compliant bundle and produce reproducible release artifacts: the main .jar with correct Bundle-* MANIFEST headers plus -sources.jar and -javadoc.jar. Publish them to a local Maven repository and verify a sample consumer project resolves the bundle and that bnd/osgi-info validates the metadata.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Develop a small C library using CMake with correct SONAME/versioned symlinks and a pkg-config .pc file, then produce release artifacts: a tar.gz containing headers and the shared library and distribution packages (.deb and .rpm) splitting runtime and -dev contents. Validate by building and running a tiny consumer program that uses pkg-config to link against the installed package and confirms the correct SONAME is loaded at runtime.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Artifact Generation & Packaging', 'Package a small C library built with CMake into two Debian packages: a versioned runtime shared library with correct SONAME (e.g., libfoo2) and a -dev package containing headers and a pkg-config file, using debhelper and dpkg-buildpackage. Install the .debs and verify by compiling and running a tiny consumer that links via pkg-config against the installed library.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Configure a CMake superbuild that fetches and builds pinned zlib and libpng from source via FetchContent/ExternalProject, then builds a small C/C++ image utility that links to them. Install targets with an exported CMake package (MyImgToolConfig.cmake), verify find_package works from a separate minimal project, and run the utility on a provided PNG sample.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Configure a CMake-based build for /app/engine using Ninja, CMakePresets.json, and a cross-compiling toolchain file to produce both native (x86_64) and ARMv7 binaries with an ENABLE_SIMD option that toggles sources and compile definitions. The workflow must emit compile_commands.json, run ctest, and generate relocatable cpack TGZ packages for each configuration, verifying the ARM artifact is an ARM ELF and the native build links against system zlib via find_package(ZLIB).', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Configure a CMake/Ninja project to run a two-stage Profile-Guided Optimization workflow: first build instrumented binaries and execute a provided training script to generate profiles, then rebuild using those profiles to emit an optimized executable at a fixed path. Verify that the profile was consumed (via build logs/artifacts) and that the optimized binary demonstrates a measurable runtime improvement over a baseline build.', NULL, ARRAY['optimization']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Create a CMake superbuild using ExternalProject_Add to compile vendored zlib and libpng from local tarballs with Ninja, then build a top-level C++ library and CLI that link them and install to /opt/app with correct RPATH. Emit pkg-config and CMake export targets, and verify by running the CLI to convert a provided PNG into a raw byte dump without network access.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Build System Configuration', 'Replace the project''s Autotools build with Meson + Ninja, adding a subproject wrap for a missing dependency, installing headers, and generating a pkg-config file, and then add a Meson cross file to support armv7hf cross-compilation with hard-float. Verify by building native and cross variants, running the native tests, and using qemu-arm to execute a sample program linked against the cross-built shared library while pkg-config resolves the correct paths.', 'hard', ARRAY['compilation']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile a CGO-based Go CLI that embeds SQLite into static binaries for linux/amd64, linux/arm64, linux/riscv64, and windows/amd64 using zig as the cross C compiler/linker. Verify by running the riscv64 binary under qemu-user to execute a SQL query and confirming the Windows PE binary via wine or PE header inspection outputs the exact expected result.', NULL, ARRAY['sql']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile a provided C/C++ utility into static aarch64 and riscv64 Linux binaries using a musl-based toolchain (e.g., Zig as cc). Validate by running both under qemu-user to process the same input and write matching SHA256 output hashes to a verification file.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile a provided Rust CLI into x86_64-unknown-linux-musl and aarch64-unknown-linux-musl static binaries plus an x86_64-pc-windows-gnu .exe using cargo with the appropriate cross-linkers. Verify outputs by running the native binary directly, the aarch64 binary under qemu-aarch64, and the Windows .exe under wine, asserting exact stdout including the embedded target triplet in --version.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile the provided CMake-based CLI to produce three artifacts: x86_64-linux-musl (fully static), aarch64-linux-gnu, and x86_64-w64-mingw32. Verify correctness by inspecting ELF/PE headers and running the ARM64 build under qemu-aarch64 and the Windows build under wine, ensuring the musl binary has no glibc dependency and all builds honor a fixed SOURCE_DATE_EPOCH.', NULL, NULL),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Cross-Compilation & Multi-Platform Builds', 'Cross-compile the ripgrep Rust project into statically-linked binaries for x86_64-unknown-linux-musl and aarch64-unknown-linux-musl from an x86_64 host, packaging each artifact with a LICENSE file and SHA256 checksum. Validate the ARM64 build by executing it under qemu-aarch64 on a provided test corpus and verifying the expected grep results.', NULL, ARRAY['rust']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Implement and manually compile a Java+JNI demo: generate headers with javac -h, compile the C code into libnativeops.so using gcc -fPIC -shared with the correct JDK include paths, and compile the Java class. Verify by running the Java harness with -Djava.library.path to confirm correct native results and by inspecting exported JNI symbols with nm/objdump.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Manually compile a C shared library and a Java program that calls it via JNI using only javac/java and gcc/clang, correctly setting include paths, -fPIC/-shared, and runtime library paths (java.library.path or LD_LIBRARY_PATH). Execute the Java program to verify the native method loads and returns an exact computed result.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Manually compile a modular Java application using javac, then assemble a minimized custom runtime image with jlink that runs the app without relying on a system JRE. Validate the image by launching the program and confirming only required modules are included with debug symbols and locales stripped.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Manually compile a small Java project into a multi‑release executable JAR: build base classes with javac --release 8, compile alternate implementations for Java 11 into META-INF/versions/11, and assemble with a manifest declaring Multi-Release: true and Main-Class using only javac and jar. Verify by running the JAR to see Java‑11‑specific behavior and by listing the archive to confirm versioned entries.', NULL, ARRAY['java']),
('Build & Dependency Management', 'Source Compilation & Build Systems', 'Manual Compilation', 'Perform a two-phase profile-guided optimization build of the C program in /app/pgotask: first compile with -fprofile-generate and run it over the input corpus in /app/corpus to emit profiles, then recompile with -fprofile-use to produce /app/bin/app_pgo. Verify by timing both pre- and post-PGO binaries on the same workload and writing the speedup and file sizes to /app/pgo_report.txt.', NULL, ARRAY['optimization']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Build a pipeline that scans /app/logs for mixed-format Apache/Nginx logs (plain, .gz, .bz2, .zst), parses into a normalized schema, enriches with country and ASN via provided lookup CSVs, applies prefix-preserving IP anonymization, and outputs a SQLite database plus per-day aggregates (CSV and Parquet).', NULL, ARRAY['database']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Create a script that ingests a mixed drop folder of .csv.gz, newline-delimited .json, and .xlsx transaction files, normalizes columns and timestamps to UTC, converts currencies using /app/rates.toml, and de-identifies emails by hashing with a SALT env var. Load the cleaned data into a SQLite database with indexes and emit per-day partitioned CSVs under /app/out plus a manifest.json of row counts and SHA256 checksums.', NULL, ARRAY['database']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Create a script that ingests mixed Apache/Nginx logs under /app/logs, normalizes timestamps to UTC, de-duplicates requests, and stitches events into user sessions using IP+User-Agent with a 30-minute inactivity threshold. Load results into /app/sessions.db (SQLite) with sessions and events tables, and write /app/landing_pages.csv listing the top 10 landing pages with session counts and bounce rate.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Create a script that ingests mixed-format web server access logs from /app/logs (plain .log, .gz, .bz2 in Common/Combined Log Format), parses and normalizes timestamps to ISO-8601 UTC, enriches with country via /app/GeoLite2-Country.mmdb, and de-duplicates by (ip, timestamp, request). Write partitioned, snappy-compressed Parquet files with a fixed schema to /app/out/year=YYYY/month=MM/day=DD.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'ETL (Extract-Transform-Load) Workflows', 'Implement an idempotent incremental ETL that extracts customer transactions from CSV files in /app/inbox/, newline-delimited JSON in /app/stream/, and a local mock REST endpoint, normalizes timestamps/timezones and converts currencies to USD using a provided FX rates file, and de-duplicates near-duplicates. Load results into a SQLite star schema, persist a checkpoint so only new or changed data is processed on re-run, and write a data-quality report of nulls, duplicate keys, and outliers to /app/reports/.', NULL, ARRAY['rest']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Build a Makefile-driven ETL that ingests JSONL.gz logs from /app/raw, validates/normalizes them, partitions to date-based Parquet, aggregates per-endpoint metrics, and emits a CSV plus HTML report. The DAG must be incrementally rebuildable using checksum stamps so only affected partitions and downstream artifacts are recomputed, with targets for full run, incremental update, and clean.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Create a Makefile-driven ETL that reads a manifest of resources (URL, filename, sha256), downloads and verifies them, normalizes mixed CSV/TSV/JSONL into a common JSONL schema, merges/deduplicates by timestamp, and outputs final.jsonl plus a machine-readable provenance report. The workflow must use stamp files and checksum-based invalidation so reruns are no-ops when inputs are unchanged, and touching any single source triggers only the necessary downstream steps.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Create a Makefile-driven ETL that verifies and decompresses /app/logs/*.log.gz, normalizes records to JSONL, partitions by day, and generates daily summaries plus a SHA256 manifest of inputs/outputs. The DAG must be idempotent and incremental (reruns only when dependencies change) and provide targets all, day-YYYYMMDD, and clean.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Create a tiny DAG runner that reads /app/dag.yaml and executes a 5-stage local ETL (extract, normalize, dedupe, aggregate, report) with dependency-aware incremental rebuilds using content hashes, per-task timeouts/retries, max-parallelism, and a dry-run mode. The harness mutates a single source file and expects only downstream nodes to re-run, emitting updated outputs plus a provenance log and a DOT graph of the executed sub-DAG.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Pipeline Orchestration', 'Implement a lightweight DAG runner that reads /app/pipeline.yml (tasks with command, inputs, outputs, and depends) and executes them in topological order with a max concurrency of 2, skipping tasks whose outputs are up-to-date and retrying failures once. Write an execution log to /app/run.log and concatenate the outputs of all leaf tasks into /app/build/summary.txt.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Python/Perl/Ruby Scripting', 'Implement a Python CLI that reads /app/pipeline.yaml to run a small DAG-based ETL over mixed CSV/JSON/TSV in /app/data: normalize schemas and timestamps to UTC, join with /app/exchange_rates.json to convert currencies to USD, and write a partitioned Parquet dataset to /app/warehouse plus a Markdown summary report. The runner must compute content hashes to skip unchanged steps, execute dependencies in parallel, and emit a reproducibility manifest of inputs/outputs at /app/manifest.json.', NULL, ARRAY['python', 'parallel']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Python/Perl/Ruby Scripting', 'Implement a Python tool that scans /app/config for JSON, YAML, TOML, and INI files, expands ${ENV_VAR:-default} and ${config.some.path} references with type-aware coercions (e.g., durations like 5m, sizes like 1.5GB), merges them by a specified priority, and validates against /app/schema.json (JSON Schema). Output a normalized config.json and a resolution_report.json detailing per-key source precedence, coercions, defaults applied, and any unresolved references.', NULL, ARRAY['python']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Python/Perl/Ruby Scripting', 'Write a Python script that reads a workflow YAML at /app/workflow.yaml defining tasks (id, command, deps, env, max_retries, timeout), executes the DAG in dependency order with retries/timeouts, and captures per-task stdout/stderr to /app/logs/<id>.log. Output /app/report.json summarizing each task’s status (success/failed/skipped), attempt count, start/end timestamps, exit code, and a topologically sorted execution list.', NULL, ARRAY['python']),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Shell Scripting & CLI Automation', 'Create two Bash scripts: backup.sh performs idempotent, incremental backups of /app/data into date-stamped snapshots using rsync --link-dest with per-snapshot SHA256 manifests and a flock-based lock; prune.sh enforces a configurable GFS retention policy, verifies snapshot integrity before deletion, and writes a JSON report of kept/removed snapshots and verification results.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Shell Scripting & CLI Automation', 'Write a Bash script that organizes media files from /app/photos into a date-based library at /app/library/YYYY/MM/DD by reading capture timestamps from EXIF/QuickTime metadata (fall back to file mtime), renames files to timestamp_shortsha.ext, and de-duplicates exact content by SHA-256. Produce /app/manifest.csv with header original_path,new_path,taken_at,sha256,mime and make the workflow idempotent so re-running makes no changes.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Task Scheduling & Cron Jobs', 'Create two cron jobs: one runs every 10 minutes to promote ''stable'' CSVs from /app/incoming to /app/staging (unchanged for ≥90s), the other runs at 00:05 UTC to consolidate all staged CSVs into a deduplicated, timestamp-sorted daily report at /app/reports/YYYY-MM-DD.csv.gz. Use flock to prevent overlaps, write last-run metadata as JSON to /app/status.json, and provide a one-shot script to trigger both jobs immediately for verification.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Task Scheduling & Cron Jobs', 'Set up a cron-orchestrated ETL with three scripts: every 2 minutes ingest and idempotently upsert JSON files from /app/inbox into SQLite, hourly export a rolling 24h metrics CSV to /app/out/report.csv and prune old rows, and daily rotate/compress processed inputs to /app/archive. Enforce flock-based locking, CRON_TZ=UTC, and a catch-up mechanism that detects downtime and replays missed windows.', NULL, NULL),
('Data Processing & Scripting', 'Automation & Workflow Scripting', 'Task Scheduling & Cron Jobs', 'Write a backup script that snapshots /app/db.sqlite to /app/backups as timestamped compressed files, retaining the last 7 daily and last 4 Sunday weekly backups. Install a cron.d entry to run it at 02:17 UTC daily with flock-based locking and append-only logging to /app/backup.log.', NULL, ARRAY['logging']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Build a script that ingests mixed-format web access logs (CSV and JSONL), normalizes them to a single schema, sessionizes requests per client with a 30-minute inactivity timeout, and computes per-session aggregates (request count, duration, total bytes). Output a compact session table and a brief text summary with top landing paths and percentile session lengths.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a command-line script that scans /app/logs for web access logs (plain or .gz), normalizes URLs by collapsing numeric IDs/UUIDs to placeholders, and aggregates per-normalized-endpoint metrics (request count, total bytes, median/p95/p99 latency, and 5xx error rate) for a specified UTC day. Output a 2-space-indented JSON array sorted by descending request count to /app/endpoint_metrics.json.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a script that ingests a directory of mixed-format telemetry files (CSV and JSONL) with heterogeneous timestamp formats, normalizes them to UTC minute buckets, and deduplicates events by (device_id, metric, ts). Aggregate per device and metric to produce hourly stats (count, mean, median, p95, min, max) and write aggregates.parquet plus a concise anomalies.csv listing hours with data coverage <80%.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a script that reads /app/points.parquet of GPS pings, converts timestamps to UTC and rounds to 15-minute buckets, and bins lat/lon into 7-character geohashes. Remove per-cell daily outliers using median absolute deviation and write /app/aggregates.csv with rows (bucket_start, geohash7, count, distinct_src, mean_value, min_value, max_value) sorted by bucket then geohash.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Aggregation & Reduction', 'Create a terminal pipeline that scans /app/logs for web-event files in CSV, JSONL, and .jsonl.gz, normalizes fields (utc_ts, user_id, url, status, bytes), and computes 5-minute rollups with totals, distinct users, bytes p50/p95/p99, and the top-3 URLs by count. Write /app/rollup.parquet and /app/top_urls.csv, and support an incremental mode that updates only new windows using /app/watermark.txt.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that merges a product catalog CSV (product_id, sku, upc), a price events NDJSON (sku, price, timestamp), and a UPC alias TSV by normalizing UPCs and resolving aliases to canonical IDs, then joining on sku/upc. Output a consolidated CSV per product_id with latest price and count of price events, plus an unmatched.csv listing any price events that could not be linked.', NULL, NULL),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that merges an Apache access_log (CLF/combined) and an application event stream (JSON Lines) into a single CSV by first joining on a shared request_id when available; if absent, fall back to the nearest timestamp within ±2 seconds for the same client IP and URL path, and mark which join strategy was used. Output must contain one row per web request with selected fields from both sources and unmatched records discarded.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'Data Cleaning & Transformation', 'Data Merging & Joins', 'Create a script that merges three datasets—orders.csv, refunds.csv, and chargebacks.jsonl—into a single per-order report by joining on order_id (exact) and de-duping conflicting events by the latest event_timestamp. Compute each order’s final_status (Pending/Fulfilled/Refunded/Chargeback) and net_amount, and write a sorted CSV with a fixed header.', NULL, NULL);
