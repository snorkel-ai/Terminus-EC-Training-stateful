-- Compact batch 18/29: rows 851-900

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Data Versioning & Dependency Control', 'Initialize a Git+DVC project tracking /app/data.csv with a local remote and a two-stage, params.yaml-driven pipeline (preprocess -> analysis) that produces metrics.json and commits the resulting dvc.lock. Pin Python dependencies via a generated lockfile (e.g., pip-tools) and run in a fresh venv; then modify the data to create a second version, check out the original DVC tag to reproduce identical metrics and checksums, and write the original data hash and metric to /app/answer.json.', NULL, ARRAY['git', 'python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Build a deterministic three-stage pipeline (preprocess → train → evaluate) that versions data and intermediates with DVC and logs every run to a local MLflow backend, capturing git commit, parameter values, dataset checksums, metrics, and artifacts. Provide a CLI that reads params.yaml (supports sweeps) and writes /app/provenance.json summarizing the latest run’s lineage (stage order, input/output file hashes, DVC stage IDs, MLflow run IDs), reproducing identical metrics on rerun unless inputs or parameters change.', NULL, ARRAY['backend', 'git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Build a small ML experiment pipeline (data → preprocess → train → eval) that uses DVC to version data and stages and MLflow (file backend) to log params, metrics, artifacts, code version, and environment. Provide a CLI to run a parameter sweep and emit a single provenance.json that links each produced model and report to its DVC hash, MLflow run ID, Git commit, code diff, and random seed, enabling exact reruns.', NULL, ARRAY['backend', 'git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Create a DVC-driven workflow (data → preprocess → model) that logs parameters, metrics, and artifacts to MLflow, and emits a provenance.json capturing dataset checksums, code commit, DVC stage graph, environment snapshot, and MLflow run IDs. Provide a CLI to reproduce any past run from only an MLflow run ID by restoring DVC versions and the environment, then verify artifact byte equality and write a reproducibility report.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Create a DVC-tracked pipeline (preprocess → train → evaluate) for a small scikit-learn task, and instrument each run with MLflow to log parameters, metrics, artifacts, git commit, and DVC data hashes. Prove reproducibility by rerunning dvc repro to obtain byte-identical outputs and emit a provenance report linking MLflow run IDs to the exact DVC stage versions used to produce the final metrics.', NULL, ARRAY['git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Experiment Logging & Provenance Tracking', 'Create an MLflow Project that runs a parameterized training pipeline on /app/data.csv, logging parameters, metrics, model artifacts, the Git commit SHA, and an input data checksum for each run. Implement a CLI sweep that launches multiple runs and writes the best run_id and its recorded git_commit to /app/answer.txt, with fixed seeding to make reruns reproduce the same metrics and artifacts.', NULL, ARRAY['logging', 'git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Convert a geospatial analysis notebook that reprojects a provided shapefile and computes polygon areas under two CRSs into a headless, reproducible CLI executed via papermill/nbconvert, pinning GDAL/PROJ and setting deterministic env vars (e.g., PROJ network/grid settings and single-threaded BLAS). The run must produce identical CSV/PNG artifacts and a results.json with area summaries and SHA256 hashes across repeated clean-container executions.', NULL, ARRAY['container']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Convert a provided Jupyter notebook into a deterministic, headless analysis pipeline by parameterizing randomness, extracting it to a Python script, and orchestrating execution with a Makefile that builds a fully pinned, hashed environment (e.g., pip-tools) and runs papermill to regenerate results. The pipeline must yield byte-identical CSV/PNG outputs across reruns and emit a manifest.json capturing package lock hashes, data checksums, seeds, and system info for verification.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Create a CLI pipeline that executes /app/analysis.ipynb with papermill using explicit parameters (including a fixed RNG seed), producing deterministic metrics, tables, and plots in /app/outputs and verifying bit-for-bit identical artifacts across two consecutive runs via SHA256 checksums. The run must also emit a locked requirements file and environment manifest (Python, OS, and package versions) alongside the outputs to ensure re-execution fidelity.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Create a CLI that executes /app/analysis.ipynb via papermill with user parameters, seeds all RNGs (random, NumPy, pandas, and torch if present) and forces single-threaded BLAS for determinism, then exports CSV outputs and a deterministic HTML report. The tool must write a manifest with parameters, pip-freeze, and SHA256 checksums of artifacts and provide a verify mode that re-runs and asserts bitwise identity for identical parameters.', NULL, ARRAY['pandas']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Notebook & Script Reproducibility', 'Refactor a provided Jupyter notebook with stochastic analyses into a deterministic, parameterized workflow that runs in a locked Python environment and produces byte-for-byte identical outputs on repeated runs. Expose a single terminal entrypoint that pins dependencies, executes the notebook (via papermill or a jupytext-converted script) with fixed seeds and constrained BLAS threads, and writes results.json plus a reproducibility checksum.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a CLI pipeline that performs a seeded analysis on a provided dataset, exports figures and tables, and assembles a fully self-contained HTML report (embedded images, no external assets) plus a manifest with SHA256 checksums and environment metadata. Orchestrate with Makefile/Snakemake so unchanged inputs trigger cached steps and reruns produce byte-identical outputs.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a Makefile-driven pipeline that ingests experiment CSVs, computes grouped summaries with bootstrap CIs, renders figures/tables, and compiles a templated Markdown into a standalone HTML (and optional PDF) report via Pandoc with embedded assets. The workflow must support incremental rebuilds and include a provenance appendix (git commit, CLI args, pip freeze), writing /app/report/index.html and a machine-readable /app/results.json.', NULL, ARRAY['git']),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a Snakemake pipeline that parameterizes and executes a Jupyter notebook (via Papermill) across rows in a config CSV, aggregates outputs into figures and tables, and compiles a Quarto/Pandoc HTML+PDF report with embedded provenance (input checksums, software versions) and a run manifest. The workflow must be fully incremental (no re-execution on unchanged inputs), produce deterministic artifacts, and write the final reports and exported assets to a versioned /app/report directory.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a Snakemake pipeline that runs a parameterized Monte Carlo study, exports figures and summary tables, and renders a Quarto report to both HTML and PDF. The workflow must record seeds, config, and dependency versions into a manifest to ensure exact reproducibility across runs.', NULL, NULL),
('Scientific Computing & Analysis', 'Reproducible Research & Workflow Automation', 'Report Generation & Automation', 'Build a reproducible, Makefile-driven pipeline that reads a provided CSV, computes summary statistics and bootstrap 95% CIs in Python, saves tables/figures, and renders a self-contained HTML report from a Jinja2 template under /app/report. The build must be deterministic with a fixed seed and incremental (only re-running steps when inputs or templates change).', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Build a CLI that ingests a directory of environmental sensor CSV/TSV files with mixed time zones and units, converts all measurements to SI using declared metadata, aligns timestamps to UTC, resamples to uniform 1-minute intervals, flags/removes outliers, and imputes short gaps. Write a single tidy, columnar Parquet dataset with standardized NaNs and stable column order, plus a JSON file summarizing QC metrics and unit conversions applied.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Build a CLI that ingests a folder of heterogeneous NetCDF climate model files, maps variables to CF-standard names, converts units to SI, reprojects to a target grid, and aligns time onto a unified daily calendar with gap filling. Output a consolidated chunked Zarr store and a manifest CSV documenting provenance, unit conversions, and optional baseline-normalized anomaly fields.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Create a CLI that ingests a directory of daily CF-netCDF climate files, harmonizes units (e.g., K→°C), decodes mixed time units/calendars, merges along time, masks fill values, removes outliers, and resamples to monthly means. Compute 1991–2020 per-gridcell climatology and z-score anomalies, then export a chunked, compressed Zarr dataset with consolidated metadata and a Parquet summary index.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Create a CLI tool that ingests a DICOM CT series, converts pixel data to Hounsfield Units via RescaleSlope/Intercept (handling per-slice calibration), clips to [-1024, 3071], and resamples to 1 mm isotropic voxels while preserving spatial metadata. Write a single 3D NIfTI (.nii.gz) with correct RAS affine and a JSON summary of original/resampled spacings, dimensions, and any slices skipped due to corruption.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Data Cleaning & Transformation', 'Create a Python CLI that ingests a directory of daily CF-NetCDF climate files, harmonizes variable names/attributes, converts units to a specified target, stitches a continuous time axis over a given range, and repairs single-day gaps via flagged interpolation. Compute per-calendar-month climatology over a baseline and standardized anomalies, then write a CF-compliant compressed NetCDF of the cleaned series plus a CSV of the monthly climatology.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Build a Python CLI that ingests a multi-band satellite GeoTIFF (with NIR and Red), computes NDVI with nodata handling, reprojects to EPSG:3857, and writes a colorized XYZ tile pyramid into an MBTiles database with correct bounds and metadata. Additionally, vectorize pixels where NDVI exceeds a threshold into simplified GeoJSON polygons and export a PNG quicklook map.', NULL, ARRAY['python', 'database']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Build a Python CLI that ingests a time-series of multispectral GeoTIFFs and a polygon ROI, decodes per-pixel cloud masks from QA bands, harmonizes projection/resolution, and computes a cloud-free median NDVI composite. Output a cloud-optimized GeoTIFF with overviews, a quicklook PNG with ROI outline and scale bar, and a CSV of per-ROI summary statistics.', NULL, ARRAY['python', 'cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Create a CLI that georectifies an unreferenced aerial image using provided ground control points (pixel ↔ lon/lat), warps to a target CRS, and outputs a tiled, compressed Cloud-Optimized GeoTIFF with correct nodata and overviews. Also produce a quicklook PNG, a GeoJSON footprint of the warped image, and a JSON report of per-point residuals and overall RMSE.', NULL, ARRAY['cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Create a Python CLI that mosaics multispectral GeoTIFF tiles, reprojects to EPSG:3857, clips to polygons in a GeoJSON ROI, computes NDVI from specified Red/NIR bands while honoring nodata and an optional cloud mask, and saves a color-mapped NDVI PNG plus a Cloud-Optimized GeoTIFF. Also compute per-polygon zonal statistics (mean, median, std, pixel count) and write them to a GeoJSON output.', NULL, ARRAY['python', 'cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Image & Geospatial Data Handling', 'Create a Python tool that reads a folder of multi-date satellite GeoTIFFs with varying CRSs and resolutions plus an AOI polygon, reprojects and clips them to a common grid, and builds a cloud-robust medoid mosaic across dates using RGB+NIR bands. Write the mosaic as a Cloud-Optimized GeoTIFF, a PNG quicklook with AOI overlay, and a JSON report of per-band mean/std and fraction of valid pixels.', NULL, ARRAY['python', 'cloud']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Build a CLI tool that loads an irregularly sampled light curve from /app/lightcurve.csv (t, flux, sigma), computes a generalized Lomb–Scargle periodogram to identify the top periodicities with false-alarm probabilities, and produces publication-quality periodogram and phase-folded plots. Refine each candidate via weighted sinusoid fitting and write a summary CSV of frequency, period, semi-amplitude, phase, and FAP.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Create a CLI tool that computes a multitaper (DPSS) power spectral density of a 1-D signal and automatically detects narrowband line components above a robust noise floor. Apply notch filtering at detected lines and save the cleaned signal, PSD before/after, and a figure highlighting the peaks removed.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Create a CLI tool that computes power spectral density estimates for multichannel signals using both Welch and multitaper (DPSS) methods, saving PSDs and 95% confidence intervals to CSV plus comparison plots. Validate normalization via Parseval’s theorem by requiring the PSD-integrated variance to match the time-domain variance within 2% and report any channels that fail.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Implement a Python CLI that loads a multichannel time series, applies adaptive notch filtering at mains and harmonics plus a zero-phase bandpass, then computes multitaper PSDs and magnitude-squared coherence; it must detect and report dominant peaks, band powers, spectral entropy, and a coherence matrix to /app/results.json and save publication-quality PSD/coherence plots. Include a test mode that synthesizes known signals to validate peak frequencies within ±0.2 Hz and coherence above 0.9.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Spectral & Signal Processing', 'Implement a matched-filter detector for transient chirp signals: read a 1D noisy time series and a template waveform, estimate the noise PSD via Welch, whiten both, and compute the matched-filter SNR time series using FFT-based convolution. Write the peak detection time and SNR to a results JSON and save plots of amplitude spectral density (pre/post whitening) and the SNR time series.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Build a Python CLI that loads a gridded NetCDF atmospheric dataset (temperature on lat–lon–pressure), computes a zonal-mean latitude–pressure cross-section and derives the tropopause pressure using the WMO lapse-rate criterion, then plots a publication-quality contour/contourf figure with an inverted log-pressure axis, labeled isotherms, and an overlaid tropopause line using Matplotlib. Save both PNG and SVG figures and export the zonal-mean fields and tropopause profile to specified output files.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Build a Python tool that loads a gridded NetCDF climate dataset, computes a 1981–2010 monthly climatology and anomalies for a chosen year, and generates a publication-quality, three-panel figure: (1) a global Robinson-projection contour map of annual-mean anomaly with coastlines and significance hatching, (2) an equatorial Hovmöller diagram (time vs longitude), and (3) a zonal-mean anomaly profile, all using a shared color normalization. Save both PNG and PDF outputs with consistent, colorblind-safe styling and embedded metadata.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Create a Python CLI that reads a NetCDF surface temperature dataset, computes a 1981–2010 monthly climatology and 2016 anomalies, and produces a publication-quality multi-panel figure: a global anomaly map (Robinson projection with coastlines), a latitude–time Hovmöller, an area-weighted global-mean time series, and an anomaly histogram. Save both PNG and vector PDF with colorblind-safe palettes, labeled colorbars, and consistent typography.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Create a Python script that loads a 2D velocity field (u, v) from /app/velocity.npy, computes speed and vorticity, and generates a two-panel Matplotlib figure: left panel shows a pseudocolor speed heatmap with overlaid streamlines and a sparsified quiver; right panel shows filled vorticity contours with contour lines. Save the figure as /app/flow_figure.png and /app/flow_figure.pdf with labeled colorbars, equal aspect ratio, consistent fonts, and grid-aligned axes.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Data Processing & Visualization', 'Visualization & Plotting', 'Write a Python CLI that loads gridded geophysical data (lat, lon, time, variable) from a NetCDF file, computes a 30-year climatology and anomalies for a target month, and generates a 3-panel publication figure: global anomaly map with coastlines, zonal-mean latitude–anomaly cross-section, and a time series with rolling mean and confidence band. Use Matplotlib (and Cartopy for maps) with a symmetric, colorblind-safe diverging colormap centered at zero and fixed normalization across panels, saving the figure PNG and a JSON with summary statistics.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 1D finite-volume solver for the linear advection equation u_t + a u_x = 0 on [0,1] with periodic boundaries, supporting both first-order upwind and second-order MUSCL-Hancock schemes with CFL-controlled time stepping and CLI selection. Validate against the exact traveling-wave solution for a smooth periodic initial condition by outputting solutions at requested times and reporting L1/L2 errors that demonstrate first- vs second-order convergence over at least three uniform grid refinements.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 1D heat equation solver using the Crank–Nicolson scheme with a Thomas tridiagonal solver on a uniform grid with Dirichlet boundaries. Validate by comparing to the analytical solution u(x,t)=exp(-pi^2*t)*sin(pi*x) (alpha=1), reporting max and L2 errors at specified times and demonstrating near second-order convergence under grid refinement.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 2D Poisson solver on the unit square using a geometric multigrid V-cycle with red-black Gauss–Seidel smoothing and second-order finite differences, with CLI options for grid size and cycle parameters. Validate via a manufactured solution (e.g., u(x,y)=sin(pi x) sin(pi y)) to demonstrate O(h^2) convergence and quantify residual reduction per cycle versus a plain Gauss–Seidel baseline.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a 2D Poisson solver on the unit square with Dirichlet boundaries using second-order finite differences with at least two solvers (e.g., Gauss-Seidel and Conjugate Gradient) exposed via a CLI. Validate against the analytical solution u(x,y)=sin(pi x)sin(pi y) by grid refinement (e.g., N=32,64,128), reporting L2/L∞ errors and observed order to a results file and failing if order < 1.9 or residual tolerances are unmet.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Algorithm Implementation & Validation', 'Implement a Gauss–Legendre quadrature generator using the Golub–Welsch algorithm to compute nodes and weights for arbitrary n (e.g., up to 2048) and provide a CLI to write them to disk. Validate by integrating polynomials up to degree 2n−1 over [-1, 1] and reporting the maximum absolute error against exact values, ensuring it stays below a specified tolerance.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Optimize a naive Python finite-element stiffness matrix assembly for a 2D Poisson problem that performs incremental CSR updates inside nested loops by profiling hotspots and refactoring to a vectorized batch COO (I,J,V) build with a single CSR conversion. Verify numerical equivalence of the assembled matrix and improved end-to-end solve time, and emit a JSON report of timings and speedups.', NULL, ARRAY['python', 'profiling']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile a naive finite-element stiffness-matrix assembly for a 2D Poisson problem on triangular meshes and optimize the hotspot by vectorizing element computations and constructing the global matrix in CSR with preallocated buffers or Numba. Validate numerical equivalence within tolerance across provided meshes and emit timing and speedup metrics.', NULL, NULL),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile a naive nested-loop Python implementation of pairwise Euclidean distances between two sets of 128D vectors and replace the hotspot with a vectorized formulation and/or a Numba/Cython kernel using cache-friendly memory access and preallocation. Validate numerical equivalence (≤1e-8), emit before/after profiles and timing summaries, and achieve at least a 20x speedup on inputs of size Q=5000, D=50000.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile a naive pure-Python 3D heat equation explicit solver stepping 100 iterations on a 128^3 grid, then vectorize the stencil update, optimize memory access, and accelerate with Numba (parallel) to achieve ≥10× speedup while keeping max absolute error ≤1e-6 versus the baseline. Provide a CLI benchmark that runs pre/post-optimization versions, captures timing and max error, and writes a JSON report to /app/bench.json.', NULL, ARRAY['python', 'parallel', 'optimization']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Code Optimization & Profiling', 'Profile and optimize a naive 3D 7-point Jacobi solver for the Poisson equation on a uniform grid, transforming a triple-nested loop baseline into a high-performance version via cache blocking/tiling, vectorized memory access, and optional Numba or OpenMP. The CLI should run solves on several grid sizes, verify residual reduction against a reference solution with fixed boundary conditions, and print a profiling report that shows at least a 5× speedup over the baseline.', NULL, ARRAY['performance', 'profiling']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Design and package a Python library for uncertainty propagation offering linearized (Jacobian-based) and Monte Carlo estimators for vector-valued functions, with interchangeable NumPy and JAX backends. Ship a type-hinted API, Sphinx docs with runnable examples, unit tests with finite-difference validation, a CLI that evaluates models from JSON, and a wheel for local installation.', NULL, ARRAY['python', 'api', 'installation']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Develop a Python interval arithmetic library that guarantees enclosure for elementary functions using outward rounding and vectorized NumPy operations, with a clean API, type hints, and Sphinx docs driven by doctests. Include a CLI to evaluate expressions over named intervals and a test suite that verifies inclusion properties and monotonicity on randomized cases.', NULL, ARRAY['python', 'api']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Develop a Python library for lazy linear operators (matrix-free) that supports composition, adjoints, and iterative solvers (CG/LSQR) with NumPy/SciPy backends. Package it with pyproject.toml, Sphinx docs with examples and autodoc, a small CLI to apply/solve from JSON input, and unit tests verifying algebraic identities and solver accuracy.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Scientific Software Engineering', 'Library Development & Documentation', 'Develop a Python library that implements polynomial chaos expansion (PCE) for uncertainty propagation with Legendre/Hermite bases, supporting coefficient fitting from samples, surrogate evaluation, and moment estimation. Package it with a CLI to build/evaluate PCEs from CSV data, comprehensive unit tests, and Sphinx-based API/user docs with examples.', NULL, ARRAY['python', 'api']);
