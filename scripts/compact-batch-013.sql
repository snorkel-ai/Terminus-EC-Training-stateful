-- Compact batch 13/29: rows 601-650

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Shell Puzzle Games', 'Starting at /app/maze/start, traverse a symlink-based filesystem maze where each encountered file contains JSON giving a regex for the next filename plus a token fragment and sequence number. Using only shell tools (find, sed/awk, jq, sort), detect and avoid cycles, collect and order fragments, then base64-decode and gunzip the concatenated string to plaintext and write it to /app/answer.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'Shell Puzzle Games', 'Traverse a directory tree containing mixed-format event fragments (logs, JSON, CSV, and filename-encoded timestamps), some nested in unlabeled archives, and normalize their heterogeneous timestamps (RFC3339, epoch ms/nanos, DOY, base36) using shell tools. After deduping by event id and globally sorting, extract one character per event to reconstruct the hidden message and write it to /app/message.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Open lazygit in the provided Git repo and resolve a pending merge by choosing “theirs” for config/app.yml and accepting only the first two conflicting hunks from “ours” in src/core.py, then finalize the merge with the supplied message and exit. After leaving the TUI, write the resulting HEAD commit SHA and the total count of remaining conflict markers (<<< or >>>) in the repo to /app/answer.txt.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Open the provided mixed CSV/JSONL dataset in the VisiData TUI, interactively filter to completed EMEA orders in Q3 2024, join users to orders, group by user, compute total spend, sort descending, and export a 2‑column TSV to /app/report.tsv. All data manipulation must be performed inside VisiData via keyboard-driven operations, then exit cleanly.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Use the tig ncurses Git interface to locate the first commit that introduced a target function signature and determine the first tag that contains it by navigating log, search, and blame views. Record the exact commit SHA and tag name to /app/results.txt as ''SHA TAG'' after exiting the TUI.', NULL, ARRAY['git']),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Using the lazygit TUI, resolve a prepared merge conflict by selectively staging hunks to restore a passing implementation, commit with the exact message ''merge-fix: OK'', and push to the provided local bare remote. After the test suite passes, write the resulting commit SHA to /app/result.txt.', NULL, NULL),
('Interactive Challenges & Games', 'Tool & CLI Mastery Challenges', 'TUI (Text User Interface) Interaction', 'Using the visidata TUI, open /app/orders.csv and interactively create a pivot grouped by region and product_category for orders in 2023-Q4, calculating total_revenue and order_count, then sort by total_revenue descending and export the result to /app/summary.tsv. Also write the single top row of that pivot (tab-separated) to /app/top.txt.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Build a CLI that ingests a labeled multivariate time-series CSV, slices it into fixed windows, and generates augmented samples using jitter, scaling, time-warp, permutation, and magnitude-warp with deterministic seeds while preserving label alignment. Save the augmented dataset and a per-sample transform manifest, and produce a small DTW-based diversity report.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Build a CPU-only Python CLI that reads WAV files from /app/speech, impulse responses from /app/rir, and noise from /app/noise, and synthesizes an augmented set by convolving speech with a chosen RIR and mixing noise at randomized target SNRs, resampling as needed and peak-normalizing to -1 dBFS without clipping. Save 16-bit PCM WAVs under /app/aug deterministically given a --seed and write /app/aug/metadata.csv listing source paths, chosen RIR/noise, SNR, RT60 estimate of the RIR, output path, and random seed.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Create a CLI that augments multivariate time-series by injecting labeled anomalies—spikes, level shifts, trend changes, and seasonal pattern breaks—under user-controlled prevalence, duration, and SNR, producing /app/aug_series.csv, /app/aug_labels.csv, and a JSONL manifest of per-sample transforms. Include a deterministic seed and a ''validate'' mode that checks non-overlap, exact target prevalence by type, and summarizes augmentation statistics.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Create a Python CLI that ingests multivariate IoT sensor readings from /app/sensor.csv, trains a variational autoencoder on normal windows, then synthesizes a balanced dataset with labeled normal and anomalous sequences by sampling and latent-space perturbations plus jitter, scaling, and time-warp augmentations. Save train/val/test Parquet files (fixed-length windows) and a manifest JSONL capturing augmentation provenance to /app.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Augmentation & Synthesis', 'Create a Python CLI that performs class-balanced augmentation of a mixed-type tabular dataset by implementing SMOTE-NC from scratch, honoring per-column constraints (numeric bounds, integer-only, allowed categories) and a fixed RNG seed. Output the augmented CSV and a JSON report with pre/post class ratios and drift metrics (KS for numeric, chi-square for categorical), with tests ensuring constraints are met and no target leakage occurs.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a CLI pipeline that ingests a directory of messy CSVs (mixed encodings, delimiters, and locales) and reconciles their schemas, producing a single standardized Parquet dataset and a JSON data-quality report. Handle Unicode NFC and BOM normalization, locale-aware numeric/currency parsing, ISO-8601 UTC datetime normalization, group-wise imputation, MAD-based outlier capping, MinHash de-duplication on a text column, and optional tokenization to fixed-vocabulary IDs.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a CLI pipeline that ingests multiple vendor CSVs of product listings with mixed encodings, locale-specific number/currency formats, and heterogeneous timestamp columns, then standardizes them. Normalize text to UTF-8 NFC, parse dates to UTC ISO-8601, harmonize decimal/thousand separators, convert currencies via a provided FX table, impute missing numeric fields per-category with robust medians, remove outliers via MAD, deduplicate by SKU/title similarity, and write a cleaned Parquet plus a data-quality summary JSON.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a chunked, streaming CLI that consolidates heterogeneous CSV/TSV/JSON sources with mixed encodings into a single cleaned Parquet: auto-detect encoding/delimiters, coerce to a target schema from schema.json, normalize booleans/numerics, parse and UTC-normalize datetimes, impute missing values, cap outliers via MAD, and deduplicate by composite keys. Emit data_quality.json with per-column nulls/outlier counts/type-coercions and bad-record samples, ensuring determinism (seeded), memory safety on multi-GB files, and resilience to BOMs, malformed rows, and embedded newlines.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Build a streaming CLI that merges multiple CSV shards with mixed encodings and a nested JSON column, infers a unified schema, standardizes datetimes to UTC ISO-8601, normalizes column names, and outputs cleaned Parquet deterministically. Impute missing values (group-wise median for numerics, mode for categoricals, forward-fill within user sessions), cap outliers per group via IQR, tokenize a free-text column while preserving emojis, and generate a data-quality report JSON with validation metrics.', 'hard', NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Data Cleaning & Preprocessing', 'Create a CLI that consolidates multi-sensor time-series CSVs with differing time zones and sampling rates: normalize timestamps to UTC, auto-detect and correct per-sensor clock drift via alignment to a shared ''sync'' channel, resample to 1 Hz, impute gaps ≤5s, and flag/remove outliers using robust MAD. Save the cleaned dataset to Parquet and emit a JSON audit of imputations, drift corrections, and outlier statistics to support reproducibility.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Build a CLI that performs a purged, group-aware time-series split with an adjustable embargo to prevent leakage, using entity_id as the group and preserving chronological order. It must output train/val/test CSVs that maintain marginal and pairwise label co-occurrence within ±2% via iterative multi-label stratification and emit a JSON report of balance and leakage diagnostics.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a CLI that performs leakage-safe train/val/test splitting for a multi-label text dataset by first clustering near-duplicate documents with MinHash LSH and then applying iterative stratification at the cluster level to match label marginals while assigning whole clusters to a single split. Write split assignments to /app/splits.csv and a duplicate report to /app/dup_report.json with cluster members and pairwise Jaccard estimates.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a Purged Group Time Series Split that creates train/validation/test indices for timestamped events grouped by an entity_id, enforcing a configurable gap and embargo to prevent temporal leakage while approximately preserving class balance via quantile-based stratification. Provide a CLI that reads /app/events.csv and writes split index files plus a JSON report with leakage checks and per-split label prevalence.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a deterministic multi-label, group-aware stratified k-fold splitter with an optional time-based embargo to prevent leakage across folds. Provide a CLI that reads id/group_id/timestamp/labels columns and outputs per-fold splits plus a JSON report of label balance, zero group overlap, and temporal ordering, scaling to millions of rows via streaming.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Dataset Splitting & Sampling', 'Implement a spatially stratified data splitter that groups samples by 5-character geohash and assigns whole geohashes to train/val/test to prevent spatial leakage. Maintain class balance for the species label as closely as possible, enforce reproducibility via salted hashing, and write split indices and a summary (per-split label histograms and KL divergence vs overall) to disk.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Build a CLI that performs leakage-safe K-fold target encoding with additive smoothing for multiple categorical columns (including pairwise interactions), producing out-of-fold encodings for train, encodings for test via a holdout, and a serialized mapping for reuse. It must stream-process large CSVs (10M+ rows), be deterministic, and robustly handle missing and unseen categories.', NULL, NULL),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Build a CPU-only Python CLI that loads directed graphs from /app/graphs/*.edgelist and computes reproducible 128-dimensional node2vec embeddings via fixed-seed random walks and skip-gram training, writing /app/embeddings/{graph}.npy and a matching {graph}_nodes.txt listing nodes in row order. Ensure deterministic node ordering across runs, gracefully handle disconnected nodes, and fit any walk/model parameters using only the provided train split to prevent leakage.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Build a Python CLI that performs leakage-safe out-of-fold target encoding and frequency encoding for multiple high-cardinality categorical columns on a time-stamped dataset using chronological folds with smoothing and optional noise regularization. Output transformed train/test Parquet files and a reusable encoder artifact to ensure consistent application on future data.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Create a Python CLI that reads an edge list from /app/graph.csv and computes per-node features including node2vec embeddings and structural metrics (degree, betweenness, clustering coefficient, PageRank), then standardizes and writes a single feature table with a node_id column to /app/node_features.parquet. Ensure deterministic results with fixed RNG seeds and correct handling of isolated nodes and disconnected components.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Data Preparation & Feature Engineering', 'Feature Extraction & Transformation', 'Implement a leakage-safe categorical target encoder that computes out-of-fold, James-Stein–smoothed means for high-cardinality features (and selected feature crosses), augments with count/frequency stats, and uses a hashed fallback for unseen categories at inference. Provide a CLI that fits on train.csv and transforms both train/test into /app/processed/*.csv in chunked mode with deterministic seeds and memory bounds.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a Mixture-of-Experts Transformer feed-forward block in PyTorch with top-2 gating, capacity-based token dispatch (including overflow handling), and an auxiliary load-balancing loss. Integrate it into a small decoder-only model and provide tests for routing correctness, gradient flow, and equivalence to a dense FFN when num_experts=1.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a PyTorch LoRA adapter system that injects low‑rank adapters into a vanilla Transformer’s attention (Q, K, V, and output) and MLP projections, with per-module rank/alpha settings, freezing of base weights, and precise merge/unmerge of adapters into the backbone. Provide a CLI that trains only the adapters on a tiny language-modeling corpus and verifies identical logits pre/post-merge within a tight tolerance while confirming no non-adapter parameters changed.', NULL, ARRAY['pytorch', 'modeling']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a PyTorch Mixture-of-Experts Transformer block with top-2 routing, per-expert capacity with token dropping, and an auxiliary load-balancing loss, supporting variable sequence lengths and attention masks. Ensure numerically stable, deterministic routing and gradient flow across dtypes on CPU, and include a small CLI to verify balanced expert utilization on synthetic data.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a PyTorch decoder-only Transformer block with grouped-query attention (GQA), rotary position embeddings (RoPE), pre-norm RMSNorm, and an efficient KV cache that supports incremental autoregressive generation with a stable fallback when scaled_dot_product_attention is unavailable. Ensure correct causal masking, mixed-precision stability, variable-length batch handling, and robust cache updates during token appends and sequence reordering.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Architecture Design & Implementation', 'Implement a minimal decoder-only Transformer in PyTorch that supports rotary positional embeddings (RoPE) and grouped-query attention (GQA) from first principles (no Transformers/timm), with a flag to fall back to standard multi-head attention. Train CPU-only on TinyShakespeare for a short run, then generate a deterministic 200-character sample and save model.pt and a config.json to /app.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a PyTorch MonotonicLinear layer that enforces non-negative weights on selected features and a pairwise monotonicity loss that penalizes order violations, then train an MLP on a tabular regression dataset with specified monotone inputs. Report test MAE and the fraction of monotonicity violations versus an unconstrained baseline.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a PyTorch Soft-DTW loss with numerically stable forward/backward that supports batched variable-length sequences via padding masks, per-sample Sakoe–Chiba windows, and mixed precision. Provide a TemporalAlignment layer that uses the expected alignment matrix to differentiably warp sequences for downstream models.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a batched Soft-DTW loss with padding masks and an optional Sakoe–Chiba band for variable-length time-series, plus a custom temporal attention pooling layer that uses differentiable DTW alignment scores as attention weights. The components must run on CPU/GPU, support mixed precision, and be memory-efficient enough to handle sequences up to length 10k.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a custom TokenDropAttention layer for a tiny Transformer that learns to stochastically drop uninformative tokens via Gumbel-Softmax gates, paired with a sparsity-consistent KL loss enforcing a user-specified target keep-rate with temperature annealing and mixed-precision safety. Train on a synthetic sequence classification dataset and verify via tests that the achieved keep-rate is within tolerance and that accuracy drop versus a vanilla attention baseline stays below a threshold.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Loss Function Design & Custom Layers', 'Implement a differentiable sorting-based Top-k pooling layer (NeuralSort/SoftSort) and a listwise NDCG surrogate loss in PyTorch to train a ranking model on variable-length item lists with padding masks. The layer and loss must be numerically stable in fp16/fp32, efficient on long lists, and include gradient checks and input validation.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Build a CPU-only PyTorch character-level LSTM language model on TinyShakespeare that stays stable on long sequences via dropout, LayerNorm on recurrent outputs, global-norm gradient clipping, gradient accumulation, and a warmup+cosine learning-rate schedule. Log per-step gradient norms and effective LR, and save a final model plus JSON metrics proving clipping occurred and the scheduler advanced.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Implement a PyTorch character-level LSTM trained on Tiny Shakespeare with two runs: an unstable baseline and a stabilized variant using dropout, gradient clipping, label-smoothed cross-entropy, and a warmup+cosine learning rate schedule. Log gradient norms and NaN/Inf checks, and verify the stabilized run achieves a target perplexity while the baseline shows exploding gradients or divergence.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Retrofit an unstable DCGAN training script to add spectral normalization to the discriminator, an R1 gradient penalty, adaptive global-norm gradient clipping, EMA of generator weights, and a cosine learning-rate schedule with warmup; train on a provided CIFAR-10 subset and write FID and Inception Score to /app/metrics.json, ensuring no NaNs and FID below a set threshold.', NULL, NULL),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Train a compact Transformer for character-level language modeling on long sequences, implementing stabilization features: gradient clipping (by norm and value), warmup+cosine LR scheduling, dropout, label smoothing, and EMA/SWA of weights. Provide a CLI to ablate each component, log per-step gradient norms and NaN/Inf events to /app/stability_log.json, and save the best-perplexity checkpoint.', NULL, ARRAY['modeling']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Training Stabilization Techniques', 'Write a PyTorch training script for CIFAR-10 that stabilizes an intentionally aggressive setup (large LR, small batch) using mixed precision with dynamic loss scaling, adaptive gradient clipping (AGC), linear warmup plus cosine decay scheduling, and EMA of weights with switchable BatchNorm/GroupNorm and dropout. Tests verify no NaNs/Inf, bounded gradient norms, correct LR schedule behavior, and a validation curve that is smoother and at least as good as a baseline without these techniques.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Build a CLI tool that instruments any PyTorch model with forward/backward hooks to record per-layer activation, weight, and gradient statistics during training, automatically flagging saturation, dead ReLUs, and vanishing/exploding gradients while rendering concise PNG plots and a JSON report. The script should run a short CPU-only training of a small CNN on synthetic data and output /app/reports/{grad_flow.png, activations.png, weights_hist.png, anomaly_report.json}.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Build a Python CLI that loads a PyTorch ResNet-18 and, via forward/backward hooks on a small image batch, saves per-layer activation/gradient histograms (PNGs), a gradient-flow plot, and a dead-ReLU heatmap. Write a debug_report.json summarizing per-layer gradient norms and flagging any vanishing/exploding gradients.', NULL, ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Create a PyTorch script that hooks into a ResNet-18 training on CIFAR-10 to record per-layer activation histograms, activation sparsity, and gradient norms across one epoch, automatically flagging layers with saturation or vanishing/exploding gradients. Produce a self-contained HTML report of these visuals and save Grad-CAM overlays for the top-3 misclassified images.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Create a Python CLI that registers forward/backward hooks on a given PyTorch model to capture per-layer activation histograms, gradient norms, and (if present) attention head entropy/rollout, then writes PNG plots and a single HTML report to /app/vis. The tool should flag vanishing/exploding gradients, NaN/Inf activations, and saturation rates per layer, exiting non-zero if any issue is detected.', NULL, ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Deep Learning & Neural Network Engineering', 'Visualization & Debugging', 'Implement a Python CLI that loads a provided PyTorch CNN and sample dataset, registers forward/backward hooks to compute per-layer activation stats (mean/std/sparsity) and gradient norms on a minibatch, and produces a gradient-flow plot plus activation histograms. Save a JSON report flagging dead ReLUs and vanishing/exploding gradients, and write all artifacts (JSON and PNGs) to /app/report so tests can verify detection of an intentionally broken layer.', NULL, ARRAY['python', 'pytorch']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Build an expert-parallel Mixture-of-Experts layer that shards experts across ranks and routes tokens via top-2 gating with a capacity factor, using all_to_all to dispatch to per-rank experts and recombining to the original order. Provide an unsharded reference and ensure numerical parity and gradient correctness across world_size 1/2/4, with support for dropless/drop policies, variable sequence lengths, and mixed precision.', 'hard', ARRAY['parallel']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement 1D tensor-parallel sharding for GPT-style attention and MLP in PyTorch with custom ColumnParallelLinear/RowParallelLinear layers using torch.distributed (Gloo), including forward/backward collectives (all-gather/reduce-scatter) and model-parallel RNG. Provide a test script that trains a tiny Transformer on synthetic data across 2–4 ranks and verifies numerical parity with an unsharded reference, correct gradient synchronization, and that memory per rank drops below a set threshold.', 'hard', ARRAY['parallel', 'pytorch', 'distributed']),
('Machine Learning & AI', 'Foundation Models & Large-Scale Systems', 'Data & Model Sharding', 'Implement CPU-only tensor parallelism for a single Transformer block using torch.distributed (Gloo), sharding QKV and MLP weights across two ranks with column/row-parallel matmuls and the necessary all-reduce/concat steps. Provide a script that verifies forward/backward parity with an unsharded baseline to 1e-5 and reports per-rank parameter and optimizer memory to demonstrate sharding benefits.', 'hard', ARRAY['distributed', 'parallel']);
