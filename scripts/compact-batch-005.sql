-- Compact batch 5/29: rows 201-250

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create a script that reassembles a split ZIP in /app/parts (dataset.zip.001, .002...), verifies integrity, and extracts its contents. Auto-detect and convert all text files to UTF-8 with LF line endings (leaving binaries unchanged), then repack the normalized tree into a deterministic /app/normalized.tar.xz with sorted entries, fixed timestamps/permissions, and write a SHA-256 manifest to /app/manifest.txt.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create a script that scans /app/inbox for .eml or mbox files, decodes MIME parts (base64 and quoted-printable) while honoring per-part charsets (UTF-8, ISO-8859-1, Windows-1252, UTF-16LE), and extracts only CSV or CSV.GZ attachments. Convert all text to UTF-8 with LF line endings, decompress .gz attachments, deduplicate identical rows, and write a single merged /app/combined.csv.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Create two scripts: one that normalizes a directory by auto-detecting and converting text file encodings to UTF-8 with LF line endings (leaving binaries intact) and then produces a deterministic tar.gz (lexicographic order, fixed uid/gid/uname/gname, mtime from SOURCE_DATE_EPOCH) along with a MANIFEST.sha256 of pre/post hashes. The second script verifies determinism by re-packing and comparing the tarball and manifest, reporting any mismatches.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Encoding & Compression', 'Implement a script that recursively normalizes a mixed-encoding dataset in /app/inbox: detect each text file’s encoding (BOM-aware; fallback heuristic), convert to UTF-8 without BOM, normalize Unicode to NFC, unify line endings to LF, and transparently gunzip any *.gz logs. Package the result as a deterministic tar.gz at /app/normalized.tar.gz (sorted entries, uid/gid=0, mtime=0) and write /app/manifest.csv summarizing original_path, output_path, original_encoding, original_eol, was_gzipped, bytes_before, bytes_after, and sha256_after.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a CLI script that reads all multi-document YAML files in /app/k8s (resolving anchors/aliases) and converts each document into a normalized JSON object with dot-notated keys, outputting a consolidated JSON Lines file at /app/resources.jsonl. Then convert that JSONL to Parquet at /app/resources.parquet with inferred column types, lexicographically ordered columns, and an added source_filename field.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a CLI that converts a directory of heterogeneous CSV files (mixed encodings: UTF-8/UTF-16/ISO-8859-1; varying delimiters and decimal separators) into a single UTF-8 Parquet dataset at /app/output using a provided schema.yml for field names and types. The tool must auto-detect each file’s encoding and dialect, standardize dates and numerics, and write a detection_summary.json reporting per-file encoding, delimiter, quotechar, decimal separator, and any type coercions.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a script that converts a directory of YAML documents that use anchors, aliases, and merge keys into a single NDJSON file, fully resolving all references and merges. Each line must be a canonicalized JSON object with deterministically sorted keys, ISO-8601-normalized timestamps, and an added source field containing the original filename.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'File Format Conversion', 'Create a script that converts all XML files in /app/input_xml (with multiple namespaces) into YAML files in /app/yaml, representing attributes with an @ prefix, text with _text, preserving child order, and turning repeated tags into sequences. If any element named Data contains base64 text, decode it to /app/blobs/<stem>_<n>.bin and replace the YAML value with the relative blob path.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Create a CLI script that infers a JSON Schema (Draft 2020-12) from all NDJSON files in /app/samples, unioning fields and types, distinguishing integer vs number, detecting nullability, min/max, enum for low-cardinality strings, and formats (date-time, uuid, email). Validate /app/new_records.jsonl against the inferred schema, writing the schema to /app/schema.json and a newline-delimited list of invalid record indices with short reasons to /app/invalid.txt.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Create a command-line tool that scans /app/datasets for mixed CSV and JSONL files, infers a unified typed schema (nullability, unions, enums from low-cardinality strings, and date/time/decimal detection) and emits a JSON Schema Draft 2020-12 file. Validate every record against the schema with a detailed per-file report of failures (row number and reason), and write normalized, UTF-8, type-coerced Parquet outputs conforming to the inferred schema.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Create a script that infers a relational schema from CSVs in /app/training (column types, primary keys, and foreign keys via uniqueness and value-domain overlap). Validate CSVs in /app/incoming against the inferred schema, writing /app/schema.json and /app/report.txt with type mismatches, duplicate PKs, and FK violations, handling delimiter and encoding detection.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Implement a CLI that infers a JSON Schema (Draft 2020-12) from /app/samples/*.jsonl by detecting nested field types (including unions), required/optional via coverage threshold, numeric min/max, and common string formats (date-time, email, uuid), then writes /app/schema.json. Validate /app/target.jsonl against the inferred schema and produce /app/validation_report.json with per-record JSON Pointer paths of violations and a summary of counts by rule.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Schema Inference & Validation', 'Infer a column schema from a corpus of heterogeneous delimited files in /app/samples (auto-detect delimiter, header presence, and encoding), determining data types, null tokens, and required fields (present in ≥98% of rows), and emit an Arrow-compatible schema to /app/schema.json. Validate /app/batch.csv against that schema, writing /app/invalid_rows.csv with columns row,field,error for all violations and also produce a normalized UTF-8, comma-delimited, headered file of the valid rows at /app/valid.csv.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Create a script that parses multiple timestamped lsof snapshots in /app/snapshots, correlates PIDs across files to compute per-process file-descriptor deltas and type breakdowns (REG/SOCK/FIFO), and flags processes with monotonically increasing counts as potential leaks. Write a ranked summary to /app/leaks.txt listing PID, command, total growth, and per-type growth for the top offenders.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Create a script that scans /app/logs for application logs (including rotated .log and .log.gz files) with RFC3339 timestamps and multi-line Java stack traces, grouping each exception event as the ERROR line plus its following stack frames and normalizing all timestamps to UTC. Output /app/errors.csv listing exception_type, sha1_message_signature (message with volatile values like hex addresses stripped), first_seen, last_seen, count, top_3_frames (semicolon-separated), and up to five associated reqId values (comma-separated).', NULL, ARRAY['java']),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Create a script that scans /app/logs for web access logs in mixed formats (Apache combined, Nginx default, and JSON-lines), including rotated .gz files, normalizes them to a common schema, and writes a timestamp-sorted /app/normalized.ndjson. Then compute per-hour aggregates (requests, unique sessions using a 30-minute inactivity window, 4xx/5xx rates, and P50/P95 latency) and save a 2-space-indented JSON report to /app/traffic_report.json.', NULL, ARRAY['web']),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Implement a script that scans /app/logs for mixed-format server logs (nginx access, JSON app logs, and syslog; some gzipped), correlates events by a shared request_id, and normalizes timestamps to UTC to reconstruct per-request timelines. Output timeline.csv with per-request start/end/status and phase durations, plus slow_requests.txt listing the 20 slowest requests with the inferred bottleneck phase.', NULL, NULL),
('Data Processing & Scripting', 'File Parsing & Format Conversion', 'Text & Log Parsing', 'Ingest mixed-format service logs from /app/services/* (Apache/Nginx text, JSON app logs, and RFC5424 syslog), normalize core fields, and correlate events by request_id to reconstruct per-request timelines and end-to-end latencies. Output traces.jsonl (one record per request with ordered spans and missing-service gaps) and summary.csv with per-endpoint counts, mean, and P95 latency.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Create a Bash script that performs rotating, hard-link deduplicated snapshots of /app/source into /app/backups using rsync --link-dest, honoring patterns from /app/.backupignore and default excludes (.git, node_modules, *.tmp). Each run must atomically create a timestamped snapshot, enforce a 7-daily/4-weekly/6-monthly retention policy with pruning, emit a manifest with SHA-256 for new/changed files, compute total and dedup-saved bytes, and append a CSV summary to /app/backup_report.csv.', 'hard', ARRAY['git']),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Create a POSIX shell backup tool that performs rsync-based hard-link snapshots of /data into /repo/YYYY-MM-DD using --link-dest for incrementals, generates a JSON manifest with per-file SHA256 and total logical size, then zstd-compresses and age-encrypts the manifest with a provided public key. Implement verify and restore modes that reconstruct the latest snapshot into /restore (preserving sparse files, symlinks, permissions, and mtimes) and enforce GFS retention (7 daily, 4 weekly, 6 monthly) while producing a human-readable report.txt.', 'hard', NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Implement a Bash snapshot-backup tool that reads /app/backup.toml for include/exclude globs and a retention policy, creates timestamped rsync snapshots with --link-dest under /app/backups, then tars each snapshot, compresses with zstd, and encrypts with age using a provided public key. Provide verify and restore modes that generate/validate a SHA256 manifest per snapshot and prune snapshots per policy.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Write a command-line script that creates time-stamped snapshot backups of /app/source, using rsync with --link-dest for unchanged files and packing small files (<256 KiB) into a single zstd-compressed tar encrypted with GPG, honoring exclude patterns from /app/.backupignore and preserving permissions, symlinks, xattrs, and ACLs. Implement a restore mode that reconstructs to /app/restore from a chosen snapshot by merging the snapshot tree with the decrypted tarball and verifies integrity against a SHA-256 manifest, pruning old snapshots according to /app/retention.yaml.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Archival & Backup Scripting', 'Write a script that creates timestamped, rsync-based snapshot backups of /app/source into /app/snapshots using hard links for unchanged files, enforces a retention policy defined in /app/retention.json (daily/weekly/monthly), and emits a per-snapshot manifest with SHA256 checksums and sizes. Include restore and verify modes: restore a named snapshot to /app/restore and scan all snapshots to produce /app/verify_report.txt listing missing files and any checksum mismatches.', 'hard', NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create a command-line script that scans /app/photos for JPEG/HEIC images, extracts EXIF capture timestamp and camera model, renames each to YYYYMMDD_HHMMSS_{make-model}.{ext} (collisions resolved with _01, _02...), de-duplicates by SHA256 hash, and moves files into /app/library/YYYY/MM/ directories. Produce a CSV at /app/ingest_log.csv listing original_path,new_path,hash,was_duplicate,source_timestamp_used (exif|mtime), and write a JSON summary (/app/summary.json) with counts by camera and month.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create a script that recursively scans /app/photos, reads EXIF capture time and camera model from image files, renames them to YYYYMMDD_HHMMSS_[Model]_[seq].ext, and moves them into /app/library/YYYY/MM/DD/. Exact duplicates (by SHA-256) should be moved to /app/duplicates and /app/manifest.csv must map each original path to its final path and status (moved, duplicate, exif_fallback).', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create a script that scans /app/photos, renames images to YYYY-MM-DD_HHMMSS_{camera}.ext using EXIF (falling back to mtime), and moves them into /app/library/YYYY/MM/DD while leaving non-images untouched. Detect exact and near-duplicate photos via perceptual hashing, keep a single canonical copy in the dated folder, move duplicates to /app/library/_duplicates, and write a manifest CSV mapping original paths to final locations with hashes and duplicate-of entries.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Create two CLI scripts: one scans /app/input for images/videos, extracts capture date from EXIF or filename fallback, deduplicates by perceptual hash, moves a canonical copy to /app/library/YYYY/MM/ with a normalized slug while replacing duplicates with relative symlinks, and writes a manifest.json. The second script uses manifest.json to fully restore the original directory layout and filenames.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Batch File Operations', 'Write a script that scans /app/incoming for images/videos, extracts capture timestamps from embedded metadata (falling back to file mtime), and batch renames/moves them to /app/library/YYYY/MM/YYYYMMDD_HHMMSS_{counter}.{ext}, keeping sidecar files (.xmp/.srt) in sync. On timestamp collisions, deterministically order and increment the counter without gaps and output /app/manifest.csv mapping old_path,new_path for every moved file.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Create a command-line tool that scans /app/project to identify orphaned media assets (e.g., png/jpg/svg/gif/pdf/mp4) not referenced by any Markdown/HTML/YAML/CSV files via relative links, front matter, or image tags. The tool must respect .gitignore rules, safely handle symlinks, decode URL-encoded paths, and output orphaned files sorted by size descending to /app/orphans.txt.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Create a script that recursively scans /app/docs for Markdown files, extracts relative image links (e.g., ![...](./img/foo.png)), and verifies that each referenced file exists (ignoring absolute URLs and anchors). Write a JSON array to /app/missing_images_report.json listing missing references with fields: md_file, line, ref, and resolved_path, sorted by md_file then line.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Create a script that recursively scans /app/workspace for filenames that normalize to identical casefolded NFC forms (to detect Unicode confusables/collisions) and flags names containing invisible or bidirectional control characters. Write /app/filename_audit.csv listing original absolute path, normalized form, issue type(s), and any colliding peers, sorted deterministically.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Scan /app/docs for Markdown files, extract relative image and link targets, canonicalize paths, and compare against the real files under /app/docs (excluding hidden and build artifacts). Write /app/report.json with two sorted arrays: missing_references (referenced but absent) and orphan_files (present but never referenced), using paths relative to /app/docs.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'File Discovery & Search', 'Write a script that recursively scans /app/site, finds all asset files under /app/site/assets (extensions: png,jpg,jpeg,svg,gif,mp3,mp4,pdf,webp) and reports those not referenced by any Markdown/HTML/CSS/JS file in the project. Ignore directories .git, node_modules, dist, and build; output a sorted newline-separated list of orphan paths to /app/orphans.txt.', NULL, ARRAY['git']),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Build a CLI that recursively scans a directory and emits a deterministic JSONL manifest per entry (path, type, size, mode, uid/gid, nlink, inode, mtime, SHA-256 for files, symlink targets, and summarized xattrs/ACLs), plus anomalies.csv flagging world-writable/SUID/SGID files, broken symlinks, future timestamps, and duplicate content by hash. Provide a subcommand to diff two manifests and output delta.csv enumerating added/removed/changed items with standardized change reasons for audit.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Create a script that recursively inventories /app/project, extracting normalized metadata for every file, directory, and symlink (path, type, size, mode octal, uid/gid, mtime in UTC ISO 8601, inode+device), computing SHA-256 only for regular files ≤50 MB and recording symlink targets, while skipping paths ignored by the repo’s .gitignore. Write a deterministic, path-sorted JSON Lines manifest to /app/manifest.jsonl and a /app/summary.txt with counts by type, total bytes, detected hardlink groups, and duplicate files grouped by content hash.', 'hard', NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Implement a CLI that recursively audits a directory and writes a JSONL snapshot of each item’s metadata (path, type, size, permissions, owner/group, mtime/ctime, inode, link count, device, xattrs if available) plus a SHA-256 for regular files. When given a prior snapshot, generate a changes report listing added/removed/modified files with old/new metadata and per-extension aggregates, avoiding symlink loops and not crossing filesystem boundaries.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Write a script that recursively audits /app/site and produces /app/manifest.csv in deterministic path order with per-entry path, type, mode (octal), uid, gid, size, mtime (UTC ISO), inode, nlink, and either sha256 for regular files or symlink_target for symlinks. Additionally, generate /app/anomalies.log listing any world-writable entries, files with setuid/setgid bits, and broken symlinks, one issue per line.', NULL, NULL),
('Data Processing & Scripting', 'File System Operations', 'Metadata Extraction & Logging', 'Write a script that recursively scans /app/data, producing a manifest (CSV) of every path with type, size, mtime (UTC), permissions (octal), uid/gid, inode, hardlink count, and SHA-256 for regular files, plus symlink targets. Generate a separate report that groups hard-linked files and content-duplicate files (same hash but different inodes) and flags world-writable or SUID/SGID entries.', 'hard', NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Build a command-line tool that fetches all tickets updated since the last run from a GitHub-like REST API using pagination and ETag-based conditional requests, storing normalized records in /app/issues.sqlite. After syncing, compute a summary (new/updated counts, per-label totals, top 3 authors) and POST it as JSON to a provided webhook URL, honoring rate-limit headers with retry/backoff and writing the exact payload to /app/sync_report.json.', NULL, ARRAY['rest', 'api']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Create a command-line script that reads SKUs from /app/skus.csv and an API key from the environment, then queries a paginated inventory/pricing REST API with conditional GETs (ETag/If-None-Match) and backoff for 429 to produce /app/inventory.csv. For any items below the reorder threshold in the input, batch POST a JSON payload to a /reorders endpoint and write the response to /app/reorder_response.json.', NULL, ARRAY['api', 'rest']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Implement a CLI that submits a batch job to a REST API (POST returns 202 + Location), then polls with exponential backoff and ETag/Retry-After handling to download paginated JSON results into a SQLite database with idempotent upserts. Compute daily aggregates and POST an HMAC-signed summary to a webhook, persisting a resume cursor and writing only OK or ERROR to /app/result.txt.', NULL, ARRAY['rest', 'api', 'database']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Write a CLI that reads SKUs from /app/skus.csv, authenticates via OAuth2 to a provided mock REST API, and fetches product metadata through paginated endpoints while respecting rate limits using retries and ETag-based conditional requests. Normalize and deduplicate variants, then POST an idempotent batched inventory snapshot to a submission endpoint and emit /app/run_report.json summarizing cache hits, retries, and per-batch results.', NULL, ARRAY['rest', 'api']),
('Data Processing & Scripting', 'Integration with External Systems', 'API Data Fetching & Posting', 'Write a script that reads /app/cities.txt, fetches the last 48 hours of PM2.5/PM10 per city from a public air-quality API (e.g., Open-Meteo AQ) using curl or Python requests with ETag-based caching and retry-on-429, aggregates per-city min/mean/max, and writes /app/aq_summary.csv. Then POST a signed (HMAC-SHA256) JSON summary to a provided local webhook endpoint and print the returned confirmation ID.', NULL, ARRAY['api', 'python']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Against a local S3-compatible endpoint (MinIO) with credentials in /app/.env, implement a CLI-driven sync that uploads /app/dataset to a bucket, applies per-file metadata and content-type from /app/metadata.csv, enforces multipart uploads for files >8 MB, and verifies integrity by re-downloading and comparing SHA-256. Emit /app/report.json listing key, size, content-type, user metadata, ETag, sha256_ok, and 10-minute presigned URLs for the five largest objects.', NULL, NULL),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Build a policy-driven cross-cloud sync tool that mirrors an AWS S3 prefix to a GCS bucket, preserving Content-Type/Cache-Control and applying include/exclude and rename rules from /app/policy.yaml; use checksum-aware diffing that reconciles S3 ETags (including multipart) with GCS CRC32C/MD5 to avoid redundant transfers. Support dry-run and apply modes, persist a local SQLite manifest for idempotency, and write /app/report.json listing created/updated/deleted/skipped objects and any conflicts.', NULL, ARRAY['cloud', 'aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Create a command-line tool that syncs /app/dataset to both an AWS S3 bucket and a GCS bucket, reading include/exclude rules from /app/.syncignore and per-file metadata (Content-Type, Cache-Control, custom tags/labels) from /app/metadata.json. The script must perform a dry-run and then an actual run, preserve metadata, use multipart/resumable uploads, verify integrity via checksums (ETag/MD5 for S3 and CRC32C for GCS), and emit a machine-readable reconciliation report at /app/sync_report.json listing created/updated/skipped objects and any mismatches across clouds.', NULL, ARRAY['aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Create a script that audits and reconciles a Google Cloud Storage bucket prefix to match an AWS S3 bucket prefix: copy missing/outdated objects, preserve Content-Type and user metadata, and delete GCS extras only if not modified in the last 10 minutes. Output /app/audit_report.csv listing key, action (copied, updated, deleted, skipped), source ETag, destination hash, and size, with exponential backoff retries for transient errors.', NULL, ARRAY['cloud', 'aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Cloud Storage Interaction', 'Write a script that uses AWS CLI against a provided S3-compatible endpoint to perform an idempotent two-way sync between the remote prefix s3://tb-datasets/projects and /app/cache, honoring patterns from /app/.syncignore and verifying integrity via ETag/MD5. Persist object metadata (Content-Type, Cache-Control) to /app/cache/_metadata.json and emit /app/sync_report.txt with counts of added, updated, and removed items; delete locals only when a --prune flag is given.', NULL, ARRAY['aws']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Create a CLI script that connects to a PostgreSQL database using credentials from /app/db.env, runs a parameterized denormalizing query over customers, orders, and payments to produce per-customer summaries (first/last order, order count, total paid, outstanding balance) with emails masked and reconciliation checks enforced. Export results as partitioned NDJSON files by last_name initial under /app/export/ and write a manifest.json with per-partition row counts, min/max timestamps, and SHA256 checksums.', NULL, ARRAY['postgresql', 'database']),
('Data Processing & Scripting', 'Integration with External Systems', 'Database Query & Export', 'Create a command-line script that connects to a PostgreSQL database using credentials from /app/db.env, queries paid invoices between two dates (joining invoices, customers, and payments), masks PII (email SHA256, phone last 4), normalizes timestamps to UTC ISO-8601, and exports the result as month-partitioned NDJSON gzip files in /app/export sorted by invoice_id. Write a _manifest.json in the same folder with the query inputs, partition file names, row counts, and SHA256 checksums of each gzip.', NULL, ARRAY['postgresql', 'database']);
