-- Compact batch 16/29: rows 751-800

INSERT INTO task_inspiration (category, subcategory, subsubcategory, description, difficulty, tags) VALUES
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Create a custom Gymnasium environment backed by a SimPy discrete-event simulation of a two-station production line with stochastic arrivals and a controllable dispatch/batching policy. Train a Stable-Baselines3 PPO agent on CPU to minimize average job latency, logging learning curves and saving a checkpoint and evaluation report that demonstrates at least a 20% latency reduction versus a fixed-rate heuristic baseline.', NULL, ARRAY['logging']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Create and register a custom Gymnasium environment ''ThermalControl-v0'' whose dynamics (heat capacity, loss rate, actuator limits, and disturbance sequence) are loaded from /app/dynamics.yaml, and train a Soft Actor-Critic agent with stable-baselines3 (automatic entropy tuning) to keep temperature within a target band via domain-randomized episodes. Save the trained policy to /app/checkpoints and a 100-episode seeded evaluation log (time, state, action, reward) to /app/eval.csv for automated verification.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Environment Setup & Policy Training', 'Install Ray RLlib with PettingZoo and SuperSuit, configure the MPE simple_spread_v3 multi-agent environment, and train a shared-parameter PPO policy using vectorized parallel environments. Export the trained policy to TorchScript and run a seeded evaluation that writes per-agent rewards and coverage metrics to /app/mpe_eval.json.', NULL, ARRAY['parallel']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement a PSRO self-play pipeline for the Rock-Paper-Scissors-Lizard-Spock normal-form game, iteratively training best-response policies via no-regret updates against the evolving meta-strategy. After convergence, compute the mixed Nash distribution and exploitability, and write them to /app/solution.json, passing tests by staying below a specified exploitability threshold.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement a self-play Counterfactual Regret Minimization trainer for Kuhn Poker with a minimal game engine including chance nodes, information-set caching, and average-strategy export. The script should train to a low-exploitability policy verified by a best-response evaluator, be reproducible via seeding, and finish on CPU within tight time limits.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement a self-play training pipeline for Connect Four using OpenSpiel where a PyTorch policy–value network is optimized via AlphaZero-style MCTS. After training, evaluate against a depth-limited minimax baseline over 100 games and save win/draw/loss and Elo metrics to /app/results.json.', NULL, ARRAY['pytorch']),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Implement from-scratch CFR+ self-play for Kuhn Poker with a CLI to train for N iterations and compute exploitability via an exact best response. Save the average strategy to /app/kuhn_policy.json and ensure the final policy’s exploitability is ≤0.05 chips.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Multi-Agent & Self-Play Training', 'Train a near-Nash strategy for Kuhn Poker via self-play Counterfactual Regret Minimization (CFR/CFR+) using OpenSpiel (pyspiel), then serialize the average policy and compute exploitability. Produce a policy artifact and a metrics report demonstrating exploitability below a specified threshold.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Augment Gymnasium’s LunarLander-v2 with a configurable multi-objective reward balancing landing accuracy, fuel efficiency, and leg-contact stability, then sweep at least 32 coefficient sets where each trains a lightweight PPO agent for a fixed budget. Compute and save the Pareto frontier and knee-point selection with CSV/plots of episodic return, crash rate, and fuel use to verify the chosen reward balances competing objectives.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Design and evaluate multi-objective rewards for Gymnasium’s Taxi-v3 by augmenting the sparse reward with penalties for illegal pickup/dropoff and a discomfort cost proportional to action changes, implementing both scalarized (lambda-weighted) and Lagrangian-constrained formulations. Train a tabular Q-learning agent across a sweep of coefficients and report the Pareto frontier between episode length and discomfort with a baseline comparison to the original sparse reward.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Implement a Gymnasium-compatible ''Windy Keys-and-Doors'' gridworld and design a potential-based reward shaping function that balances goal completion, energy use, and safety while preserving the optimal policy. Provide a training/evaluation script that compares shaped vs. sparse rewards (e.g., with tabular Q-learning), demonstrating faster learning and fewer collisions, verifying equal optimal returns, and flagging reward-hacking behaviors.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'Implement a custom Gymnasium GridWorld with a sparse goal reward, time penalty, and stochastic hazard tiles, then compare two rewards: potential-based shaping via Manhattan-distance potential and a naive per-step progress bonus. Train PPO under both and report success rate, hazard contacts, and path optimality gap to highlight shaping invariance vs reward hacking.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Reward Design & Evaluation', 'In a 2D gridworld with a key, a locked door, and a fragile vase, implement and compare three rewards: sparse goal-only, potential-based shaping with step penalty, and a side-effect avoidance term that penalizes action impact relative to an inaction baseline using object-position L1 distance. Train a fixed PPO agent under each reward, detect reward hacking like key pick/drop loops, and produce a summary of goal success, vase collisions, loop frequency, and sample efficiency.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a disk-backed prioritized replay buffer daemon that ingests CartPole-v1 transitions from multiple local producer scripts over TCP, computes n-step returns across episode boundaries, and persists state for crash-safe restart. Provide a client to sample mini-batches with PER (alpha/beta) and importance weights, with a test harness that verifies sampling distribution, deterministic checksums for sampled batches, and replay continuity after restart.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a goal-conditioned 2D grid environment and generate large-scale rollouts to an on-disk replay buffer supporting Hindsight Experience Replay, n-step returns, and prioritized sampling with importance weights. Train a lightweight DQN for several thousand updates from this buffer and output success-rate curves and sampling diagnostics derived from the stored trajectories.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a memory-mapped, Zarr-backed replay buffer for Gymnasium Dict/Box spaces with prioritized experience replay, n-step returns, and sequence sampling for RNN policies (burn-in, overlap, zero-padding). Provide a CLI to run vectorized rollouts to populate the store, then deterministically sample batches under a fixed seed and output a JSON report verifying priority distributions, importance-sampling weights, and crash-safe resume behavior.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a multiprocessing rollout + learner system that generates goal-conditioned episodes and stores them in a segment-tree prioritized replay buffer with n-step returns and on-the-fly Hindsight Experience Replay relabeling. The buffer must support batched append/sample/update with importance-sampling weights, be mmap-backed to handle 10M+ transitions, and a training script should reach a target success rate while validating sampling distribution and bias correction.', NULL, NULL),
('Machine Learning & AI', 'Reinforcement Learning & Simulation-Based Training', 'Simulation Rollouts & Replay Buffers', 'Implement a prioritized experience replay buffer using a segment tree that supports n-step returns, sequence sampling for recurrent policies, and per-sample importance weights. Provide a CLI to collect rollouts from parallel Gymnasium environments, persist the buffer to disk in chunked files, then reload to verify sampling probabilities, TD-error updates, and n-step target consistency.', NULL, ARRAY['parallel']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Create a CLI pipeline that attacks a pretrained image classifier with EOT-PGD over random crops, rotations, and Gaussian noise, then reports robust accuracy and worst-case loss. Implement a defense via randomized smoothing with calibrated sigma to compute per-sample certified radii and write a CSV summarizing clean/robust accuracy and certificates for a test subset.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Implement a CPU-only Python CLI that loads a provided CIFAR-10 model, wraps it with a non-differentiable defense (bit-depth reduction plus randomized resizing), and evaluates robust accuracy under FGSM, PGD, and BPDA+EOT-PGD. Save per-attack metrics to /app/robust_metrics.json and 32 adversarial samples per attack to /app/adv/{attack}/, demonstrating that BPDA+EOT meaningfully reduces reported robustness compared to naive PGD.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Train a small CNN on MNIST (CPU) and implement FGSM and PGD-Linf attacks. Add a feature-squeezing detector (bit-depth reduction + median smoothing) and an adversarially trained variant, then report clean/robust accuracy at eps=0.3 and ROC-AUC for detection, saving metrics and checkpoints.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Train a small CNN on MNIST, implement FGSM and multi-step PGD (with Expectation-over-Transformation for stochastic defenses) to craft adversarial examples at multiple L∞ epsilons, and report clean/robust accuracies with saved adversarial image grids. Add PGD adversarial training and a randomized smoothing inference defense, then re-evaluate and output a JSON metrics report plus a file of estimated certified radii for 100 test samples.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Adversarial Robustness & Defense', 'Train a small CNN on a CIFAR-10 subset, wrap it with an L2 randomized smoothing certifier (configurable sigma), and compute per-example certified radii with 95% confidence for 200 test images, writing certificates.csv and a summary JSON. Implement an EOT-PGD attack that accounts for the smoothing noise and report robust vs attacked accuracies across L2 epsilons, verifying certified accuracy lower-bounds are not violated.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Build a CLI pipeline that trains a baseline binary classifier on /app/data.csv, computes demographic parity, equal opportunity, and disparate impact for sex and race, then applies Calibrated Equalized Odds post-processing to the model’s probabilities to reduce violations. Write a before/after report with bootstrap confidence intervals to /app/fairness_report.json and save learned group-specific thresholds/weights to /app/thresholds.json.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Build a CLI that loads /app/loans.csv containing features, a binary label y, and a sensitive attribute S, trains a logistic regression baseline, reports fairness metrics per group (demographic parity difference, equalized odds difference, disparate impact), then applies Kamiran–Calders reweighing and a per-group threshold optimizer to enforce equal opportunity within 2% while minimizing accuracy loss. Save before/after metrics and per-group confusion matrices to /app/fairness_report.json, persist the learned thresholds, and repeat the evaluation under a covariate-shifted test split using importance weighting to verify fairness generalization.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Create a Python CLI that loads /app/data.csv with features, label, and protected_attribute, audits a logistic-regression baseline via demographic parity difference, equalized odds difference, and calibration within groups, then mitigates bias using reweighting plus group-wise threshold optimization. Write a before/after fairness report to /app/fairness_report.json, requiring <2% accuracy drop and at least 50% reduction in each measured disparity.', NULL, ARRAY['python', 'optimization']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Create a Python CLI that trains a binary classifier on a tabular dataset, scans all intersectional subgroups (combinations of up to 3 sensitive attributes) to find the worst demographic parity difference and equalized odds violation, and then applies a per-group threshold optimizer to reduce violations below a target while keeping overall AUC within 2%. Output a before/after fairness report (JSON) and save the calibrated, debiased model and the learned group thresholds.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Bias Detection & Mitigation', 'Create a command-line pipeline that trains a credit-approval classifier, computes subgroup fairness metrics (demographic parity, equalized odds, disparate impact), and then applies reweighing plus threshold optimization to mitigate bias. Save a before/after fairness report JSON, per-group decision thresholds, and the mitigated model artifact.', NULL, ARRAY['optimization']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a CLI pipeline that scans a raw text corpus, detects PII (emails, phones, names, addresses) via regex+NER, applies consistent pseudonymization, and produces both a redacted dataset and a provenance ledger with SHA-256 hashes, source URLs, and SPDX license IDs. Then evaluate a fine-tuned text generation model for privacy compliance by running canary extraction and membership inference tests, emitting a JSON report of leakage metrics and pass/fail flags against given thresholds.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a CLI that audits a JSONL training dataset and a local text-generation model for policy compliance by verifying data provenance (required source_url and license in an allowed whitelist) and detecting/redacting PII (emails, phones, addresses, SSNs) with deterministic hashing. Run a fixed red-teaming prompt suite against the model to flag PII leakage and disallowed content, then emit a compliance_report.json with per-rule counts, sample snippets, thresholds, overall pass/fail, and a sanitized_data.jsonl output.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a Python CLI that scans /app/data/{train,test} and /app/logs/inference.log for PII (names via a bundled dictionary, emails, phones, IPs, street addresses, government IDs) and validates that each record has provenance in /app/metadata.jsonl with a source, timestamp, and SPDX license from an allowed list. Automatically redact violations, emit a tamper-evident audit to /app/compliance_report.json with SHA-256 hashes and a redaction map, and exit non-zero if any unredacted PII or disallowed licenses remain.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Build a streaming Python CLI that ingests a large CSV of free-text training data plus a whitelist of permitted licenses, detects and redacts PII (names, emails, phone numbers, addresses, SSNs) via regex and spaCy NER, and validates each row’s provenance against a provided source-to-license map. Output a JSONL compliance report with SHA-256 hashes of raw and redacted text, per-row PII categories found/redacted, license verdicts, and exit non-zero if any violation exceeds configured thresholds.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Ethical & Policy Compliance Validation', 'Create a terminal CLI that validates a dataset’s licensing and provenance for model training by parsing per-file metadata and SPDX identifiers, verifying cryptographic hashes against a manifest, and checking compatibility with a specified usage policy (e.g., commercial use). The tool should emit a machine-readable compliance report and a symlinked approved/ subset, failing with clear diagnostics for missing, incompatible, or conflicting licenses.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Build a CPU-only CLI that trains a simple MLP on a tabular dataset and computes per-sample feature attributions using LIME, KernelSHAP, and Integrated Gradients, normalizing outputs into a common schema. Export JSONL attributions, a global feature ranking, and quantitative consistency metrics (additivity/completeness checks, Kendall tau agreement, and deletion/insertion AUCs) to predefined paths.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Create a CLI tool that trains a scikit-learn tabular classifier on a provided CSV, computes per-sample SHAP (TreeExplainer) and LIME attributions, and writes unified explanations to /app/explanations.json with global summaries. Evaluate faithfulness by producing deletion/insertion curves that mask features by attribution rank, outputting Kendall rank agreement and AUC metrics to /app/faithfulness.csv with deterministic seeding and memory‑efficient batching.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Create a CPU-only Python CLI that trains a scikit-learn RandomForestClassifier on /app/data/train.csv (binary target column ''y''), computes per-sample feature attributions with SHAP TreeExplainer and LIME TabularExplainer on the test split across 10 random seeds, and quantifies explanation stability via Kendall tau of top-10 feature rankings for each sample. Save per-method global summaries (mean absolute attribution), per-sample top-10 attributions, and a comparison report with median stability and method win-rate to /app/explainability/, and exit with nonzero status if SHAP''s median stability is lower than LIME''s.', NULL, ARRAY['python']),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Implement a CLI that explains predictions of a pretrained HuggingFace sentiment model on a text file using both SHAP and LIME, repeating each method over multiple random seeds to quantify stability (Kendall tau of token-importance ranks and top-k Jaccard overlap). Write a JSON summary of stability metrics, a CSV of per-example attributions, and HTML token heatmaps for the most unstable cases.', NULL, NULL),
('Machine Learning & AI', 'Responsible AI & Model Robustness', 'Explainability & Interpretability', 'Train a gradient boosting classifier on the UCI Adult dataset and generate local explanations for 20 test instances using both SHAP and LIME. Implement a faithfulness evaluation via deletion/insertion curves over ranked features, report AUC metrics for each method, and save per-instance attributions (JSON) and summary plots to /app/.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Build a CLI tool that ingests a set of NORAD TLEs and a ground-station coordinates file, propagates each orbit with SGP4 over a 24-hour window, and computes visibility passes above a given elevation threshold (AOS/LOS times, max elevation, range). Write per-satellite CSV summaries and an aggregated ICS calendar of visible passes.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Build a Python CLI that reads a time–flux light curve from /app/lightcurve.csv, robustly detrends it, and uses a Box Least Squares search to detect the strongest transiting-planet signal. Write the inferred period, transit epoch, depth, duration, and SNR to /app/transit.json and save the phase-folded, binned light curve to /app/phase.csv.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Implement a CLI tool that ingests asteroid astrometric observations in MPC 80-column format, computes a preliminary orbit (e.g., Gauss) and refines it via weighted least-squares differential corrections. Propagate the fitted state to a target UTC with a two-body Kepler solver and write osculating elements, residual RMS, and a topocentric RA/Dec ephemeris from a specified observatory code.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Implement an exoplanet transit search tool that reads a stellar light curve, detrends it, and runs a Box Least Squares period search over a specified grid with careful, unit-aware time handling. Output the best-fit ephemeris (period, epoch, duration, depth) with detection metrics and a phase-folded, binned curve to standardized files.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Astronomy & Astrophysics Computation', 'Implement an initial orbit determination and refinement pipeline that reads multi-epoch asteroid astrometry (UTC, RA, Dec, observatory) to compute a heliocentric two-body solution using Gauss’ method followed by nonlinear least-squares differential corrections. Output Keplerian elements at a target epoch, a 6×6 covariance estimate, propagated ephemerides, and postfit residual statistics.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Build a CLI tool that reads CF-compliant NetCDF files of monthly precipitation and potential evapotranspiration, computes the 12-month SPEI per grid cell via log-logistic fitting with robust handling of missing values, and writes a CF-compliant NetCDF of SPEI. Additionally, produce a CSV time series of global land-area fraction in drought (SPEI <= -1) for each month.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Build a zero-dimensional energy balance climate model that ingests historical radiative forcing and GMST series, estimates the climate feedback and effective heat capacity via constrained least squares, then projects global-mean temperature through 2100 under provided forcing scenarios. Output the fitted parameters, hindcast skill metrics, and scenario trajectories to standardized files.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Create a Python CLI that reads daily gridded NetCDF temperature and precipitation and computes three ETCCDI indices—TX90p, CDD, and PRCPTOT—per grid cell for a user-specified baseline, honoring CF calendars, masks, and leap years, and writes CF-compliant NetCDF outputs. Also compute area-weighted global and region-mean time series using cell areas and save a summary CSV.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Climate & Environmental Modeling', 'Implement a two-box global energy balance model (mixed-layer plus deep ocean) driven by a provided radiative forcing time series to simulate global-mean temperature and ocean heat uptake. Calibrate feedback, heat capacities, and exchange parameters against historical GMST and OHC data, then output ECS, TCR, fitted parameters, and scenario projections.', NULL, NULL),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Build a Python CLI that loads an SBML metabolic model and uses COBRApy with a GLPK solver to perform FBA, then evaluates growth under multiple media conditions and all single-gene knockouts to identify essential genes. Save growth rates per condition and the essential gene list to standardized JSON/CSV outputs.', NULL, ARRAY['python']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Create a BioPython-based tool that reads a coding DNA sequence and a list of restriction enzymes, then computes the minimal set of synonymous mutations needed to remove all listed recognition sites while preserving the protein sequence. Ensure no new listed sites are created, favor host-preferred codons to keep GC content within ±5% of the original, and output the mutated sequence and a CSV change log.', NULL, ARRAY['coding']),
('Scientific Computing & Analysis', 'Domain-Specific Computation', 'Computational Chemistry & Biology', 'Create a Python CLI that loads a metabolic network SBML (/app/model.xml), builds the stoichiometric matrix, and performs flux balance analysis to maximize a specified biomass reaction using linear programming. Validate mass balance and bounds, identify exchange reactions, then write the optimal flux vector and dual shadow prices to CSV files.', NULL, ARRAY['python']);
