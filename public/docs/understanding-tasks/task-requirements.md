# Task Requirements

This page lists all requirements your task must meet to pass review. Use this as a reference during development.

---

## Structural Requirements

All task submissions must include and be organized within a single directory containing the following file structure and metadata.

| File | Required | Description |
| :--- | :---: | :--- |
| **`task.toml`** | ‚úÖ | **The Manifest File.** Includes the following required keys: |
| ‚Ü≥ `task_id` | ‚úÖ |   Unique identifier for the task. |
| ‚Ü≥ `task_type` | ‚úÖ | Primary category from the 9-type taxonomy. |
| ‚Ü≥ `task_subtypes` | ‚úÖ | List of challenge areas (e.g., Long Context, DB Interaction). |
| ‚Ü≥ `difficulty` | ‚úÖ | Tier based on frontier model pass rates. |
| ‚Ü≥ `codebase_scale` | ‚úÖ | Size of environment context (None, Small, Large). |
| ‚Ü≥ `number_of_milestones` | ‚úÖ | Total checkpoints (use `0` if not a milestone task). |
| ‚Ü≥ `milestone_descriptions` | ‚úÖ | Detailed success criteria for each phase. |
| ‚Ü≥ `relevant_languages` | ‚úÖ | List of programming languages used in the task. |
| ‚Ü≥ `tags` | ‚úÖ | 3-6 free-form keywords for tools/libraries (e.g., FFmpeg, Redis). |
| ‚Ü≥ `runtime_limits` | ‚úÖ | Defined timeouts for agent, verifier, and build. |
| **`instruction.md`** | ‚úÖ | Precise, human-style task instructions and requirements. |
| **`environment/Dockerfile`** | ‚úÖ | Environment setup (or `docker-compose.yaml` for multi-container). |
| **`solution/solve.sh`** | ‚úÖ | The reference Oracle solution script. |
| **`tests/test.sh`** | ‚úÖ | The main test runner (must produce the final reward file). |
| **`tests/test_outputs.py`** | ‚úÖ | Deterministic state validation (Pytest). |
| **`rubrics.txt`** | ‚úÖ | Process-based evaluation.
| **`README.md`** | üí° | *[Optional]* Additional documentation or contributor notes. |


> **Pro-Tip for Directory Hygiene:**
> Always ensure your `tests/` directory is self-contained. If your task transitions from a Python unit test to a Playwright UI test, remember to update your `task.toml` metadata to reflect the correct verifier type and ensure the `Dockerfile` has the necessary browser dependencies.

<pdf-download src="/Terminus-EC-Training-stateful/template-task.zip" title="Download Task Skeleton Template"></pdf-download>

---

## instruction.md Requirements

Your `instruction.md` **must strictly** adhere to the following length and structure constraints:

### 1. Problem Description (1-2 Paragraphs)
Provide a high-level overview of the issue or the goal. State the "why" and the current state of the environment.
* **Do:** Use technical shorthand where appropriate.
* **Don't:** Provide a history of the programming language or unnecessary fluff.

### 2. Requirements (Max 2 Paragraphs / 20 Bullets)
List the specific constraints, expected outputs, or milestones. 
* **Limit:** No more than 20 bullet points total.
* **Clarity:** Ensure that requirements are measurable but not "spoon-fed." The agent should have to reason about *how* to implement the requirement.

```markdown
# Task Title

Clear, unambiguous instructions for the agent.

## Requirements
- All requirements explicitly listed
- Use absolute paths (/app/file.txt)
- Specify output file names
- Define data schemas completely
```

### Writing Quality Standards

- **Human-written** ‚Äî No LLM-generated task descriptions
- **Clear and unambiguous** ‚Äî A first-time reader should understand completely
- **Explicit requirements** ‚Äî Don't assume anything; state everything
- **Absolute paths** ‚Äî Always use `/app/file.txt`, never relative paths
- **Named outputs** ‚Äî Tell the agent exactly what files to create and where
- **Markdown formatting** ‚Äî Use proper markdown for readability

> **Consult the [Prompt Styling Guide](/portal/docs/reference/prompt-styling)**


## task.toml Requirements

**Task Metadata:** A task.toml file that contains required metadata such as:
* Difficulty: The accuracy (out of 5 attempts) for the frontier models described in the Difficulty section.
* Task Type: Each task must have exactly one task type from the list of tasks defined in the Task Type section.
* Task Subtypes: Each task will have a list of subtypes that apply to specific challenge areas. See the Task Subtypes section for more information.
* Number of Milestones: The number of milestones present in the task.
* Milestone Descriptions: Full text descriptions of what was accomplished at each milestone.
* Codebase Context Scale: The size of the environment with regards to the number of files in the project. See the Project Scale section for more information.
* Runtime Limits: Each task must specify timeouts, including maximum agent runtime (agent_timeout_sec), maximum verifier runtime (verifier_timeout_sec), and maximum environment build runtime (environment_build_timeout_sec), to ensure tasks are bounded and reproducible.
* Tags: Each task must include ~3-6 descriptive tags in the manifest. Tags are free-form keywords that capture important tools, libraries, techniques, or subtopics relevant to the task.

```toml
version = "1.0"

[metadata]
difficulty = "medium"
category = "debugging"
tags = ["category", "type", "difficulty"]
author_name = "anonymous"
author_email = "anonymous"

[verifier]
timeout_sec = 120.0

[agent]
timeout_sec = 600.

[environment]
build_timeout_sec = 600.0
cpus = 1
memory_mb = 2048
storage_mb = 10240
```

---

## Solution Requirements

### solve.sh Standards

```bash
#!/bin/bash
set -euo pipefail

# Your solution commands here
```

- **Human-written** ‚Äî No LLM-generated solutions
- **Deterministic** ‚Äî Same output every run (use seeds for randomness)
- **Self-contained** ‚Äî All logic in the script, no external dependencies
- **Idempotent** ‚Äî Can run multiple times safely

### What Makes a Good Solution

‚úÖ Good:
```bash
# Clear, step-by-step commands
cd /app
python process_data.py --seed 42 > output.txt
```

‚ùå Bad:
```bash
# Relies on random behavior
python script.py  # Non-deterministic output
```

---

## Test Requirements

### test_outputs.py Standards

```python
"""Test docstring explaining what this test file validates."""

def test_output_file_exists():
    """Verify that the required output file was created."""
    assert Path("/app/output.txt").exists()

def test_output_format():
    """Verify output follows the specified JSON schema."""
    # Test implementation
```

### Test Quality Checklist

- [ ] **Docstrings on every test** ‚Äî Explain what behavior is being tested
- [ ] **Test behavior, not implementation** ‚Äî Verify outcomes, not methods
- [ ] **One test per requirement** ‚Äî Each requirement has a corresponding test
- [ ] **No leaked answers** ‚Äî Tests shouldn't reveal the solution
- [ ] **Anti-cheating measures** ‚Äî Include canary strings, avoid predictable patterns

### Canary Strings

Include unique canary strings to detect cheating:

```python
CANARY_STRING = "TERMINUS_TASK_abc123xyz"

def test_canary_present():
    """Verify canary string is in output (anti-cheating measure)."""
    output = Path("/app/output.txt").read_text()
    assert CANARY_STRING in output
```

---

## Environment Requirements

The environment definition must be in the `environment/` folder.

### Dockerfile Location

Your Dockerfile must be in `environment/Dockerfile`:

```dockerfile
FROM python:3.11-slim
# or
FROM node:20-slim
# or
FROM ubuntu:22.04
```

### Dependency Pinning

All dependencies **must** have pinned versions:

```dockerfile
# ‚úÖ Good
RUN pip install pandas==2.0.0 numpy==1.24.0

# ‚ùå Bad
RUN pip install pandas numpy
```

### Security Requirements

- [ ] No privileged containers
- [ ] Solution files not baked into image
- [ ] Test dependencies not pre-installed
- [ ] Minimal attack surface
- [ ] Environment folder structure prevents accidental file copying

---

## Difficulty Requirements

Your task **must** have a pass rate below 80% against SOTA agents.

| Pass Rate | Difficulty | Status |
|-----------|------------|--------|
| < 40% | Hard | ‚úÖ Accepted |
| 40-60% | Medium | ‚úÖ Accepted |
| 60-80% | Easy | ‚úÖ Accepted |
| > 80% | Too Easy | ‚ùå Rejected |

### How to Verify

Run your task against real agents (minimum 2-3 times each):

```bash
# GPT-5.2
harbor run -a terminus-2 -m openai/@openai-tbench/gpt-5-2 -p <task-folder>

# Claude Opus 4.6
harbor run -a terminus-2 -m openai/@anthropic-tbench/claude-opus-4-6-20250929 -p <task-folder>
```

---

## Anti-Cheating Requirements

Tasks must be resistant to shortcut solutions:

### Required Measures

1. **Canary strings** ‚Äî Unique identifiers that must appear in output
2. **Dynamic elements** ‚Äî Use computed values, not hardcoded answers
3. **Validation depth** ‚Äî Test multiple aspects of the solution
4. **Non-trivial verification** ‚Äî Answers shouldn't be derivable from tests

### Red Flags

Your task will be rejected if:

- Solution can be copied from test assertions
- Output format reveals the answer structure
- Hardcoded values can pass tests
- Agent can "guess" the correct answer

---

## Automated Check Requirements

Your task must pass all CI and LLMaJ checks:

### CI Checks (Must Pass)

| Check | What It Validates |
|-------|-------------------|
| `pinned_dependencies` | All packages have version pins |
| `typos` | No spelling errors in code |
| `tests_or_solution_in_image` | Solution not in Docker image |
| `check_canary` | Canary strings present |
| `check_dockerfile_references` | All referenced files exist |
| `check_test_sh` | Test runner uses uv and produces reward file |
| `check_task_absolute_path` | Uses absolute paths |
| `ruff` | Python code passes linting |
| `validate_task_fields` | task.toml is complete |

### LLMaJ Checks (Must Pass)

| Check | What It Validates |
|-------|-------------------|
| `behavior_in_task_description` | Requirements clearly stated |
| `behavior_in_tests` | Tests verify behavior |
| `informative_test_docstrings` | Tests have good docstrings |
| `anti_cheating_measures` | Task can't be gamed |
| `hardcoded_solution` | Solution isn't trivially derivable |
| `file_reference_mentioned` | Output files specified |
| `structured_data_schema` | Data formats defined |

---

## Summary Checklist

Before submitting, verify:

- [ ] All required files present (`instruction.md`, `task.toml`, `environment/Dockerfile`, `solution/solve.sh`, `tests/test.sh`)
- [ ] `instruction.md` is human-written and clear
- [ ] `task.toml` has all required fields
- [ ] `tests/test.sh` produces `/logs/verifier/reward.txt`
- [ ] solution/solve.sh is deterministic
- [ ] All tests have docstrings
- [ ] Dependencies are pinned
- [ ] Canary strings included
- [ ] Pass rate < 80%
- [ ] CI checks pass locally
- [ ] LLMaJ checks pass locally

---

## Next Steps

- [Quality Guidelines](/portal/docs/reference/quality-guidelines) ‚Äî Required quality standards for TBench 2.0
- [Submission Checklist](/portal/docs/submitting-tasks/submission-checklist) ‚Äî Final pre-submission verification
- [CI Checks Reference](/portal/docs/testing-and-validation/ci-checks-reference) ‚Äî Detailed check explanations
- [Common Errors](/portal/docs/reviewing-tasks/common-errors) ‚Äî Avoid these mistakes
