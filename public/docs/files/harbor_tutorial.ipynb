{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harbor Framework Tutorial\n",
    "\n",
    "Learn Harbor by running a simple hello-world example.\n",
    "\n",
    "**Steps:**\n",
    "1. Install Harbor\n",
    "2. Set API key\n",
    "3. Run hello-world oracle (downloads and tests)\n",
    "4. View the task template\n",
    "5. Run with Claude AI agent\n",
    "6. View results\n",
    "7. Create a new task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1: Clone Harbor Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Harbor repository and set task path\n",
    "import os\n",
    "\n",
    "!git clone https://github.com/laude-institute/harbor.git\n",
    "\n",
    "# Define the task path for use throughout the notebook\n",
    "task_path = os.path.join(\".\", \"harbor\", \"examples\", \"tasks\", \"hello-world\")\n",
    "\n",
    "print(\"✓ Harbor repository cloned\")\n",
    "print(f\"✓ Task path set to: {task_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Harbor\n",
    "!uv tool install harbor\n",
    "\n",
    "# Update PATH so harbor command works\n",
    "import os\n",
    "os.environ[\"PATH\"] = f\"/root/.local/bin:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "print(\"✓ Harbor installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Set Your API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpt_model = \"openai/@openai-tbench/gpt-5\"\n",
    "claude_model = \"openai/@anthropic-tbench/claude-sonnet-4-5-20250929\"\n",
    "# Replace with your actual Anthropic API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.portkey.ai/v1\"\n",
    "\n",
    "print(\"✓ API key set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run Hello-World Oracle\n",
    "\n",
    "The **oracle agent** runs the reference solution to verify the task works correctly.\n",
    "\n",
    "This will:\n",
    "- Use the local hello-world task from the cloned repository\n",
    "- Build a Docker container\n",
    "- Run the reference solution (solution/solve.sh)\n",
    "- Verify tests pass\n",
    "- Show you if it succeeded (should be 100% pass)\n",
    "\n",
    "**This is your hello-world test - it should always pass!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run oracle agent on the local hello-world task\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TEST ORACLE ({task_path}):\")\n",
    "print(\"=\" * 70)\n",
    "!harbor run -p {task_path} -a oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run oracle agent on the local hello-world task interactive\n",
    "import os\n",
    "\n",
    "abs_task_path = os.path.abspath(task_path)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"TEST ORACLE INTERACTIVE({abs_task_path}):\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"harbor tasks start-env -p {abs_task_path} -a -i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: View the Task Template\n",
    "\n",
    "Now let's look at what's in the hello-world task from the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the task instruction\n",
    "instruction_path = os.path.join(task_path, \"instruction.md\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TASK INSTRUCTION (what agents need to do):\")\n",
    "print(\"=\" * 70)\n",
    "with open(instruction_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the reference solution\n",
    "solution_path = os.path.join(task_path, \"solution\", \"solve.sh\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"REFERENCE SOLUTION (what oracle ran):\")\n",
    "print(\"=\" * 70)\n",
    "with open(solution_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the test script\n",
    "test_path = os.path.join(task_path, \"tests\", \"test.sh\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST SCRIPT (how solutions are verified):\")\n",
    "print(\"=\" * 70)\n",
    "with open(test_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run with Claude AI Agent\n",
    "\n",
    "Now let's see how Claude solves the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(f\"RUN CHECK ({gpt_model}) ({task_path}):\")\n",
    "print(\"=\" * 70)\n",
    "!harbor tasks check -m {gpt_model} {task_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Claude Code agent on the local hello-world task\n",
    "print(\"=\" * 70)\n",
    "print(f\"RUN AGENT ({claude_model}) ({task_path}):\")\n",
    "print(\"=\" * 70)\n",
    "!harbor run -p {task_path} -m {claude_model} -a terminus-2\n",
    "# !harbor run -p {task_path} -m {claude_model} -a claude-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Find all job runs\n",
    "job_dirs = sorted(glob.glob(\"jobs/*\"), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "print(f\"Total runs: {len(job_dirs)}\\n\")\n",
    "\n",
    "# Show latest run\n",
    "for i, job in enumerate(job_dirs[:1], 1):\n",
    "    config_file = f\"{job}/config.json\"\n",
    "    result_file = f\"{job}/result.json\"\n",
    "    \n",
    "    if Path(config_file).exists() and Path(result_file).exists():\n",
    "        try:\n",
    "            with open(config_file) as f:\n",
    "                config = json.load(f)\n",
    "            with open(result_file) as f:\n",
    "                result = json.load(f)\n",
    "            \n",
    "            # Extract Agent and Model info\n",
    "            agent_info = config.get('agents', [{}])[0]\n",
    "            agent_name = agent_info.get('name', 'N/A')\n",
    "            model_name = agent_info.get('model_name', 'N/A')\n",
    "            \n",
    "            # Extract Status and Score\n",
    "            stats = result.get('stats', {})\n",
    "            n_errors = stats.get('n_errors', 0)\n",
    "            evals = stats.get('evals', {})\n",
    "            \n",
    "            status = \"Unknown\"\n",
    "            score = \"N/A\"\n",
    "            \n",
    "            if n_errors > 0:\n",
    "                status = \"Error\"\n",
    "                # Check for specific exceptions if available\n",
    "                for key, val in evals.items():\n",
    "                     if val.get('n_errors', 0) > 0:\n",
    "                         exceptions = val.get('exception_stats', {})\n",
    "                         if exceptions:\n",
    "                             status = f\"Error ({', '.join(exceptions.keys())})\"\n",
    "            elif evals:\n",
    "                # Assuming single evaluation key for now or taking the first one\n",
    "                first_eval = next(iter(evals.values()))\n",
    "                metrics = first_eval.get('metrics', [{}])\n",
    "                if metrics:\n",
    "                    mean_score = metrics[0].get('mean', 0)\n",
    "                    score = f\"{mean_score:.2f}\"\n",
    "                    if mean_score == 1.0:\n",
    "                        status = \"Pass\"\n",
    "                    else:\n",
    "                        status = \"Fail\"\n",
    "            \n",
    "            # Calculate Duration\n",
    "            start_time = result.get('started_at')\n",
    "            end_time = result.get('finished_at')\n",
    "            duration = \"N/A\"\n",
    "            if start_time and end_time:\n",
    "                try:\n",
    "                    start = datetime.fromisoformat(start_time)\n",
    "                    end = datetime.fromisoformat(end_time)\n",
    "                    duration = str(end - start).split('.')[0] # Remove microseconds\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            print(f\"{i}. {Path(job).name}\")\n",
    "            print(f\"   Agent:    {agent_name}\")\n",
    "            print(f\"   Model:    {model_name}\")\n",
    "            print(f\"   Status:   {status}\")\n",
    "            print(f\"   Score:    {score}\")\n",
    "            print(f\"   Duration: {duration}\")\n",
    "            print()\n",
    "\n",
    "            # --- List Agent Responses ---\n",
    "            print(\"   Agent Responses:\")\n",
    "            # Find task instance directories (e.g., hello-world__...)\n",
    "            task_dirs = glob.glob(f\"{job}/*__*\")\n",
    "            for task_dir in task_dirs:\n",
    "                task_name = Path(task_dir).name\n",
    "                print(f\"   Task Instance: {task_name}\")\n",
    "                \n",
    "                # Find episode directories\n",
    "                episode_dirs = glob.glob(f\"{task_dir}/agent/episode-*\")\n",
    "                \n",
    "                # Sort by episode number\n",
    "                def get_episode_num(path):\n",
    "                    match = re.search(r'episode-(\\d+)', path)\n",
    "                    return int(match.group(1)) if match else -1\n",
    "                \n",
    "                episode_dirs.sort(key=get_episode_num)\n",
    "                \n",
    "                for ep_dir in episode_dirs:\n",
    "                    ep_num = get_episode_num(ep_dir)\n",
    "                    response_file = Path(ep_dir) / \"response.txt\"\n",
    "                    if response_file.exists():\n",
    "                        with open(response_file, 'r') as f:\n",
    "                            response_content = f.read().strip()\n",
    "                        \n",
    "                        # Truncate if too long for display\n",
    "                        display_content = response_content\n",
    "                        if len(display_content) > 200:\n",
    "                             display_content = display_content[:200] + \"...\"\n",
    "                        \n",
    "                        print(f\"      Episode {ep_num}: {display_content.replace(chr(10), ' ')}\") # Replace newlines with spaces for compact view\n",
    "                print()\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{i}. {Path(job).name} - Error reading results: {e}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create a Custom Task\n",
    "\n",
    "Let's create a new task by copying the hello-world template. We'll call it `my-task`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define new task path\n",
    "new_task_name = \"my-task\"\n",
    "new_task_path = os.path.join(\"tasks\", new_task_name)\n",
    "\n",
    "# Source path (hello-world)\n",
    "source_path = os.path.join(\"harbor\", \"examples\", \"tasks\", \"hello-world\")\n",
    "\n",
    "# Copy if it doesn't exist\n",
    "if not os.path.exists(new_task_path):\n",
    "    shutil.copytree(source_path, new_task_path)\n",
    "    print(f\"✓ Created new task at: {new_task_path}\")\n",
    "else:\n",
    "    print(f\"✓ Task already exists at: {new_task_path}\")\n",
    "\n",
    "# List files\n",
    "print(\"\\nTask files:\")\n",
    "for f in os.listdir(new_task_path):\n",
    "    print(f\" - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Task Instructions\n",
    "\n",
    "Modify the cell below to change what the agent needs to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the instruction.md file\n",
    "instruction_content = \"\"\"\n",
    "Create a python script named `calculate.py` that prints the sum of 5 and 7.\n",
    "The output should be exactly \"12\".\n",
    "\"\"\"\n",
    "\n",
    "instruction_file = os.path.join(new_task_path, \"instruction.md\")\n",
    "\n",
    "with open(instruction_file, \"w\") as f:\n",
    "    f.write(instruction_content.strip())\n",
    "\n",
    "print(f\"✓ Updated instructions in {instruction_file}\")\n",
    "print(\"-\" * 40)\n",
    "print(instruction_content.strip())\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Reference Solution & Tests\n",
    "\n",
    "Since we changed the task, we need to update the solution and tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the solution/solve.sh\n",
    "solution_content = \"\"\"#!/bin/bash\n",
    "echo 'print(5 + 7)' > calculate.py\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(new_task_path, \"solution\", \"solve.sh\"), \"w\") as f:\n",
    "    f.write(solution_content)\n",
    "os.chmod(os.path.join(new_task_path, \"solution\", \"solve.sh\"), 0o755)\n",
    "\n",
    "# Update tests/test_state.py\n",
    "test_state_content = \"\"\"from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "def test_calculate_file_exists():\n",
    "    file_path = Path(\"/app/calculate.py\")\n",
    "    assert file_path.exists(), f\"File {file_path} does not exist\"\n",
    "\n",
    "def test_calculate_output():\n",
    "    file_path = Path(\"/app/calculate.py\")\n",
    "    # Run the script and capture output\n",
    "    result = subprocess.run([\"python3\", str(file_path)], capture_output=True, text=True)\n",
    "    content = result.stdout.strip()\n",
    "    expected_content = \"12\"\n",
    "\n",
    "    assert content == expected_content, (\n",
    "        f\"Output is '{content}', expected '{expected_content}'\"\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(new_task_path, \"tests\", \"test_state.py\"), \"w\") as f:\n",
    "    f.write(test_state_content)\n",
    "\n",
    "print(\"✓ Updated solution and tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run oracle on the new task\n",
    "print(\"=\" * 70)\n",
    "print(f\"TEST ORACLE ({new_task_path}):\")\n",
    "print(\"=\" * 70)\n",
    "!harbor run -p {new_task_path} -a oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Run with Agent\n",
    "\n",
    "Now that we've verified the task with the oracle, let's run the AI agent on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent on the new task\n",
    "print(\"=\" * 70)\n",
    "print(f\"RUN AGENT ({claude_model}) ({new_task_path}):\")\n",
    "print(\"=\" * 70)\n",
    "!harbor run -p {new_task_path} -m {claude_model} -a terminus-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: View Results\n",
    "\n",
    "Let's see how the agent performed on our custom task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Find all job runs\n",
    "job_dirs = sorted(glob.glob(\"jobs/*\"), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "print(f\"Total runs: {len(job_dirs)}\\n\")\n",
    "\n",
    "# Show latest run\n",
    "for i, job in enumerate(job_dirs[:1], 1):\n",
    "    config_file = f\"{job}/config.json\"\n",
    "    result_file = f\"{job}/result.json\"\n",
    "    \n",
    "    if Path(config_file).exists() and Path(result_file).exists():\n",
    "        try:\n",
    "            with open(config_file) as f:\n",
    "                config = json.load(f)\n",
    "            with open(result_file) as f:\n",
    "                result = json.load(f)\n",
    "            \n",
    "            # Extract Agent and Model info\n",
    "            agent_info = config.get('agents', [{}])[0]\n",
    "            agent_name = agent_info.get('name', 'N/A')\n",
    "            model_name = agent_info.get('model_name', 'N/A')\n",
    "            \n",
    "            # Extract Status and Score\n",
    "            stats = result.get('stats', {})\n",
    "            n_errors = stats.get('n_errors', 0)\n",
    "            evals = stats.get('evals', {})\n",
    "            \n",
    "            status = \"Unknown\"\n",
    "            score = \"N/A\"\n",
    "            \n",
    "            if n_errors > 0:\n",
    "                status = \"Error\"\n",
    "                # Check for specific exceptions if available\n",
    "                for key, val in evals.items():\n",
    "                     if val.get('n_errors', 0) > 0:\n",
    "                         exceptions = val.get('exception_stats', {})\n",
    "                         if exceptions:\n",
    "                             status = f\"Error ({', '.join(exceptions.keys())})\"\n",
    "            elif evals:\n",
    "                # Assuming single evaluation key for now or taking the first one\n",
    "                first_eval = next(iter(evals.values()))\n",
    "                metrics = first_eval.get('metrics', [{}])\n",
    "                if metrics:\n",
    "                    mean_score = metrics[0].get('mean', 0)\n",
    "                    score = f\"{mean_score:.2f}\"\n",
    "                    if mean_score == 1.0:\n",
    "                        status = \"Pass\"\n",
    "                    else:\n",
    "                        status = \"Fail\"\n",
    "            \n",
    "            # Calculate Duration\n",
    "            start_time = result.get('started_at')\n",
    "            end_time = result.get('finished_at')\n",
    "            duration = \"N/A\"\n",
    "            if start_time and end_time:\n",
    "                try:\n",
    "                    start = datetime.fromisoformat(start_time)\n",
    "                    end = datetime.fromisoformat(end_time)\n",
    "                    duration = str(end - start).split('.')[0] # Remove microseconds\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            print(f\"{i}. {Path(job).name}\")\n",
    "            print(f\"   Agent:    {agent_name}\")\n",
    "            print(f\"   Model:    {model_name}\")\n",
    "            print(f\"   Status:   {status}\")\n",
    "            print(f\"   Score:    {score}\")\n",
    "            print(f\"   Duration: {duration}\")\n",
    "            print()\n",
    "\n",
    "            # --- List Agent Responses ---\n",
    "            print(\"   Agent Responses:\")\n",
    "            # Find task instance directories (e.g., hello-world__...)\n",
    "            task_dirs = glob.glob(f\"{job}/*__*\")\n",
    "            for task_dir in task_dirs:\n",
    "                task_name = Path(task_dir).name\n",
    "                print(f\"   Task Instance: {task_name}\")\n",
    "                \n",
    "                # Find episode directories\n",
    "                episode_dirs = glob.glob(f\"{task_dir}/agent/episode-*\")\n",
    "                \n",
    "                # Sort by episode number\n",
    "                def get_episode_num(path):\n",
    "                    match = re.search(r'episode-(\\d+)', path)\n",
    "                    return int(match.group(1)) if match else -1\n",
    "                \n",
    "                episode_dirs.sort(key=get_episode_num)\n",
    "                \n",
    "                for ep_dir in episode_dirs:\n",
    "                    ep_num = get_episode_num(ep_dir)\n",
    "                    response_file = Path(ep_dir) / \"response.txt\"\n",
    "                    if response_file.exists():\n",
    "                        with open(response_file, 'r') as f:\n",
    "                            response_content = f.read().strip()\n",
    "                        \n",
    "                        # Truncate if too long for display\n",
    "                        display_content = response_content\n",
    "                        if len(display_content) > 200:\n",
    "                             display_content = display_content[:200] + \"...\"\n",
    "                        \n",
    "                        print(f\"      Episode {ep_num}: {display_content.replace(chr(10), ' ')}\") # Replace newlines with spaces for compact view\n",
    "                print()\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{i}. {Path(job).name} - Error reading results: {e}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
